---
title: "[Deeplearning] Transformer 에 대해 알아보자"
categories:
  - Deeplearning
tags:
  - Deeplearning
  - Transformer
  
use_math: true
toc: true
toc_sticky: true
toc_label: "Transformer 에 대해 알아보자"
---

앞으로 Transformer 를 시작으로 ChatGPT 에 쓰인 모델까지 개인적으로 공부하는 흔적을 남기고, 또 추후에 공부한 내용들을 다시 참조할 수 있게 관련하여 블로그에 내용 정리를 하려고 합니다. 논문들과 여러 책 등을 참고해서 공부하고 이해한 내용들을 작성하고자 합니다만 그래도 저는 아직 많이 부족하여 잘못된 내용 혹은 잘못 이해한 내용들을 작성할 수도 있습니다. 그러니 100% 신뢰하기 보다는 참고용으로 봐주시면 좋겠습니다.   

우선 Transformer 에 대해 공부하고 알아보기 전에 딥러닝 공부 관련 글을 적게된 개인적인 경위에 대해서 간략하게만 설명하고 넘어가도록 하겠습니다.   

저는 대학원 석사 과정 중 딥러닝 모델들 위주로 자연어처리에 대해서 공부를 했었습니다만 당시 Transformer 모델이 발표된지 얼마 되지 않아 지금과 같이 어떻게 활용할 것인지에 대한 연구도 없었고, 프로젝트 진행이 급했기에 대략적인 논문 리뷰만 하고 넘어갔고, 이후에 나온 BERT 모델도 제가 석사 과정이 끝날 때 쯤에 나와 자세히 공부해 보지는 못했습니다. 그리고 이후에 취업한 회사에서는 딥러닝 위주 보다는 회사 자체에서 개발한 사전 기반 언어 분석기 전체를 관리하는 업무를 혼자 맡다 보니 딥러닝 공부를 하지 않았습니다. 그리고 해당 회사에 있으면서 이런 레거시 프로그램이 아닌 대학원 때 했던 딥러닝을 다시 하자는 생각을 하게 되었고 그 회사를 나오고 이렇게 개인적으로 다시 처음부터 공부를 시작하게 되었습니다. 그리고 공부를 할 때에는 이렇게 블로그로 자신이 공부한 내용들을 남기는 것으로도 충분히 공부에 대한 동기와 열정을 줄 수 있다는 글을 보고 앞으로는 공부한 내용들을 블로그로 정리를 꾸준히 해야겠다는 생각이 들었고, 또한 논문을 보면서 딥러닝 모델 공부를 하는데 이해가 안가는 부분에 대해서 블로그 글 들을 참고하고자 찾아 봤는데 모두다 똑같은 내용들을 모두 복사 붙여 넣기한 블로그들이 굉장히 많아 크게 도움이 되지 않는다는 생각이 들어 다른 사람에게 그래도 조금이라도 도움이 되는 포스트를 작성해보자 하는 마음도 들어 이렇게 딥러닝 공부를 시작하면서 블로그 글을 쓰게 되었습니다.

그럼 서두가 길었습니다. Transformer 에 대해 알아보도록 하겠습니다. 저는 이 글을 작성하기 위해 [Attention is all you need](https://arxiv.org/pdf/1706.03762) 논문과 Denist Rothman 이 지은 "트랜스포머로 시작하는 자연어 처리" 를 참고하였습니다. 글의 구성은 대략적으로 Transformer 모델이 등장하게 된 배경, Transformer 의 이론적 내용 정리, 실제 코드를 이용한 실습 순으로 진행하도록 하겠습니다.

## 1. Transfomer 가 등장하게 된 배경

Transformer 등장 이전에는 자연어 처리에서 RNN(Recurrent Neural Network), 특히 LSTM(Long Short-Term Memory) 모델이 주로 사용되었습니다. LSTM은 RNN의 장기 의존성 문제(long-term dependency problem)를 내부의 게이트 구조를 통해 어느 정도 완화했지만, 여전히 정보 전달이 길어질수록 학습이 어려워지는 한계가 있었습니다.

이를 해결하기 위한 시도로 Attention Mechanism이 Encoder-Decoder 구조에 도입되었고, 이 메커니즘은 각 입력 토큰의 중요도를 계산하여 멀리 떨어진 토큰 간의 관계도 학습할 수 있는 가능성을 보여주었습니다.

하지만 LSTM 기반 모델은 순차적으로 계산되는 구조 때문에 병렬 처리가 어려워 속도가 매우 느리며, Attention이 도입되었음에도 불구하고 장기 의존성 완전 해결에는 여전히 한계가 존재했습니다.

이러한 문제를 극복하고자 연구자들은 Attention 자체의 표현력에 주목하기 시작했습니다.
"그렇다면 굳이 순차 모델(RNN/LSTM)을 쓰지 않고, Attention만으로 시퀀스를 처리할 수 있지 않을까?"

이 아이디어로부터 완전히 Attention 기반인 Transformer 모델이 등장하게 되었습니다.
Transformer는 입력 시퀀스를 한 번에 처리하고, Self-Attention 메커니즘을 통해 모든 위치 간의 상호작용을 계산함으로써 병렬화, 장기 의존성, 표현력이라는 측면에서 기존 LSTM 기반 모델을 압도하는 성능을 보였습니다.

## 2. Transformer 구조 및 원리

Transfomer 모델의 구조는 아래 그림과 같습니다.

<br>

<div align="center">
<img src="/assets/images/deeplearning/transformer/transformer-architecture.png" width="50%" hegiht="40%">
</div>

<br>

### Transformer 모델의 구조

#### 인코더 스택

오리지널 트랜스포머 모델의 인코더와 디코더는 층을 쌓아 올린 스택 형태(stack of layer) 로 되어 있습니다.

오리지널 트랜스포머 모델의 인코더  층은 총 N=6 개로 모두 동일한 구조입니다. 각각의 층에 멀티-헤드 어텐션 메커니즘, 완전 연결 위치별 순방향 네트워크(fully connected position-wise feed-forward network)인 두 서브층을 가지고 있습니다.

잔차 연결(residual connection)이 트랜스포머 모델의 각 서브 층을 둘러싸고 있습니다. 잔차 연결은 서브 층의 입력 x 를 층 정규화(layer normalization) 함수에 전달하여, 위치 인코딩(positional encoding)과 같은 주용한 정보가 손실되지 않도록 보장합니다.

#### 디코더 스택

트랜스포머의 디코더 역시 인코더처럼 층을 쌓아 올린 스택 형태입니다. 트랜스포머의 인코더처럼, N=6개 디코더 층의 구조는 모두 동일합니다. 각 층은 3개의 서브층으로 이루어져 있는데, 멀티-헤드 마스크드 어텐션(multi-head masked attention) 메커니즘, 멀티-헤드-어텐션 메커니즘, 완전 연결 위치별 순방향 네트워크입니다.

디코더에는 세 번째 주요 층인 마스크드 멀티-헤드 어텐션이 있는데 이 층에서는 주어진 위치 이후의 모든 단어를 마스킹 함으로써, 트랜스포머가 나머지 시퀀스를 보지 않고 스스로의 추론에 근거하여 연산하도록 합니다. 마스크드 멀티-헤드 어텐션은 "Attention is all you need" 논문에서 자세히 다루지 않고 “To prevent positions from attending to subsequent positions, we mask the future positions (setting them to −∞) in the scaled dot-product attention.” 로 굉장히 간단한 설명만 하고 있습니다. 그래서 지금과 달리 트랜스포머가 발표됐을 당시에는 논문만 보고 트랜스포머 모델을 구현하기에는 굉장히 어려웠습니다. 따라서 관련해서 뒤에서 트랜스포머 모델의 원리 부분에서 좀 더 자세히 알아보도록 하겠습니다.

트랜스포머의 최종 출력은 한 번에 하나의 출력 시퀀스만 생성합니다. 그래서 각 층에 사용한 차원을 vocab size 차원으로 바꾸어 주는 선형 층이 있고, 선형 층을 통해 나온 결과를 소프트맥스하여 값이 가장 큰 토큰을 출력으로 사용합니다.

### Transformer 모델의 Task

Transformer 모델의 Task 는 Encoder-Decoder 구조 기반의 언어 번역 Task 입니다. 즉 입력 문장을 넣어주면 학습한 다른 언어로 번역을 해줍니다.

<br>

### Transformer 모델의 Input 과 Output

#### Input

- 학습된 모델의 input : 번역할 문장
- 학습 데이터의 input
  - Encoder input : 번역할 문장
  - Decoder input : 기존 번역된 문장에서 시작 위치에 [START] 토큰을 추가하고, 마지막 토큰은 제거된 문장

```
학습된 모델의 input
input : I love you

학습 데이터의 Encoder input
{
  "input_ids": I love you
}

학습 데이터의 Decoder input
{
  "input_ids": [START, Ich, liebe]
}

```

#### Output

- 학습된 모델의 output : 번역된 문장
- 학습 데이터의 ouput : 번역된 문장의 token

```
학습된 모델의 output
output : Ich liebe dich

학습 데이터의 output
{
  "labels": [Ich, liebe, dich]
}
```

### Transformer 모델의 원리

#### 입력 임베딩

Transfomer 에 사용된 입력 임베딩은 구글이 2013년 발표한 스킵 그램(skip-gram) 을 이용해 생성된 임베딩을 사용합니다. 다만 스킵 그램의 입력으로 사용되는 token 은 BPE(Byte-Pair-Encoding), 워드 피스(word piece), 센텐스 피스(sentence piece) 와 같은 토크나이저를 이용합니다.

인코더 스택에서는 번역할 언어의 문장에서 토큰을 추출하고 각 토큰을 임베딩하여 입력 임베딩으로 사용합니다. 디코더 스택에서는 번역될 언어의 문장에서 토큰을 추출하고 각 토큰을 임베딩하여 입력 임베딩으로 사용합니다.

#### 위치 인코딩 (Positional Encoding)

Transformer 에서는 attention 기법만 사용하게 됩니다. 하지만 attention 기법은  뛰어난 성능, 장기 의존성 문제 해결 등의 장점이 있지만 한 가지 부족한 점이 각 토큰의 위치 정보가 존재하지 않아 각 토큰이 어디에 위치하는지를 구별할 수 없다는 것입니다. 그렇다고 위치 벡터를 별개로 학습하면 트랜스포머의 학습 속도가 매우 느려질 수 있고, 어텐션의 서브 층이 너무 복잡해질 위험이 있습니다. 따라서 추가적인 벡터를 사용하는 대신, 입력 임베딩에 위치 인코딩 값을 더하여 시퀀스 내 토큰의 위치를 표현하였습니다. 사용한 공식은 다음과 같습니다.

$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\end{aligned}
$$

<br>

여기서:

* $pos$는 단어의 **시퀀스 상 위치**를 의미합니다.
* $i$는 **임베딩 차원 내 인덱스**입니다.
* $d_{\text{model}}$은 전체 임베딩 벡터의 **차원 수**입니다 (예: 512).
* 짝수 인덱스에는 사인 함수, 홀수 인덱스에는 코사인 함수를 사용합니다.

이렇게 만들어진 위치 임베딩은 기존 입력 임베딩과 합쳐져서 Transformer 의 입력으로 사용됩니다.

#### 멀티-헤드 어텐션

멀티 헤드 어텐션은 이전에 LSTM과 같이 모델 내부에서 학습이 되는 웨이트들을 토큰 하나씩 loop 로 보면서 학습하는 것이 아니라 입력된 문장의 전체 토큰들을 지정한 헤드의 수로 나누어 self-attention 을 수행하며, Q, K, V 라는 출력을 위해 각 다른 정보를 학습하도록 하는 웨이트들을 두어 단순히 층을 많이 쌓아서 학습이 잘 되는 것이 아닌 웨이트가 학습 될 때 필요한 정보들만 학습하도록 한 어텐션 기법입니다. 그렇다면 멀티-헤드 어텐션에 대해서 자세히 알아보도록 하겠습니다.

##### 멀티-헤드

멀티-헤드는 "Attention is all you need" 에서 설명하길 병렬처리를 하기 위한 방법이라고 소개가 됩니다.
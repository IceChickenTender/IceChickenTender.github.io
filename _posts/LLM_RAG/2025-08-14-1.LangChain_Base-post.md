---
title: "[LLM/RAG] LangChain 기초"
categories:
  - LLM/RAG
  - LangChain
tags:
  - LLM/RAG
  - LangChain

use_math: true  
toc: true
toc_sticky: true
toc_label: "LangChain 기초"
---

LLM/RAG 에 대해서 제대로 공부하기 위해 기초적인 Langchain 에 대해서 공부해보고자 합니다.   그리고 공부하는 과정에서 추후에 제가 참고하기 위해 블로그로 기록해 놓고자 합니다. LangChain 공부는 wiki docs 에 있는 `판다스 스튜디오`님이 작성하신 `랭체인(LangChain) 입문부터 응용까지` 를 참고하여 진행하도록 하겠습니다. 또한 실습 코드들은 구글 코랩에 작성되는 코드들이니 이를 참고해 주시기 바랍니다.

# 1. LangChain 개요

## 1.1 개요

랭체인(LangChain)은 **해리슨 체이스(Harrison Chase)**가 2022년 10월에 시작한 오픈 소스 프로젝트입니다.  
당시 그는 머신러닝 스타트업 **로버스트 인텔리전스(Robust Intelligence)**에서 근무하며, 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발이 데이터 연결, 체인 구성, 외부 API 통합 등 많은 복잡한 작업을 요구한다는 한계를 직접 경험했습니다.  
이를 해결하고자, **챗봇·질의응답(QA)·자동 요약** 등 다양한 LLM 애플리케이션을 쉽고 빠르게 구축할 수 있는 개발 프레임워크를 설계했습니다.

---

## 1.2 성장 과정
- **2023년 4월**  
  랭체인은 법인으로 전환하고 **세쿼이아캐피털(Sequoia Capital)** 등 주요 벤처캐피털로부터 투자를 유치하며 빠르게 성장했습니다.
- **2024년 1월**  
  최초의 안정적(stable) 버전인 **v0.1.0**을 출시하였으며, Chains·Agents·Tools·RAG 통합 등 LLM 개발에 필수적인 기능을 체계적으로 지원하기 시작했습니다.

---

## 1.3 버전 안정화와 개선

이전 버전까지는 잦은 업데이트와 그로 인한 버그, 코드 오류로 인해 개발자들이 불편을 겪는 경우가 많았습니다.  
그러나 **v0.1.0** 이후에는 이러한 문제가 상당 부분 개선되었으며, 현재는 보다 **안정적이고 개발자 친화적인 환경**을 제공합니다.

---

## 1.4 주요 특징
- **Chains** : LLM 호출을 여러 단계로 연결해 복잡한 작업을 구성
- **Agents** : 동적으로 어떤 도구(함수, API, DB)를 호출할지 결정
- **Tools** : 검색, 계산, API 호출 등 외부 리소스와 연동
- **RAG 지원** : 검색 기반의 프롬프트 구성으로 LLM 응답 품질 향상
- **다양한 통합성** : OpenAI, Anthropic, Hugging Face 등 주요 LLM과 연동 가능

---

## 1.5 정리
랭체인은 LLM 애플리케이션 개발의 복잡성을 크게 줄여주는 프레임워크로,  
**빠른 프로토타이핑 → 안정적인 서비스 구현**까지 전 과정을 지원합니다.  
기업과 개인 개발자 모두에게 유용하며, RAG 기반 서비스나 에이전트 시스템 구축에도 널리 활용되고 있습니다.

---

## 1.6 LangChain 프레임워크의 구성

랭체인(LangChain) 프레임워크는 LLM 애플리케이션 개발에 도움이 되는 여러 구성 요소로 이루어져 있습니다. 특히 개발자들이 다양한 LLM 작업을 신속하게 구축하고 배포할 수 있도록 설계되었습니다. 랭체인의 주요 구성 요소는 다음과 같습니다.

1. **랭체인 라이브러리(LangChain Libraries)** : 파이썬과 자바스크립트 라이브러리를 포함하며, 다양한 컴포넌트의 인터페이스와 통합, 이 콤포넌트들을 체인과 에이전트로 결합할 수 있는 기본 런타임, 그리고 체인과 에이전트의 사용 가능한 구현이 가능합니다.

2. **랭체인 템플릿(LangChain Templates)**: 다양한 작업을 위해 쉽게 배포할 수 있는 참조 아키텍처 모음입니다. 이 템플릿은 개발자들이 특정 작업에 맞춰 빠르게 애플리케이션을 구축할 수 있도록 돕습니다.

3. **랭서브(LangServe)**: 래엧인 체인을 REST API 로 배포할 수 있게 해주는 라이브러리입니다. 이를 통해 개발자들은 자신의 애플리케이션을 외부 시스템과 쉽게 통합할 수 있습니다.

4. **랭스미스(LangSmith)**: 개발자 플랫폼으로, LLM 프레임워크에서 구축된 체인을 디버깅, 테스트, 평가, 모니터링할 수 있으며, 랭체인과의 원활한 통합을 지원합니다.

우선 제가 참고한 wiki docs 에서는 랭체인 라이브러리(LangChain Libraries) 위주로 다루기 때문에 저 또한 랭체인 라이브러리 위주로 진행하고 추후에 다른 프레임워크들에 대해서도 알아보도록 하겠습니다.

---

## 1.7 필수 라이브러리 설치

사용할 랭체인 버전은 v0.3 이상 버전을 사용할 예정입니다.

---

### 1.7.1 랭체인 설치

구글 코랩에서 다음과 같이 작성 후 실행해 줍니다.

```python
!pip install langchain>=0.3.0
```

---

### 1.7.2 OpenAI 관련 패키지 설치

OpenAI 모델을 사용할 때 필요한 의존성 라이브러리를 설치하는 방법입니다.   
구글 코랩에서 다음과 같이 작성 후 실행해 줍니다.

langchain-openai 설치
```python
!pip install langchain-openai>=0.2.0
```

tiktoken 설치
```python
!pip install tiktoken
```

---

# 2. LLM 체인(LLMChain) 만들기

## 2.1 LLMChain 이란?

`LLMChain` 은 **프롬프트 → LLM 호출 → 출력 반환**의 가장 기본적인 파이프라인을 캡슐화한 클래스입니다. 즉 "프롬프트를 만들고 그걸 모델에 넣어 한 번 추론하는" 템플릿화된 단일 스텝 체인입니다. 핵심 구성은 다음과 같습니다.

---

## 2.2 기본 LLM 체인의 구성 요소

1. 프롬프트(Prompt): 사용자 또는 시스템에서 제고앟는 입력으로, LLM 에게 특정 작업을 수행하도록 요청하는 지시문입니다. 프롬프트는 질문, 명령, 문장 시작 부분 등 다양한 형태를 취할 수 있으며, LLM 의 응답을 유도하는 데 중요한 역할을 합니다.

2. LLM(Large Language Model): GPT, Gemini 등 대규모 언어 모델로, 대량의 텍스트 데이터에서 학습하여 언어를 이해하고 생성할 수 있는 인공지능 시스템입니다. LLM 은 프롬프트를 바탕으로 적절한 응답을 생성하거나, 주어진 작업을 수행하는데 사용됩니다.

---

## 2.2.1 일반적인 작동 방식

1. 프롬프트 생성: 사용자의 요구 사항이나 특정 작업을 정의하는 프롬프트를 생성합니다. 이 프롬프트는 LLM 에게 전달되기 전에, 자겁의 목적과 맥락을 명확히 전달하기 위해 최적화될 수 있습니다.

2. LLM 처리: LLM 은 제공된 프롬프트를 분석하고, 학습된 지식을 바탕으로 적절한 응답을 생성합니다. 이 과정에서 LLM 은 내부적으로 다양한 언어 패턴과 내외부 지식을 활용하여, 요청된 작업을 수행하거나 정보를 제공합니다.

3. 응답 반환: LLM 에 의해 생성된 응답은 최종 사용자에게 필요한 형태로 변환되어 제공됩니다. 이 응답은 직접적인 답변, 생성된 텍스트, 요약된 정보 등 다양한 형태를 취할 수 있습니다.

---

## 2.2.2 실습 예제

### 2.2.2.1 LLM 으로부터 답변 받기

OpenAI 의 ChatOpenAI 함수를 사용하면 OpenAI 의 여러 모델에 API 로 접근할 수 있습니다. 다음 예제는 랭체인에서 gpt-4o-mini 모델을 사용하여 LLM 모델 인스턴스를 생성하고, "지구의 자전 주기는?" 라는 프롬프트를 LLM 모델에 전달하는 과정을 보여줍니다. 실행 결과로 "지구의 자전 주기는?" 에 대한 답변을 반환합니다.

```python
import os
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = 'API_KEY'

llm = ChatOpenAI(model="gpt-4o-mini")

llm.invoke("지구의 자전 주기는?")
```

```
실행 결과

AIMessage(content="지구의 자전 주기는 약 24시간입니다. 정확히는 23시간 56분 4초로, 이를 '항성일'이라고 합니다. 그러나 우리가 일반적으로 사용하는 24시간은 태양이 하늘에서 동일한 위치에 돌아오는 시간을 기준으로 한 '태양일'입니다.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 15, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C4kgwz8xFLUk3WrJtrlcxvKaRM2Ew', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f1453437-9460-4eab-a7bf-2f7113f935d8-0', usage_metadata={'input_tokens': 15, 'output_tokens': 68, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

---

### 2.2.2.2 프롬프트 템플릿 적용

이번에는 간단한 프롬프트 템플릿을 적용해 보도록 하겠습니다. 더 자세한 내용은 추후에 다룰 예정입니다. 예제에서는 `langchain_core` 의 `prompts` 모듈에서 `ChatPromptTemplate` 클래스를 사용하여 천문학 전문가로서 질문에 답변하는 형식의 프롬프트 템플릿을 생성합니다. 이 템플릿을 사용하면, 입력으로 주어진 질문 `{input}` 에 대해 천문학 전문가의 관점에서 답변을 생성하는 질문 프롬프트를 만들 수 있습니다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("You are an expert in astronomy. Answer te question. <Question>: {input}")

llm = ChatOpenAI(model="gpt-4o-mini")

chain = prompt | llm

chain.invoke({"input": "지구의 자전 주기는?"})
```

실행 결과를 보면 이전에는 "지구의 자전 주기는 약 24시간입니다. 정확히는 23시간 56분 4초로, 이를 '항성일'이라고 합니다. 그러나 우리가 일반적으로 사용하는 24시간은 태양이 하늘에서 동일한 위치에 돌아오는 시간을 기준으로 한 '태양일'입니다." 라고 한 답변과는 다른 답변을 받아온 것을 확인할 수 있습니다.

```
실행결과

AIMessage(content='지구의 자전 주기는 약 24시간입니다. 정확히 말하자면, 지구가 한 바퀴 자전하는 데 걸리는 시간은 약 23시간 56분 4초로, 이를 태양일로 환산하면 약 24시간이 됩니다. 태양일은 지구가 태양을 기준으로 하루를 완성하는 데 걸리는 시간을 의미합니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 29, 'total_tokens': 115, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C4krNA5uwKQbjjG95Pqv7Hk4ddx7G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--17a488f7-0861-4d34-b9d4-8b469aa6422d-0', usage_metadata={'input_tokens': 29, 'output_tokens': 86, 'total_tokens': 115, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

또한 `Parser` 를 적용하여 특정 출력 형태로 LLM 의 결과를 받아볼 수 있습니다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("You are an expert in astronomy. Answer te question. <Question>: {input}")

llm = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()

chain = prompt | llm | output_parser

chain.invoke({"input": "지구의 자전 주기는?"})
```

그럼 이전 출력들과는 다르게 단순히 LLM 의 답변만을 출력합니다.

```
실행결과

지구의 자전 주기는 약 24시간입니다. 더 정확하게는 평균적으로 23시간 56분 4초로, 이를 '항성일'이라고 합니다. 그러나 우리가 일상적으로 사용하는 24시간은 태양일로, 태양이 같은 위치에 다시 나타나기까지의 시간을 기준으로 합니다. 태양일은 항성일보다 약 4분 정도 더 긴 이유는 지구가 태양 주위를 공전하고 있기 때문입니다.
```

## 2.3 멀티 체인(Multi-Chain)

멀티 체인(Multi-Chain) 은 여러 개의 체인을 연결해 복합적인 구조로 작용하는 것을 말합니다. 이러한 구조는 각기 다른 목적을 가진 여러 체인을 조합하여, 입력 데이터를 다양한 방식으로 처리하고 최종적인 결과를 도출할 수 있도록 합니다. 복잡한 데이터 처리, 의사 결정, AI 기반 작업 흐름을 설계할 때 특히 유용합니다.

### 2.3.1 순차적인 연결 체인

다음 예제를 통해서 2개의 체인(chain1, chain2) 를 정의하고, 순차적으로 체인을 연결하여 수행하는 작업을 해보겠습니다.   

첫 번째 체인(chain1) 은 한국어 단어를 영어로 번역하는 작업을 수행합니다. `ChatPromptTemplate.from_template` 를 사용하여 프롬프트를 정의하고, `ChatOpenAI` 인스턴스로 GPT 모델을 사용하며, `StrOutputParser()` 를 통해 모델의 출력을 문자열로 파싱합니다.   

chain1 은 입력받은 "미래" 라는 단어를 영어로 번역하라는 요청을 gpt-4o-mini 모델에 전달하고, 모델이 생성한 번역 결과를 출력합니다. 체인을 실행하는 invoke 메소드는 입력으로 딕셔너리(dictionary) 객체를 받으며, 이 딕셔너리의 키는 프롬프트 템플릿에서 정의된 변수명({korean_word})와 일치해야 합니다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt1 = ChatPromptTemplate.from_template("translates {korean_word} to English")
prompt2 = ChatPromptTemplate.from_template("explain {english_word} using oxford dictionary to me in Korean.")

llm = ChatOpenAI(model="gpt-4o-mini")

chain1 = prompt1 | llm | StrOutputParser()

chain1.invoke({"korean_word": "미래"})
```

실행 결과로 wiki docs 에서는 "future" 라는 한 단어만 출력된다고 했지만 저는 아래와 같이 출력되는 것을 확인했습니다.

```
실행 결과

The Korean word "미래" translates to "future" in English.
```

chain1 에서 출력한 값을 입력값으로 받아서, 이 번역된 단어를 english_word 변수에 저장합니다. 다음으로, 이 변수를 사용해 두 번째 체인(chain2) 의 입력으로 제공하고, 영어 단어의 뜻을 한국어로 설명하는 작업을 수행합니다. 최종 출력은 문자열로 출력되도록 합니다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt1 = ChatPromptTemplate.from_template("translates {korean_word} to English")
prompt2 = ChatPromptTemplate.from_template("explain {english_word} using oxford dictionary to me in Korean.")

llm = ChatOpenAI(model="gpt-4o-mini")

chain1 = prompt1 | llm | StrOutputParser()

chain2 = (
    {"english_word":chain1}
    | prompt2
    | llm
    | StrOutputParser()
)

chain2.invoke({"korean_word":"미래"})
```

wiki docs 에서와는 결과가 조금 다릅니다. 아마도 wiki docs 가 작성될 때의 `gpt-4o-mini` 학습과 제가 블로그를 작성할 때의 `gpt-4o-mini` 학습이 조금 다를 수도 있고 아니면 또 다른 원인이 있을 것 같은데 그래도 얻고자 하는 결과는 얻을 수 있었습니다.

```
실행 결과

"미래"라는 한국어 단어는 영어로 "future"로 번역됩니다. 옥스포드 사전(Oxford Dictionary)에 따르면, "future"는 다음과 같이 정의됩니다:

1. **명사**:
   - 아직 오지 않은 시간, 특히 현재 시점 이후의 시간.
   - 어떤 일이나 사건이 일어날 것으로 예상되는 시간.

이와 같이 "미래"는 우리가 앞으로 경험하게 될 시간이나 상황을 의미합니다.
```

---

## 2.4 체인을 실행하는 방법

## 2.4.1 LangChain 의 "Runnable" 프로토콜

LangChain 의 `Runnable` 프로토콜은 사용자가 사용자 정의 체인을 쉽게 생성하고 관리할 수 있도록 설계된 핵심적인 개념입니다. 이 프로토콜을 통해, 개발자는 일관된 인터페이스를 사용하여 다양한 타입의 컴포넌트를 조합하고, 복잡한 데이터 처리 파이프라인을 구성할 수 있습니다. `Runnalbe` 프로토콜은 다음과 같은 주요 메소드를 제공합니다.

- invoke: 주어진 입력에 대해 체인을 호출하고, 결과를 반환합니다. 이 메소드는 단일 입력에 대해 동기적으로 작동합니다.
- batch: 입력 리스트에 대해 체인을 호출하고, 각 입력에 대한 결과를 리스트로 반환합니다. 이 메소드는 여러 입력데 해새 동기적으로 작동하며, 효율적인 배치 처리를 가능하게 합니다.
- stream: 입력에 대해 체인을 호출하고, 결과의 조각들을 스트리밍합니다. 이는 대용량 데이터 처리나 실시간 데이터 처리에 유용합니다.
- 비동기 버전: ainvoke, abatch, astream 등의 메소드는 각각의 동기 버전에 대한 비동기 실행을 지원합니다. 이를 통해 비동기 프로그래밍 패러다임을 사용하여 더 높은 처리 성능과 효율을 달성할 수 있습니다.

각 컴포넌트는 입력 및 출력 유형이 명확하게 정의되어 있으며, `Runnable` 프로토콜을 구현함으로써, 이러한 컴포넌트들은 입력과 출력 스키마를 검사할 수 있습니다. 이는 개발자가 타입 안정성을 보장하고, 예상치 못한 오류를 방지할 수 있도록 도와줍니다.

LangChain 을 사용하여 커스텀 체인을 생성하는 과정은 다음과 같습니다

1. 필요한 컴포넌트들을 정의하고, 각각 `Runnable` 인터페이스를 구현합니다.
2. 컴포넌트들을 조합하여 사용자 정의 체인을 생성합니다.
3. 생성된 체인을 사용하여 데이터 처리 작업을 수행합니다. 이때, `invoke`, `batch`, `stream` 메소드를 사용하여 원하는 방식으로 데이터를 처리할 수 있습니다.

LangChain 의 `Runnable` 프로토콜을 사용하면, 개발자는 보다 유연하고 확장 가능한 방식으로 데이터 처리 작업을 설계하고 구현할 수 있으며, 복잡한 언어 처리 작업을 보다 쉽게 관리할 수 있습니다.

invoke 사용 예제

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. 컴포넌트 정의

prompt = ChatPromptTemplate.from_template("지구과학에서 {topic}에 대해 간단히 설명해 주세요")
model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()

# 2. 체인 생성

chain = prompt | model | output_parser

# 3. invoke 메소드 사용
result = chain.invoke({"topic": "지구 자전"})
print("invoke 결과:", result)

```

```
실행 결과

invoke 결과: 지구 자전은 지구가 자신의 축을 중심으로 회전하는 과정을 말합니다. 이 회전은 약 24시간, 즉 하루에 한 번 이루어지며, 지구 자전으로 인해 낮과 밤이 발생합니다. 자전의 방향은 서쪽에서 동쪽으로 향하고 있으며, 이로 인해 태양은 동쪽에서 떠서 서쪽으로 지는 것처럼 보입니다.

지구의 자전은 또한 지구의 기후와 날씨에 영향을 미치며, 지구의 자전축은 약 23.5도의 경사를 가지고 있어 계절 변화에도 중요한 역할을 합니다. 고대부터 오늘날까지 지구 자전은 천문학적 현상과 다양한 자연현상을 이해하는 데 중요한 기초가 되고 있습니다.
```

```python

# batch 메소드 사용
topics = ["지구 공전", "화산 활동", "대륙 이동"]
results = chain.batch([{"topic":t} for t in topics])

for topic, result in zip(topics, results):
  print(f"{topic} 설명: {result[:50]}...") # 결과의 처음 50자만 출력

```

```
실행 결과

지구 공전 설명: 지구 공전은 지구가 태양 주위를 타원형 궤도로 돌면서 한 바퀴 도는 과정을 의미합니다. 이...
화산 활동 설명: 화산 활동은 지구 내부의 마그마가 지표로 분출되는 과정으로, 대개 지진 활동과 관련이 있습...
대륙 이동 설명: 대륙 이동은 지구의 대륙들이 서로 다른 방향으로 이동하는 과정을 설명하는 이론으로, 주로 ...
```

```python
# stream 메소드 사용

stream = chain.stream({"topic":"지진"})
print("stream 결과:")

for chunk in stream:
  print(chunk, end="", flush=True)
print()
```

```
실행 결과

stream 결과:
지진은 지구의 지각에서 발생하는 갑작스러운 진동이나 충격으로, 주로 지각 변동이나 지하에서의 압력이 변화할 때 발생합니다. 지진의 주요 원인은 다음과 같습니다.

1. **단층 활동**: 지각에 있는 두 지점이 서로 움직일 때 발생하는데, 이 과정에서 저장된 에너지가 갑작스럽게 방출되며 지진이 발생합니다.

2. **화산 활동**: 화산이 폭발하거나 마그마가 이동할 때 발생할 수 있습니다.

3. **인간 활동**: 광산 채굴, 대형 건축물 건설, 댐 건설 등 인위적인 활동도 지진을 유발할 수 있습니다.

지진의 강도는 리히터 규모나 모멘트 규모로 측정되며, 진원의 깊이와 거리에 따라 지상에서 느껴지는 강도도 달라집니다. 강한 지진은 건물이나 인프라에 심각한 피해를 줄 수 있으며, tsunamis와 같은 2차 재해를 유발할 수도 있습니다. 지진의 발생 및 예측을 위해 지진계와 같은 다양한 기상 장비가 사용됩니다.
```

```python
import nest_asyncio
import asyncio

# nest_asyncio 적용 (구글 코랩 등 주피터 노트북에서 실행 필요)
nest_asyncio.apply()

# 비동기 메소드 사용(async/await 구문 필요)

async def run_async():
  result = await chain.ainvoke({"topic":"해류"})
  print("ainvoke 결과:", result[:50], "...")

asyncio.run(run_async())
```

```
실행 결과

ainvoke 결과: 해류는 바다의 물이 일정한 방향으로 흐르는 현상을 말합니다. 해류는 여러 가지 요인에 의해 ...
```

---

## 2.5 콜백 및 스트리밍 고급제어

### 2.5.1 CallbackHandler 시스템

LangChain 문서에 따르면, 콜백 시스템은 LLM 애플리케이션의 다양한 단계를 모니터링할 수 있게 해주는 강력한 기능입니다. 이는 로깅, 모니터링, 스트리밍 및 기타 작업에 유용합니다.

#### 2.5.1.1 기본 콜백 핸들러 구현 분석

##### 2.5.1.1.1 AstronomyCallbackHandler 클래스 구조

```python
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.messages import BaseMessage
from typing import Dict, List, Any
import time
from datetime import datetime

class AstronomyCallbackHandler(BaseCallbackHandler):
    """천문학 체인 전용 콜백 핸들러"""

    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.start_time = None
        self.chain_steps = []
        self.token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

```

핵심 초기화 요소
- `BaseCallbackHandler` 를 상속받아 표준 인터페이스를 구현합니다.
- `verbose` 모드를 통해 출력 상세도를 제어합니다.
- 실행 시간 추적을 위한 `start_time`을 관리합니다.
- 체인 실행 단계를 기록하는 `chain_steps` 리스트를 유지합니다.
- 토큰 사용량을 추적하는 딕셔너리를 포함합니다

---

##### 2.5.1.1.2 핵심 콜백 메서드 구현

```python
def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs) -> None:
    """체인 시작 시 호출"""
    self.start_time = time.time()
    self.chain_steps = []

    if self.verbose:
        print(f"🚀 [체인 시작] {datetime.now().strftime('%H:%M:%S')}")
        print(f"📝 입력: {inputs}")
        print("-" * 50)

```

체인 시작 이벤트 처리
- 실행 시작 시간을 기록합니다.
- 체인 단계 리스트를 초기화합니다.
- 사용자 친화적인 로킹 형식으로 입력 정보를 출력합니다.

```python
def on_llm_end(self, response: LLMResult, **kwargs) -> None:
    """LLM 호출 완료 시 호출"""
    self.chain_steps.append("LLM 호출 완료")

    # 토큰 사용량 업데이트
    if hasattr(response, 'llm_output') and response.llm_output:
        usage = response.llm_output.get('token_usage', {})
        self.token_usage['prompt_tokens'] += usage.get('prompt_tokens', 0)
        self.token_usage['completion_tokens'] += usage.get('completion_tokens', 0)
        self.token_usage['total_tokens'] += usage.get('total_tokens', 0)

```

LLM 오나료 이벤트 처리
- 체인 단계에 완료 상태를 추가합니다.
- 응답 객체에서 토큰 사용량 정보를 추출하여 누적합니다.
- 안전한 딕셔너리 접근을 통해 예외 상황을 처리합니다.

---

#### 2.5.1.2 멀티 콜백 시스템 구현

##### 2.5.1.2.1 성능 측정 전용 콜백

```python
class PerformanceCallbackHandler(BaseCallbackHandler):
    """성능 측정 전용 콜백"""

    def __init__(self):
        self.performance_data = {}
        self.current_step = None
        self.step_start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.current_step = 'llm_processing'
        self.step_start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        if self.current_step and self.step_start_time:
            duration = time.time() - self.step_start_time
            self.performance_data[self.current_step] = duration

```

성능 메트릭 수집
- 각 처리 단계별 실행 시간을 정밀하게 측정합니다.
- LLM 처리와 전체 체인 실행 시간을 분리하여 추적합니다.
- 오버헤드 계산을 위한 데이터를 수집합니다.

---

##### 2.5.1.2.2 로깅 전용 콜벡

```python
class LoggingCallbackHandler(BaseCallbackHandler):
    """로깅 전용 콜백"""

    def __init__(self, log_file: str = None):
        self.log_file = log_file
        self.logs = []

    def _log(self, message: str):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.logs.append(log_entry)

        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry + '\n')

```

로깅 시스템 특징
- 타임스탬프가 포함된 구조화된 로그 형식을 제공합니다.
- 메모리 내 로그와 파일 로그를 동시에 지원합니다.
- 안전한 파일 쓰기를 위한 컨텍스트 매니저를 사용합니다.

---

#### 2.5.1.3 멀티 콜백 활용 패턴

##### 2.5.1.3.1 콜백 조합 사용

```python
performance_callback = PerformanceCallbackHandler()
logging_callback = LoggingCallbackHandler()

# 여러 콜백을 함께 사용
callbacks = [performance_callback, logging_callback]

# 테스트 실행
test_topics = ["블랙홀", "중성자별", "적색거성"]

for topic in test_topics:
    result = chain.invoke(
        {"topic": topic}, 
        config={"callbacks": callbacks}
    )

```

멀티 콜백 장점
- 각 콜백이 서로 다른 관점에서 체인 실행을 모니터링합니다.
- 성능, 로깅, 오류 처리를 독립적으로 관리할 수 있습니다.
- 모듈화된 구조로 재사용성이 높습니다.

---

##### 2.5.1.3.2 실시간 성능 분석

```python
# 성능 리포트 출력
perf_report = performance_callback.get_report()
print("성능 지표:")
for metric, value in perf_report.items():
    print(f"  - {metric}: {value}")

```

성능 분석 결과
- LLM 처리 시간과 전체 실행 시간을 분리하여 측정합니다.
- 오버헤드를 계산하여 최적화 포인트를 식별합니다.
- 실시간으로 성능 메트릭을 확인할 수 있습니다.

### 2.5.2 LangChain 콜백 이벤트 체계

LangChain 문서에서 정의한 주요 콜백 이벤트들은 다음과 같습니다.

|이벤트|트리거 시점|연관메소드|
|:---:|:---------:|:-------:|
|Chain start|체인 시작시 | `on_chain_start`|
|Chain end  |체인 종료 시| `on_chain_end`  |
|LLM start  |LLM 시작 시 | `on_llm_start` |
|LLM end    |LLM 종료 시 | `on_llm_end`   |
|LLM error  |LLM 오류 시 | `on_llm_error` |

### 2.5.3 실제 활용 시나리오

#### 2.5.3.1 프로덕션 모니터링

- API 호출 빈도와 응답 시간 추적
- 토큰 사용량 기반 비용 계산
- 오류율 모니터링 및 알림 시스템

#### 2.5.3.2 디버깅 및 최적화

- 체인 실행 과정의 상세 추적
- 병목 구간 식별 및 성능 튜닝
- A/B 테스트를 위한 메트릭 수집

#### 2.5.3.3 사용자 경험 개선

- 실시간 진행 상황 표시
- 스트리밍 기반 응답 제공
- 오류 발생 시 사용자 친화적 메시지 제공

이러한 CallbackHandler 시스템은 LangChain 애플리케이션의 투명성과 제어성을 크게 향상시키며, 프로덕션 환경에서의 안정적인 운영을 가능하게 합니다.

# 3. 프롬프트(Prompt)

프롬프트는 사용자와 언어 모델 간의 대화에서 질문이나 요청의 형태로 제시되는 입력문입니다. 이는 모델이 어떤 유형의 응답을 제공할지 결정하는데 중요한 역할을 합니다.   

이번 항목에서는 랭체인에서 프롬프트를 구성할 때 사용하는 대표적인 도구인 프롬프트 템플릿을 다룹니다.

## 3.1 프롬프트 작성 원칙

모델이 최대한 정확하고 유용한 정보를 제공할 수 있도록 효과적인 프롬프트를 작성하는 것이 매우 중요합니다. 좋은 프롬프트를 만들기 위해서 다음과 같은 원칙을 고려합니다.

### 3.1.1 명확성과 구체성

- 질문은 명확하고 구체적이어야 합니다. 모호한 질문은 LLM 모델의 혼란을 초래할 수 있기 때문입니다.
- 예시: "다음 주 주식 시장에 영향을 줄 수 있는 예정된 이벤트들은 무엇일까요" 는 "주식 시장에 대해 알려주세요." 보다 더 구체적이고 명확한 질문입니다. 

### 3.1.2 배경 정보를 포함

- 모델이 문맥을 이해할 수 있도록 필요한 배경 정보를 제공하는 것이 좋습니다. 이는 환각(hallucination)이 발생할 위험을 낮추고, 관련성 높은 응답을 생성하는데 도움을 줍니다.
- 예시: "2020년 미국 대선의 결과를 바탕으로 현재 정치 사오항에 대한 분석을 해주세요"

### 3.1.3 간결함

- 핵심 정보에 초점을 맞추고, 불필요한 정보는 배제합니다. 프롬프트가 길어지면 모델이 덜 중요한 부분에 집중하거나 모델이 답변을 내뱉을 때에 상당한 영향을 받는 문제가 발생할 수 있습니다.

### 3.1.4 열린 질문 사용

- 열린 질문을 통해 모델이 자세하고 풍부한 답변을 제공하도록 유도합니다. 단순한 "예" 또는 "아니오"로 대답할 수 있는 질문 보다는 더 많은 정보를 제공하는 질문이 좋습니다.
- 예시: "신재생에너지에 대한 최신 연구 동향은 무엇인가요?"

### 3.1.5 명확한 목표 설정

- 얻고자 하는 정보나 결과의 유형을 정확하게 정의합니다. 이는 모델이 명확한 지침에 따라 응답을 생성하도록 돕습니다.
- 예시: "AI 윤리에 대한 문제점과 해결 방안을 요약하여 설명해주세요."

### 3.1.6 언어와 문체

- 대화의 맥락에 적합한 언어와 문체를 선택합니다. 이는 모델이 상황에 맞는 표현을 선택하는데 도움이 됩니다.
- 예시: 공식적인 보고서를 요청하는 경우, "XX 보고서에 대한 전문적인 요약을 부탁드립니다."와 같이 정중한 문체를 사용합니다.

---

## 3.2 프롬프트 템플릿(PromptTemplate)

PromptTemplate 은 단일 문장 또는 간단한 명령을 입력하여 단일 문장 또는 간단한 응답을 생성하는데 사용되는 프롬프트를 구성할 수 있는 문자열 템플릿입니다.

### 3.2.1 구성 요소

LLM 모델에 입력할 프롬프트를 구성할 때 `지시`, `예시`, `맥락`, `질문` 과 같은 다양한 구성요소들을 조합할 수 있습니다. 다양한 시나리오에서 필요한 구성요소들을 조합하여 적용합니다.

- 지시 : 언어 모델에게 어떤 작업을 수행하도록 요청하는 구체적인 지시.
- 예시 : 요청된 작업을 수행하는 방법에 대한 하나 이상의 예시
- 맥락 : 특정 작업을 수행하기 위한 추가적인 맥락
- 질문 : 어떤 답변을 요구하는 구체적인 질문

예시 : 제품 리뷰 요약

- 지시 : "아래 제공된 제품 리뷰를 요약해 주세요."
- 예시 : "예를 들어, '이 제품은 매우 사용하기 편리하며 배터리 수명이 길다' 라는 리뷰는 '사용 편리성과 긴 배터리 수명이 특징'으로 요약할 수 있습니다"
- 맥락 : "리뷰는 스마트워치에 대한 것이며, 사용자 경험에 초점을 맞추고 있습니다."
- 질문 : "이 리뷰를 바탕으로 스마트워치의 주요 장점을 두세 문장으로 요약해 주세요."

---

### 3.2.2 문자열 템플릿

다음 예제는 `langchain_core.prompts` 모듈의 `PromptTemplate` 클래스를 사용하여, "name" 과 "age" 라는 두 개의 변수를 포함하는 프롬프트 템플릿을 정의하고 있습니다. 이 템플릿을 이용하여 실제 입력값을 해당 위치에 채워 넣어 완성된 프롬프트를 생성하는 과정을 보여줍니다.

1. `PromptTemplate.from_template` 메서드를 사용하여 문자열 템플릿으로부터 `PromptTemplate` 인스턴스를 생성합니다. 이때, `template_text` 변수에 정의된 템플릿 문자열이 사용됩니다.

2. 생성된 `PromptTemplate` 인스턴스의 `format` 메서드를 사용하여, 실제 "name" 과 "age" 값으로 템플릿에 채워서 프롬프트를 구성합니다. 여기서는 `name="홍길동"`, `age=30`으로 지정하여 호출합니다.

3. 결과적으로, `filled_prompt` 변수에는 "안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다." 라는 완성된 프롬프트 문자열이 저장됩니다.

```python
from langchain_core.prompts import PromptTemplate

# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의
template_text = "안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다."

# PromptTemplate 인스턴스를 생성
prompt_template = PromptTemplate.from_template(template_text)

# 템플릿에 값을 채워서 프롬프트를 완성
filled_prompt = prompt_template.format(name="홍길동", age=30)

filled_prompt

```

```
실행 결과

안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.
```

---

### 3.2.3 프롬프트 템플릿 간의 결합

`PromptTemplate` 클래스는 문자열을 기반으로 프롬프트 템플릿을 생성하고, '+' 연산자를 사용하여 직접 결합할 수 있는 동작을 지원합니다. `PromptTemplate` 인스턴스 간의 직접적인 결합뿐만 아니라, 이들 인스턴스와 문자열로 이루어진 템플릿을 결합하여 새로운 `PromptTemaplte` 인스턴스를 생성하는 것도 가능합니다.

- 문자열 + 문자열
- PromptTemplate + PromptTemplate
- PromptTemplate + 문자열

```python
from langchain_core.prompts import PromptTemplate

# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의
template_text = "안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다."

# PromptTemplate 인스턴스를 생성
prompt_template = PromptTemplate.from_template(template_text)

# # 템플릿에 값을 채워서 프롬프트를 완성
# filled_prompt = prompt_template.format(name="홍길동", age=30)

# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)

combined_prompt = (
    prompt_template
    + PromptTemplate.from_template("\n\n아버지를 아버지라 부를 수 없습니다.")   
    +"\n\n{language}로 번역해주세요."
)

combined_prompt
```

```
실행 결과

PromptTemplate(input_variables=['age', 'language', 'name'], input_types={}, partial_variables={}, template='안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\n\n아버지를 아버지라 부를 수 없습니다.\n\n{language}로 번역해주세요.')
```

`format` 메소드를 사용하여 앞에서 생성한 템플릿의 매개변수에 입력 값을 지정합니다. LLM 에게 전달한 프롬프트가 완성되는데, 주어진 문장을 "영어로 번역해주세요." 라는 지시사항을 포함하고 있습니다.

```python
from langchain_core.prompts import PromptTemplate

# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의
template_text = "안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다."

# PromptTemplate 인스턴스를 생성
prompt_template = PromptTemplate.from_template(template_text)

# # 템플릿에 값을 채워서 프롬프트를 완성
# filled_prompt = prompt_template.format(name="홍길동", age=30)

# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)

combined_prompt = (
    prompt_template
    + PromptTemplate.from_template("\n\n아버지를 아버지라 부를 수 없습니다.")   
    +"\n\n{language}로 번역해주세요."
)

combined_prompt.format(name="홍길동", age=30, language="영어")

```

```
실행 결과


안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.

아버지를 아버지라 부를 수 없습니다.

영어로 번역해주세요.
```

`ChatOpenAI 인스턴스를 생성하여 프롬프트 텍스트를 전달하고, 모델의 출력을 StrOutputParser` 를 통해 문자열로 변환하는 LLM 체인을 구성합니다. `invoke` 메소드를 사용하여 파이프라인을 실행하고, 최종적으로 문자열 출력을 얻습니다. 모델의 응답은 프롬프트에 주어진 문장을 영어로 번역한 텍스트가 출력됩니다.

```python
from langchain_core.prompts import PromptTemplate

# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의
template_text = "안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다."

# PromptTemplate 인스턴스를 생성
prompt_template = PromptTemplate.from_template(template_text)

# # 템플릿에 값을 채워서 프롬프트를 완성
# filled_prompt = prompt_template.format(name="홍길동", age=30)

# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)

combined_prompt = (
    prompt_template
    + PromptTemplate.from_template("\n\n아버지를 아버지라 부를 수 없습니다.")   
    +"\n\n{language}로 번역해주세요."
)

combined_prompt.format(name="홍길동", age=30, language="영어")

llm = ChatOpenAI(model="gpt-4o-mini")
chain = combined_prompt | llm | StrOutputParser()
chain.invoke({"age":30, "language":"영어", "name":"홍길동"})
```

```
실행 결과

Hello, my name is Hong Gil-dong, and I am 30 years old. I cannot call my father "father."
```

---

## 3.3 챗 프롬프트 템플릿(ChatPromptTemplate)

ChatPromptTemplate 은 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는데 사용됩니다. 이는 대화형 메돌이나 챗봇 개발에 주로 사용됩니다. 입력은 여러 메시지를 원소로 갖는 리스트로 구성되며, 각 메시지는 역할(role)과 내용(content)로 구성됩니다.

---

### 3.3.1 Message 유형

- SystemMessage : 시스템의 기능을 설명합니다.
- HumanMessage : 사용자의 질문을 나타냅니다.
- AIMessage : AI 모델의 응답을 제공합니다.
- FunctionMessage : 특정 함수 호출의 결과를 나타냅니다.
- ToolMessage : 도구 호출의 결과를 나타냅니다.

---

### 3.3.2 2-튜플 형태의 메시지 리스트

`ChatPromptTempalte.from_messages` 메서드를 사용하여 메시지 리스트로부터 `ChatPromptTempalte` 인스턴스를 생성하는 방식은 대화형 프롬프트를 생성하는데 유용합니다. 이 메서드는 2-튜플 형태의 메시지 리스트를 입력 받아, 각 메시지의 역할(type)과 내용(content) 을 기반으로 프롬프트를 구성합니다.

다음 예시에서 `ChatPromptTempalte.from_messages` 메서드는 전달된 메시지들을 기반으로 프롬프트를 구성합니다. 그리고 `format_messages` 메서드는 사용자의 입력을 프롬프트에 동적으로 삽입하여, 최종적으로 대화형 상황을 반영한 메시지 리스트를 생성합니다. 시스템은 자신의 기능을 설명하고 사용자는 천문학 관련 질문을 합니다.

```python
# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)

from langchain_core.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 천문학 질문에 답변할 수 있습니다."),
    ("user", "{user_input}"),
    
])

messages = chat_prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")

messages
```

```
실행 결과

[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]
```

chat_prompt, llm, StrOutputParser() 를 순차적인 파이프라인으로 연결하여 구성된 chain 을 사용합니다. invoke 메소드를 호출하면 사용자 입력을 받아 언어 모델에 전달하고, 모델의 응답을 처리하여 최종 문자열 결과를 반환하는 과정을 자동화하여 수행합니다. 사용자는 천문학 관련 질문에 대한 언어 모델의 응답을 얻을 수 있습니다.

```python
from langchain_core.output_parsers import StrOutputParser

chain = chat_prompt | llm | StrOutputParser()

chain.invoke({"user_input": "태양계에서 가장 큰 행성은 무엇인가요?"})

```

---

### 3.3.3 MesagePromptTemplate 활용

다음 예제는 `SystemMessagePromptTemplate` 와 `HumanMeesagePromptTemplate` 를 사용하여 천문학 질문에 답변할 수 있는 시스템에 대한 대화형 프롬프트를 생성합니다. `ChatPromptTemplate.from_messages` 메소드를 통해 시스템 메시지와 사용자 메시지 템플릿을 포함하는 챗 프롬프트를 구성합니다. 이후, `chat_prompt.format_messages` 메서드를 사용하여 사용자의 질문을 포함한 메시지 리스트를 동적으로 생성합니다.

```python
# MessagePromptTemplate 활용

from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

chat_prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("이 시스템은 천문학 질문에 답변할 수 있습니다."),
        HumanMessagePromptTemplate.from_template("{user_input}"),
    ]
)

messages = chat_prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")

messages
```

```
실행 결과

[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]
```

이렇게 생성된 메시지 리스트는 대화형 인터페이스나 언어 모델과의 상호작용을 위한 입력으로 사용될 수 있습니다. 각 메시지는 role(메시지를 말하는 주체, 여기서는 system 또는 user)과 content(메시지 내용)속성을 포함합니다. 이 구조는 시스템과 사용자 간의 대화 흐름을 명확하게 표현하며, 언어 모델이 이를 기반으로 적절한 응답을 생성할 수 있도록 돕습니다.

---

## 3.4 Few-Shot Prompt

### 3.4.1 Few-Shot 예제 사용하기

Few-shot 학습은 사전학습된 LLM 에 새로운 작업을 시킬 때, 모델을 다시 훈련시키지 않고 몇 개의 예시(질문과 그에 대한 정답)를 프롬프트에 함께 제공하여, 모델이 그 패턴을 보고 동일한 방식으로 답변을 생성하도록 유도하는 기법입니다. 이 방법은 특히 모델이 생소한 작업이나 특정 형식을 따르는 작업을 할 때 성능을 향상시킬 수 있지만, 항상 큰 성능 향상을 보장하는 것은 아니며, 예시의 품질과 상황에 따라 효과가 달라집니다.

Few-shot 학습을 활용함으로써, 언어 모델은 주어진 예제들을 참고하여 더 정확하고 일관된 응답을 생성할 수 있습니다. 이는 특히 특정 도메인이나 형식의 질문에 대해 모델의 성능을 향상시키는데 효과적입니다.

---

#### 3.4.1.1 Few-shot 예제 포맷터 생성

먼저 Few-shot 예제를 포맷팅하기 위한 템플릿을 생성합니다. `PromptTemplate` 은 질문과 답변을 포함하는 간단한 구조를 가지고 있습니다. 이 템플릿은 각 예제를 일관된 형식으로 표현할 수 있게 해주어, 모델이 입력과 출력의 패턴을 쉽게 인식할 수 있도록 합니다.

```python
from langchain_core.prompts import PromptTemplate

example_prompt = PromptTemplate.from_template("질문: {question}\n{answer}")
```

---

#### 3.4.1.2 예제 시트 생성

다음은 모델이 참조할 수 있는 질문과 다변의 예제 세트를 생성합니다. 이 예제 세트는 다양한 주제(지구과학, 생물학, 수학)를 포함하고 있어, 모델이 여러 분야의 질문에 대응할 수 있도록 준비시킵니다. 각 답변은 간결하고 직접적이어서 모델이 유사한 스타일로 답변하도록 유도합니다.

```
examples = [
    {
        "question": "지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?",
        "answer": "지구 대기의 약 78%를 차지하는 질소입니다."
    },
    {
        "question": "광합성에 필요한 주요 요소들은 무엇인가요?",
        "answer": "광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다."
    },
    {
        "question": "피타고라스 정리를 설명해주세요.",
        "answer": "피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다."
    },
    {
        "question": "지구의 자전 주기는 얼마인가요?",
        "answer": "지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다."
    },
    {
        "question": "DNA의 기본 구조를 간단히 설명해주세요.",
        "answer": "DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다."
    },
    {
        "question": "원주율(π)의 정의는 무엇인가요?",
        "answer": "원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다."
    }
]

```

---

#### 3.4.1.3 FewShotPromptTemplate 생성

다음 코드는 Few-shot 프롬프트 템플릿을 생성하고, 새로운 질문에 대한 프롬프트를 생성합니다. `FewShotPromptTempalte` 은 예제들을 결합하고 새로운 입력을 추가하여 최종 프롬프트를 생성합니다. 이 방식은 모델에게 관련 예제들을 제공하면서 새로운 질문을 처리하도록 지시합니다.

```python
# FewShotPromptTemplate 예제

from langchain_core.prompts import FewShotPromptTemplate

from langchain_core.prompts import PromptTemplate

example_prompt = PromptTemplate.from_template("질문: {question}\n{answer}")

examples = [
    {
        "question": "지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?",
        "answer": "지구 대기의 약 78%를 차지하는 질소입니다."
    },
    {
        "question": "광합성에 필요한 주요 요소들은 무엇인가요?",
        "answer": "광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다."
    },
    {
        "question": "피타고라스 정리를 설명해주세요.",
        "answer": "피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다."
    },
    {
        "question": "지구의 자전 주기는 얼마인가요?",
        "answer": "지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다."
    },
    {
        "question": "DNA의 기본 구조를 간단히 설명해주세요.",
        "answer": "DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다."
    },
    {
        "question": "원주율(π)의 정의는 무엇인가요?",
        "answer": "원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다."
    }
]

# FewShotPromptTemplate 를 생성합니다.
prompt = FewShotPromptTemplate(
    examples = examples,              # 사용할 예제들
    example_prompt = example_prompt,  # 예제 포맷팅에 사용할 템플릿
    suffix="질문: {input}",           # 예제 뒤에 추가될 접미사
    input_variables=["input"],         # 입력 변수 지정
)

# 새로운 질문에 대한 프롬프트를 생성하고 출력합니다.

print(prompt.invoke({"input":"화성의 표면이 붉은 이유는 무엇인가요?"}))
```

```
실행 결과

질문: 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?
지구 대기의 약 78%를 차지하는 질소입니다.

질문: 광합성에 필요한 주요 요소들은 무엇인가요?
광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다.

질문: 피타고라스 정리를 설명해주세요.
피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다.

질문: 지구의 자전 주기는 얼마인가요?
지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다.

질문: DNA의 기본 구조를 간단히 설명해주세요.
DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다.

질문: 원주율(π)의 정의는 무엇인가요?
원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다.

질문: 화성의 표면이 붉은 이유는 무엇인가요?

```

---

#### 3.4.1.4 예제 선택기 사용하기

다음 코드는 의미적 유사성을 기반으로 가장 관련성 높은 예제를 선택합니다. `SemanticSimilarityExampleSelector`는 입력 질문과 가장 유사한 예제를 선택합니다. 이 방법은 더 관련성 높은 컨텍스트를 제공하여 모델의 응답 품질을 향상시킬 수 있습니다.

우선 예제를 실행하기 전에 `langchain_chroma` 설치 부터 진행을 해줍니다.

```python
!pip install langchain-chroma
```

설치가 완료된 후 아래 코드를 실행해봅니다.

```python
# 예제 선택기 사용

from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

# SemanticSimilarityExampleSelector 를 초기화합니다.

example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),
    Chroma,
    k=1
)

# 새로운 질문에 대해 가장 유사한 예제를 선택합니다.

question = "화성의 표면이 붉은 이유는 무엇인가요?"
selected_examples = example_selector.select_examples({"question":question})
print(f"입력과 가장 유사한 예제:{question}")

for example in selected_examples:
  print("\n")
  for k, v in example.items():
    print(f"{k}: {v}")
```

위 코드는 다음과 같이 Few-shot 학습에 사용하기 위한 예제들 중에서 입력 질문과 가장 유사한 예제를 선택하여 출력합니다. 이를 이용해 모델은 입력과 가장 관련성 높은 예제를 참조하여 응답을 생성할 수 있습니다.

```
실행 결과

입력과 가장 유사한 예제:화성의 표면이 붉은 이유는 무엇인가요?


answer: 지구 대기의 약 78%를 차지하는 질소입니다.
question: 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?
```

---

### 3.4.2 채팅 모델에서 Few-Shot 예제 사용하기

#### 3.4.2.1 고정 예제 사용하기

가장 기본적인 Few-shot 프롬프팅 기법은 고정된 예제를 사용하는 것입니다.

```python
# 기본적인 Few-shot 프롬프팅 사용법

from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate

# 예제 정의

examples = [
    {"input": "지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?", "output": "질소입니다."},
    {"input": "광합성에 필요한 주요 요소들은 무엇인가요?", "output": "빛, 이산화탄소, 물입니다."},
]

# 예제 프롬프트 템플릿 정의
example_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}"),
        ("ai", "{output}"),
    ]
)

# Few-shot 프롬프트 템플릿 생성
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

# 최종 프롬프트 템플릿 생성
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 과학과 수학에 대해 잘 아는 교육자입니다."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)

# 모델과 체인 생성
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini", temperature=0.0)
chain = final_prompt | model

# 모델에 질문하기
result = chain.invoke({"input": "지구의 자전 주기는 얼마인가요?"})
print(result.content)
```

실행 결과는 다음과 같습니다. 초반에 단순히 gpt-4o-mini 에게 답변을 물을 때와 달리 우리가 적용한 예제와 같이 간결하고 필요한 정보만 답변 해주는 것을 확인할 수 있습니다.

```
실행 결과

Few-shot 적용 전 대답

지구의 자전 주기는 약 24시간입니다. 더 정확하게는 평균적으로 23시간 56분 4초로, 이를 '항성일'이라고 합니다. 그러나 우리가 일상적으로 사용하는 24시간은 태양일로, 태양이 같은 위치에 다시 나타나기까지의 시간을 기준으로 합니다. 태양일은 항성일보다 약 4분 정도 더 긴 이유는 지구가 태양 주위를 공전하고 있기 때문입니다.

Few-shot 적용 후 대답

지구의 자전 주기는 약 24시간, 즉 1일입니다. 정확히는 약 23시간 56분 4초입니다. 이 시간을 기준으로 하루가 구성됩니다.

```

---

#### 3.4.2.2 동적 Few-shot 프롬프팅

이 방법은 예제 선택기(ExampleSeletor) 를 사용해서 입력에 따라 전체 예제 세트에서 가장 관련성 높은 예제만 선택하여 보여주는 방법입니다.

```python
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

# 더 많은 예제 추가
examples = [
    {"input": "지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?", "output": "질소입니다."},
    {"input": "광합성에 필요한 주요 요소들은 무엇인가요?", "output": "빛, 이산화탄소, 물입니다."},
    {"input": "피타고라스 정리를 설명해주세요.", "output": "직각삼각형에서 빗변의 제곱은 다른 두 변의 제곱의 합과 같습니다."},
    {"input": "DNA의 기본 구조를 간단히 설명해주세요.", "output": "DNA는 이중 나선 구조를 가진 핵산입니다."},
    {"input": "원주율(π)의 정의는 무엇인가요?", "output": "원의 둘레와 지름의 비율입니다."},
]

# 벡터 저장소 생성
to_vectorize = [" ".join(example.values()) for example in examples]
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)

# 예제 선택기 생성
example_selector = SemanticSimilarityExampleSelector(
    vectorstore=vectorstore,
    k=2,
)

# Few-shot 프롬프트 템플릿 생성
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_selector=example_selector,
    example_prompt=ChatPromptTemplate.from_messages(
        [("human", "{input}"), ("ai", "{output}")]
    ),
)

# 최종 프롬프트 템플릿 생성
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 과학과 수학에 대해 잘 아는 교육자입니다."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)

# 모델과 체인 생성
chain = final_prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0.0)

# ===== 여기부터: 입력과 유사한 예시 출력 =====
query = "태양계에서 가장 큰 행성은 무엇인가요?"

# 1) 선택된 예시를 직접 확인
selected = example_selector.select_examples({"input": query})
print("[선택된 예시들]")
for i, ex in enumerate(selected, 1):
    print(f"{i}. Q: {ex['input']}\n   A: {ex['output']}\n")

# 2) 모델 호출
result = chain.invoke({"input": query})
print("[모델 답변]")
print(result.content)

```

wiki docs 에 있는 코드에 선택된 예시를 출력하는 코드를 추가하고 실행 결과를 출력해보았습니다. 모델 답변은 문제가 없지만 선택된 예시들을 보면 예제에서 중복으로 2개가 뽑히는 것을 확인할 수 있습니다.

```
[선택된 예시들]
1. Q: 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?
   A: 질소입니다.

2. Q: 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?
   A: 질소입니다.

[모델 답변]
태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지름이 약 142,984킬로미터로, 태양계의 다른 행성들보다 훨씬 큽니다.
```

아래는 예제들 중에서 중복을 제거하는 코드를 추가한 코드입니다.

```python
# 동적 Few-shot 프롬프팅

from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings
import uuid # uuid 모듈 임포트

# 더 많은 예제 추가
examples = [
    {"input": "지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?", "output": "질소입니다."},
    {"input": "광합성에 필요한 주요 요소들은 무엇인가요?", "output": "빛, 이산화탄소, 물입니다."},
    {"input": "피타고라스 정리를 설명해주세요.", "output": "직각삼각형에서 빗변의 제곱은 다른 두 변의 제곱의 합과 같습니다."},
    {"input": "DNA의 기본 구조를 간단히 설명해주세요.", "output": "DNA는 이중 나선 구조를 가진 핵산입니다."},
    {"input": "원주율(π)의 정의는 무엇인가요?", "output": "원의 둘레와 지름의 비율입니다."},
]

# 벡터 저장소 생성

to_vectorize = [" ".join(example.values()) for example in examples]
ids = [f"ex-{i}" for i in range(len(examples))]  # 고정 id

embeddings = OpenAIEmbeddings()

# 매번 새 컬렉션 이름으로 만들면 중복 누적 방지
collection_name = f"fewshot-{uuid.uuid4().hex[:8]}"

vectorstore = Chroma.from_texts(
    texts=to_vectorize,
    embedding=embeddings,
    metadatas=examples,
    ids=ids,
    collection_name=collection_name,
    persist_directory=None,  # 메모리만 사용(중복 누적 방지)
)

# 예제 선택기 생성
example_selector = SemanticSimilarityExampleSelector(
    vectorstore=vectorstore,
    k=2,  # 최종 예시 개수
)

# Few-shot 프롬프트 템플릿 생성
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_selector=example_selector,
    example_prompt=ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{output}"),
        ]

    ),
)

# 최종 프롬프트 템플릿 생성
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 과학과 수학에 대해 잘 아는 교육자입니다."),
        few_shot_prompt,
        ("human", "{input}"),
    ]

)

# 모델과 체인 생성
chain = final_prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0.0)

# ===== 여기부터: 입력과 유사한 예시 출력 =====
query = "태양계에서 가장 큰 행성은 무엇인가요?"

# 1) 선택된 예시를 직접 확인
selected = example_selector.select_examples({"input": query})
print("[선택된 예시들]")
for i, ex in enumerate(selected, 1):
    print(f"{i}. Q: {ex['input']}\n   A: {ex['output']}\n")

# 2) 모델 호출
result = chain.invoke({"input": query})
print("[모델 답변]")
print(result.content)
```

실행 결과는 아래와 같습니다. 모델이 선택된 예제와 같이 아주 간결하게 답변을 해주는 것을 확인할 수 있습니다.

```
실행 결과

[선택된 예시들]
1. Q: 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?
   A: 질소입니다.

2. Q: 광합성에 필요한 주요 요소들은 무엇인가요?
   A: 빛, 이산화탄소, 물입니다.

[모델 답변]
태양계에서 가장 큰 행성은 목성(Jupiter)입니다.
```

전반적으로, Few-shot 학습 기법은 모델이 특정 답변 스타일을 학습하고 다양한 주제에 대해 일관된 형식의 응답을 생성하는 데 효과적임을 알 수 있습니다. 동적 예제 선택은 더 관련성 높은 컨텍스트를 제공하여 모델의 성능을 향상시킬 수 있지만, 예제 세트의 다양성과 임베딩 모델의 성능이 중요한 요소로 작용합니다. 특히나 마지막 코드에서 Few-shot 을 위한 예제에서 중복 추출이 되지 않도록 했을 때 확실히 Few-shot 이 적용되어 모델의 답변이 달라지는 것을 확인할 수 있었습니다.

---

# 4. LangChain 의 언어 모델(Model)

랭체인에서 지원하는 LLM 모델의 종류와 기본적인 사용방법을 다룹니다.

## 4.1 LangChain 모델 유형

랭체인 문서에 따르면 LLM 과 Chat Model 클래스는 각각 다른 형태의 입력과 출력을 다루는 언어 모델을 나타냅니다. 이 두 모델은 각기 다른 특성과 용도를 가지고 있어, 사용자의 요구사항에 맞게 선택하여 사용할 수 있습니다. 일반적으로 LLM 은 주로 단일 요청에 대한 복잡한 출력을 생성하는 데 적합한 반면, Chat Model 은 사용자와의 상호작용을 통한 연속적인 대화 관리에 더 적합합니다.

### 4.1.1 LLM

랭체인ㄴ에서 Large Language Models(LLMs) 는 핵심 구성 요소로, 다양한 LLM 제공 업체와의 상호작용을 위한 표준 인터페이스를 제공합니다. 이는 랭체인이 직접 LLM 을 제공하는 것이 아니라, OpenAI, Cohere, Hugging Face 등과 같은 다양한 LLM 제공 업체로부터 모델을 사용할 수 있게 하는 플랫폼 역할을 한다는 것을 의미합니다.

#### LLM 인터페이스 특징

- 표준화된 인터페이스 : 랭체인의 LLM 클래스는 사용자가 문자열을 입력으로 제공하면, 그에 대한 응답으로 문자열을 반환하는 표준화된 방식을 제공합니다. 이는 다양한 LLM 제공 업체 간의 호환성을 보장하며, 사용자는 복잡한 API 변환 작업 없이 여러 LLM 을 쉽게 탐색하고 사용할 수 있습니다.

- 다양한 LLM 제공 업체 지원 : 랭체인은 OpenAI 의 GPT 시리즈, Cohere 의 LLM, Hugging Face 의 Transformer 모델 등 다양한 LLM 제공 업체와의 통합을 지원합니다. 이를 통해 사용자는 자신의 요구 사항에 가장 적합한 모델을 선태하여 사용할 수 있습니다.

#### 사용 사례

- 다중 LLM 통합 : 개발자는 랭체인의 LLM 클래스를 사용하여 여러 LLM 제공 업체이ㅡ 모델을 하나의 애플리케이션 내에서 쉽게 통합할 수 있습니다. 예를 들어, 특정 작업에 대해 OpenAI 의 GPT 모델과 Cohere 의 모델을 비교 평가하고, 성능이 더 우수한 모델을 선택할 수 있습니다.

- 유연한 모델 전환 : 프로젝트 요구 사항이 변경되거나 다른 LLM 제공 업체에서 더 적합한 모델이 제공될 경우, 랭체인의 표준 인터페이스를 통해 쉽게 모델을 전환할 수 있습니다. 이는 개발자가 더 나은 성능, 비용 효율성, 또는 기능적 요구 사항을 충족하는 모델로 유연하게 이동할 수 있게 합니다.

---

### 4.1.2 Chat Model

Chat Model 클래스는 랭체인에서 중요한 구성 요소로, 대화형 메시지를 입력으로 사용하고 대화형 메시지를 출력으로 반환하는 특수화된 LLM 모델입니다. 이는 일반 텍스트를 사용하는 대신 대화의 맥락을 포함한 메시지를 처리하며, 이를 통해 보다 더 자연스러운 대화를 가능하게 합니다. 

#### Chat Model 인터페이스의 특징

- 대화형 입력과 출력 : Chat Model 은 대화의 연속성을 고려하여 입력된 메시지 리스트를 기반으로 적절한 응답 메시지를 생성합니다. 챗봇, 가상 비서, 고객 지원 시스템 등 대화 기반 서비스에 어울립니다.

- 다양한 모델 제공 업체와의 통합 : 랭체인은 OpenAI, Cohere, Hugging Face 등 다양한 모델 제공 업체와의 통합을 지원합니다. 이를 통해 개발자는 여러 소스의 Chat Models 를 조합하여 활용할 수 있습니다.

---

## 4.2 LangChain 의 LLM 모델 파라미터 설정

LLM 모델의 기본 속성 값을 조정하는 방법에 대해서 살펴봅니다. 모델의 속성에 해당하는 모델 파라미터는 LLM 의 출력을 조정하고 최적화하는데 사용되며, 모델이 생성하는 텍스트의 스타일, 길이, 정화도 등에 영향을 주게 됩니다. 사용하는 모델이나 플랫폼에 따라 세부 내용은 차이가 있습니다.

일반적으로 적용되는 주요 파라미터는 다음과 같습니다.

- Temperature : 생성된 텍스트의 다양성을 조정합니다. 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크며 다양하고 예측하기 어려운 출력을 생성합니다.

- Max Tokens : 생성할 최대 토큰 수를 지정합니다. 생성할 텍스트의 길이를 제한합니다.

- Top P (Top Probability) : 생성 과정에서 특정 확률 분포 내에서 상위 P% 토큰만을 고려하는 방식입니다. 이는 출력의 다양성을 조정하는데 도움이 됩니다. (0~1 사이의 값)

- Frequency Penalty (빈도 페널티) : 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률을 조정합니다. 값이 클수록 아직 텍스트에 등장하지 않으 새로운 단어의 사용이 장려 됩니다. (0~1 사이의 값)

- Presence Penalty (존재 페널티) : 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률을 조정합니다. 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어의 사용이 장려됩니다. (0~1 사이의 값)

- Stop Sequence (정지 시퀀스) : 특정 단어나 구절이 등장할 경우 생성을 멈추도록 설정합니다. 이는 출력을 특정 포인트에서 종료하고자 할 때 사용됩니다.

### 4.2.1 LLM 모델에 직접 파라미터를 전달

첫 번째 방법에서는 모델을 호출할 때 직접 파라미터를 전달하는 방식을 사용합니다. 이 방법은 모델 인스턴스 생성 시 또는 모델을 호출하는 시점에 파라미터를 인수로 제공함으로써, 해당 호출에 대해서만 파라미터 설정을 적용합니다. 이 방식의 장점은 특정한 호출에 대한 파라미터를 사용자가 직접 세밀하게 조정할 수 있다는 것입니다. 다양한 설정을 실험하거나 특정 요청에 대해 최적화된 응답을 생성하는데 유용합니다.

#### 모델 생성 단계

먼저 모델의 기본 파라미터(params)와 선택 파라미터(kwargs)를 정의하고, 모델 인스턴스를 생성할 때 초기값으로 설정하는 예제입니다.

```python
# 파라미터 직접 조정

from langchain_openai import ChatOpenAI

# 모델 파라미터 설정
params = {
    "temperature": 0.7,         # 생성된 텍스트의 다양성 조정
    "max_tokens": 100,          # 생성할 최대 토큰 수    
}

kwargs = {
    "frequency_penalty": 0.5,   # 이미 등장한 단어의 재등장 확률
    "presence_penalty": 0.5,    # 새로운 단어의 도입을 장려
    "stop": ["\n"]              # 정지 시퀀스 설정

}

# 모델 인스턴스를 생성할 때 설정
model = ChatOpenAI(model="gpt-4o-mini", **params, model_kwargs = kwargs)


# 모델 호출
question = "태양계에서 가장 큰 행성은 무엇인가요?"
response = model.invoke(input=question)

# 전체 응답 출력
print(response)
```

```
실행 결과

content='태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지구의 약 11배 정도 되는 직경을 가지고 있으며, 질량 또한 태양계의 다른 모든 행성을 합친 것보다 더 큽니다. 목성은 가스 거인이며, 두꺼운 대기와 강력한 자기장을 가지고 있습니다.' 
```

---

#### 모델 호출 단계

다음은 앞에서 생성한 모델 인스턴스를 이용하여, `invoke` 메소드를 사용하여 새로운 호출을 할 때 모델의 기본 파라미터(params) 를 설정하는 방법입니다. 실행 결과를 보면 최대 10 토큰의 길이로 답변이 생성됩니다.

```python
# 모델 파라미터 설정
params = {
    "temperature": 0.7,         # 생성된 텍스트의 다양성 조정
    "max_tokens": 10,          # 생성할 최대 토큰 수    
}

# 모델 인스턴스를 호출할 때 전달
response = model.invoke(input=question, **params)

# 문자열 출력
print(response.content)

```

```
실행 결과

태양계에서 가장 큰 행성은 목
```

---

### 4.2.2 LLM 모델 파라미터를 추가로 바인딩 (bind 메소드)

bind 메소드를 사용하여 모델 인스턴스에 파라미터를 추가로 제공할 수 있습니다. bind 메서드를 사용하는 방식의 장점은 특정 모델 설정을 기본값으로 사용하고자 할 때 유용하며, 특수한 상황에서 일부 파라미터를 다르게 적용하고 싶을 때 사용합니다. 기본적으로 일관된 파라미터 설정을 유지하면서 상황에 맞춰 유연한 대응이 가능합니다. 이를 통해 코드의 가독성과 재사용성을 높일 수 있습니다.

```python
# LLM 모델 파라미터를 추가로 바인딩

from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 천문학 질문에 답변할 수 있습니다."),
    ("user", "{user_input}"),
])

model = ChatOpenAI(model="gpt-4o-mini", max_tokens=100)

messages = prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")

before_answer = model.invoke(messages)

# # binding 이전 출력
print(before_answer)

# 모델 호출 시 추가적인 인수를 전달하기 위해 bind 메서드 사용 (응답의 최대 길이를 10 토큰으로 제한)
chain = prompt | model.bind(max_tokens=10)

after_answer = chain.invoke({"user_input": "태양계에서 가장 큰 행성은 무엇인가요?"})

# binding 이후 출력
print(after_answer)

```

```
실행 결과

content='가장 큰 행성은 목성입니다. 목성은 태양계에서 가장 크고 질량이 가장 큰 행성으로, 지름은 약 14만 2000km에 달합니다.'
content='태양계에서 가장 큰'
```

---

# 5. 출력 파서

랭체인에서 출력 파서 (Output Parser) 는 모델의 출력을 처리하고, 그 결과를 원하는 형식으로 변환하는 역할을 합니다. 출력 파서는 모델에서 반환된 원시 텍스트를 분석하고, 특정 정보를 추출하거나, 출력을 특정 형식으로 재구성하는데 사용됩니다.

이처럼 모델과 애플리케이션 간의 인터페이스 역할을 하며, 모델의 출력을 더 가치 있고 사용하기 쉬운 형태로 변환하는 데 핵심적인 역할을 합니다. 이를 통해 개발자는 모델의 원시 출력을 직접 처리한느 복잡성을 줄이고, 애플리케이션의 특정 요구 사항에 맞게 출력을 빠르게 조정할 수 있습니다.

출력 파서의 주요 기능

- 출력 포맷 변경 : 모델의 출력을 사용자가 원하는 형식으로 변환합니다. 예를 들어, JSON 형식으로 반환된 데이터를 테이블 형식으로 변환할 수 있습니다.
- 정보 추출 : 원시 텍스트 출력에서 필요한 정보(예:날짜, 이름, 위치 등)를 추출합니다. 이를 통해 복잡한 텍스트 데이터에서 구조화된 정보를 얻을 수 있습니다.
- 결과 정제: 모델 출력에서 불필요한 정보를 제거하거나, 응답을 더 명확하게 만드는 등의 후처리 작업을 수행합니다.
- 조건부 로직 적용 : 출력 데이터를 기반으로 특정 조건에 따라 다른 처리를 수행합니다. 예를 들어, 모델의 응답에 따라 사용자에게 추가 질문을 하거나, 다른 모델을 호출할 수 있습니다.

출력 파서의 사용 사례
1. 자연어 처리(NLP) 애플리케이션 : 질문 답변 시스템에서 정확한 답변만을 추출하여 사용자에게 제공합니다.
2. 데이터 분석 : 대량의 텍스트 데이터에서 특정 패턴이나 퉁계 정보를 추출하여 분석 보고서를 생성합니다.
3. 챗봇 개발 : 대화형 모델의 출력을 분석하여 사용자의 의도를 파악하고, 적절한 대화 흐름을 유지합니다.
4. 콘텐츠 생성 : 생성된 콘텐츠에서 중요한 정보를 요약하거나, 특정 형식(예 블로그 포스트, 뉴스 기사)에 맞게 콘텐츠를 재구성합니다.

## 5.1 CSV Parser

다음 예제는 랭체인의 `CommaSeparatedListOutputParser` 를 사용하여, 모델이 생성한 테긋트에서 쉼표(,)로 구분된 항목을 추출하여 리스트 형태로 정리하여 파싱하는 과정을 보여줍니다. 이처럼 생성 모델의 출력을 구조화된 데이터 형태로 변환할 수 있으며, 이후 처리 과정에서 데이터를 더 편리하게 활용할 수 있습니다.

먼저 `CommaSeparatedListOutputParser` 인스턴스를 생성합니다. `get_format_instructions` 메소드를 호출하여 모델에 전달할 포맷 지시사항을 얻습니다. 이 지시사항은 모델이 출력을 생성할 때 쉽표로 구분된 리스트 형식으로 변환하는 과정을 안내합니다.

```python
from langchain_core.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()

print(format_instructions)

```

```
실행 결과

Your response should be a list of comma separated values, eg: `foo, bar, baz`
```

`PromptTemplate` 를 사용하여 사용자 입력(subject)에 기반한 프롬프트를 생성합니다. 프롬프트에는 사용자가 지정한 주제(subject)와 모델에 전달한 포맷 지시사항(`format_instructions`)이 포함됩니다. 그리고 `prompt`, `model`, `output_parser` 를 파이프(`|`) 연산자를 사용하여 연결해 체인을 구성합니다. 이 체인은 사용자의 입력을 받아 프롬프트를 생성하고, 생성된 프롬프트를 모델에 전달한 후 모델의 출력을 파싱하는 과정을 순차적으로 수행합니다.

최종적으로 `invoke` 메소드를 사용하여 사용자의 입력(`{"subject": "popular Korean cuisine"}`)에 대한 모델의 응답을 쉼표로 구분된 리스트 형태로 받습니다.

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate(
    template="List five {subject}.\n{format_instructions}",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions},
)

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

chain = prompt | llm | output_parser

chain.invoke({"subject": "popular Korean cusine"})

```

```
실행 결과

['Bibimbap', 'Kimchi', 'Bulgogi', 'Japchae', 'Tteokbokki']
```

실행 결과를 보면 사용자가 주제에 대해 모델이 다섯 가지 인기 있는 한국 음식을 나열하고, `CommaSeparatedListOutputParser`를 통해 이 출력을 쉼표로 구분된 리스트 형태로 파싱한 결과를 얻을 수 있습니다.

---

## 5.2 JSON Parser

다음 예제는 랭체인의 `JsonOutputParser` 와 Pydantic 을 사용하여, 모델 출력을 JSON 형식으로 파싱하고 pYDANTIC 모델로 구조화하는 과정을 설명합니다. `JsonOutputParser`는 모델의 출력을 JSON으로 해석하고, 지정된 Pydantic 모델(`CusineRecipe`)에 맞게 데이터를 구조화하여 제공합니다.

먼저 자료구조를 의미하는 `CusineRecipe` 클래스를 Pydantic `BaseModel` 을 사용하여 정의합니다. `name` 필디는 요리의 이름을 나타내고, `recipe` 필드는 해당 요리를 만드는 레시피를 뜻합니다. 출력 파서로 `JsonOutputParser` 인스턴스를 생성하고, `pydantic_object` 매개변수로 `CusineRecipe` 클래스를 전달하여, 모델 출력을 해당 Pydantic 모델로 파싱하도록 설정합니다. 그리고 `output_parser.get_format_instructions()` 메소드를 호출하여 모델에 전달한 포맷 지시사항을 얻습니다. 이 지시사항은 모델이 출력을 생성할 때 JSON 형식을 따르도록 안내하는 역할을 합니다.

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

# 자료구조 정의 (pydantic)
class CusineRecipe(BaseModel):
    name: str = Field(description="name of a cusine")
    recipe: str = Field(description="recipe to cook the cusine")

# 출력 파서 정의
output_parser = JsonOutputParser(pydantic_object=CusineRecipe)

format_instructions = output_parser.get_format_instructions()

print(format_instructions)

```

```
실행 결과

The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
"""
{"properties": {"name": {"title": "Name", "description": "name of a cusine", "type": "string"}, "recipe": {"title": "Recipe", "description": "recipe to cook the cusine", "type": "string"}}, "required": ["name", "recipe"]}
"""
```

다음으로 모델에 입력으로 전달한 프롬프트를 구성하는 것입니다. `PromptTemplate`를 사용하여 사용자 질문(`query`)을 기반으로 한 프롬프트를 생성합니다. 프롬프트에는 사용자의 질문과 모델에 전달할 포맷 지시사항이 포함됩니다.

```python
# prompt 구성

prompt = PromptTemplate(
    template = "Answer the user query. \n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions":format_instructions},
    
)

print(prompt)
```

```
실행 결과

input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}\nthe object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{"properties": {"name": {"title": "Name", "description": "name of a cusine", "type": "string"}, "recipe": {"title": "Recipe", "description": "recipe to cook the cusine", "type": "string"}}, "required": ["name", "recipe"]}\n```'} template='Answer the user query. \n{format_instructions}\n{query}\n'
```

마지막으로 체인을 구성하고 호출하여 모델의 추력을 파싱하여 반환합니다. 이 체인은 사용자의 질문을 받아 프롬프트를 생성하고, 생성된 프롬프트를 모델에 전달한 후, 모델의 출력을 JSON 형식으로 파싱하고 `CusineRecipe` 객체로 변환하는 과정을 수행합니다. 체인을 호출하면 사용자의 질문에 대한 응답을 `CusineRecipe` 형태로 받게 됩니다. 이는 모델이 "Bibimbap" 요리법에 대한 정보를 JSOB 형식으로 제공하고, 이 정보가 `CusineRecipe` 객체로 구조화되는 것을 의미합니다.

```python
chain = prompt | model | output_parser

chain.invoke({"query": "Let me know how to cook Bibimbap"})
```

```
실행 결과

{'name': 'Bibimbap',
 'recipe': '1. Cook rice according to package instructions. 2. Prepare the vegetables: julienne carrots, slice cucumber, and sauté spinach and shiitake mushrooms. 3. In a pan, fry an egg sunny side up. 4. In a large bowl, place the cooked rice and arrange the vegetables on top of the rice. 5. Add the fried egg in the center. 6. Drizzle with gochuj'}
```

---

# 6. 메모리와 대화 관리

대화형 AI 애플리케이션에서 가장 중요한 기능 중 하나는 이전 대화 내용을 기억하고 맥락을 유지하는 것입니다. LangChain 은 다양한 메모리 관리 방식을 제공하여 상태를 가진(stateful) 대화형 애플리케이션을 구축할 수 있도록 도와줍니다.

## 6.1 메모리의 필요성과 개념

### 메모리가 없는 대화의 문제점

일반적인 LLM 은 상태를 저장하지 않기 때문에, 각 요청은 독립적으로 처리됩니다. 이는 다음과 같은 문제를 야기합니다

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# 첫 번째 질문
response1 = llm.invoke("안녕하세요, 제 이름은 민수입니다.")
print("응답 1:", response1.content)

# 두 번째 질문
response2 = llm.invoke("제 이름이 뭐였죠?")
print("응답 2:", response2.content)

```

```
실행 결과

응답 1: 안녕하세요 민수님! 만나서 반갑습니다. 어떤 도움이 필요하신가요?
응답 2: 죄송하지만 이전 대화 내용을 기억하지 못합니다. 성함을 알려주시면 감사하겠습니다.
```

### 메모리의 역할

메모리 시스템은 다음과 같은 기능을 제공합니다.

1. 대화 기록 저장 : 이전 메시지들을 저장하고 관리
2. 컨텍스트 유지: 대화의 흐름과 맥락 보전
3. 개인화 : 사용자별 대화 섹션 관리
4. 효율성 : 토큰 사용량 최적화를 위한 메모리 관리

## 6.2 RunnableWithMessageHistory 기본 사용법

LangChain v0.3+ 에서 `RunnableWithMessageHistory` 를 사용한 메모리 기능 구현이 가능합니다. 대화형 AI 시스템에서 이전 대화 내용을 기억하고 활용하는 핵심 매커니즘에 대해서 알아보겠습니다.

### RunnableWithMessageHistory 개요

LangChain 문서에 따르면, v0.3 이후부터는 LangGraph 기반의 메모리 시스템을 권장하지만, `RunnableWithMessageHistory` 는 여전히 간간한 채팅 애플리케이션에서 효과적으로 사용할 수 있으며 향후에도 지원될 예정입니다.

### 구현 단계별 분석

#### 1. 기본 컴포넌트 설정

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory

prompt = ChatPromptTemplate.from_messages([
    ("system", "당신은 천문학 전문가입니다. 사용자와 친근한 대화를 나누며 천문학 질문에 답변해주세요."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

```

핵심 유성 요소

- `MesagesPlaceholder(variable_name="history` : 대화 히스토리가 삽입될 위치를 지정합니다
- 이 플레이스홀더를 통해 이전 대화 내용이 프롬프트에 동적으로 포함됩니다.
- 시스템 메시지는 AI 의 역할과 성격을 정의합니다.

#### 2. 메모리 저장소 구성

```python
store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

```

메모리 관리 메커니즘

- `InMemoryChatMessageHistory` : 메모리 내에서 대화 기록을 저장합니다.
- `session_id` 를 통해 서로 다른 사용자나 대화 세션을 구분합니다.
- 각 세션마다 독립적인 대화 히스토리를 유지합니다.
- 프로세스가 종료되면 데이터가 사라지는 인메모리 방식입니다.

#### 3. 메모리 기능을 가진 체인 생성

```python

chain_with_history = RunnableWithMessageHistory(
    chain,                           # 기본 체인 (prompt | llm)
    get_session_history,             # 세션 히스토리를 가져오는 함수
    input_messages_key="question",   # 사용자 입력 키
    history_messages_key="history",  # 대화 기록 키
)

```

매개변수 설명

- `chain` : 기본 LLM 체인(프롬프트 + 모델)입니다.
- `get_session_history` : 세션별 히스토리를 관리하는 함수입니다.
- `input_messages_key` : 새로운 사용자 입력을 식별하는 키입니다.
- `history_messages_key` : 프롬프트 템플릿의 `MessagesPlaceholder` 와 일치해야 합니다.

### 대화 실행과 메모리 작동 과정

#### 대화 흐름 분석

```python
config = {"configurable": {"session_id": "astronomy_chat_1"}}

# 첫 번째 대화
response1 = chain_with_history.invoke(
    {"question": "안녕하세요, 저는 지구과학을 공부하는 학생입니다."},
    config=config
)

```

첫 번째 호출 시 내부 동작

  1. 새로운 `InMemoryChatMessageHistory` 인스턴스가 생성됩니다.
  2. 히스토리가 비어있으므로 ㅅ ㅣ스템 메시지와 사용자 메시지만 모델에 전달됩니다.
  3. AI 응답이 생성되고 히스토리에 저장됩니다.

```python
# 두 번째 대화 - 컨텍스트 유지
response2 = chain_with_history.invoke(
    {"question": "태양계에서 가장 큰 행성은 무엇인가요?"},
    config=config
)
```

두 번째 호출 시 내부 동작

  1. 같은 `session_id` 로 기존 히스토리를 조회합니다.
  2. 이전 대화(사용자 메시지 + AI 응답)가 `history` 플레이스홀더에 삽입됩니다.
  3. 새로운 질문과 함께 전체 컨텍스트가 모델에 전달됩니다.

```python
# 세 번째 대화 - 참조 관계 이해
response3 = chain_with_history.invoke(
    {"question": "그 행성의 위성은 몇 개나 되나요?"},
    config=config
)

```

세 번째 호출 시 컨텐스트 활용

- "그 행성"이라는 표현이 이전 대화의 "목성"을 참조한다는 것을 AI 가 이해합니다.
- 모든 이전 대화 히스토리가 컨텍스트로 제공되어 자연스러운 대화가 가능합니다.

---

전체 코드는 다음과 같습니다.

```python
# RunnableWithMessageHistory

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([
    ("system", "당신은 천문학 전문가입니다. 사용자와 친근한 대화를 나누며 천문학 질문에 답변해주세요."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

chain = prompt | llm | StrOutputParser()

store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

chain_with_history = RunnableWithMessageHistory(
    chain,                           # 기본 체인 (prompt | llm)
    get_session_history,             # 세션 히스토리를 가져오는 함수
    input_messages_key="question",   # 사용자 입력 키
    history_messages_key="history",  # 대화 기록 키
)

config = {"configurable": {"session_id": "astronomy_chat_1"}}

# 첫 번째 대화
response1 = chain_with_history.invoke(
    {"question": "안녕하세요, 저는 지구과학을 공부하는 학생입니다."},
    config=config
)

print(f"response1 : {response1}")

# 두 번째 대화 - 컨텍스트 유지
response2 = chain_with_history.invoke(
    {"question": "태양계에서 가장 큰 행성은 무엇인가요?"},
    config=config
)

print(f"response2 : {response2}")

# 세 번째 대화 - 참조 관계 이해
response3 = chain_with_history.invoke(
    {"question": "그 행성의 위성은 몇 개나 되나요?"},
    config=config
)

print(f"response3 : {response3}")

```

```
실행 결과

response1 : 안녕하세요! 지구과학을 공부하신다니 멋지네요. 천문학에 관련된 질문이나 궁금한 점이 있다면 언제든지 물어보세요. 함께 이야기해봐요!
response2 : 태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지름이 약 139,820킬로미터로, 지구보다 11배나 더 큽니다. 주로 수소와 헬륨으로 이루어져 있으며, 강력한 자기장과 많은 위성을 가지고 있는 특징이 있죠. 궁금한 점이 더 있으신가요?
response3 : 목성은 현재까지 확인된 위성이 80개 이상 있습니다. 그 중 가장 큰 네 개의 위성인 갈릴레오 위성(유로파, 이오, 가니메데, 칼리스토)은 특히 유명한데, 갈릴레오 갈릴레이가 1610년에 발견했습니다. 이 위성들은 각각 독특한 특징을 가지고 있어 흥미롭습니다. 예를 들어, 유로파는 얼음으로 덮인 표면 아래에 액체 상태의 바다가 있을 것으로 예상되어 많은 과학자들의 관심을 끌고 있습니다. 도움이 필요하시면 언제든지 말씀해 주세요!
```

### 실제 메시지 구조 예지

첫 번째 호출 시 모델에 전달되는 메시지

```
[
    SystemMessage(content="당신은 천문학 전문가입니다..."),
    HumanMessage(content="안녕하세요, 저는 지구과학을 공부하는 학생입니다.")
]

```

세 번째 호출 시 모델에 전달되는 메시지

```
[
    SystemMessage(content="당신은 천문학 전문가입니다..."),
    HumanMessage(content="안녕하세요, 저는 지구과학을 공부하는 학생입니다."),
    AIMessage(content="안녕하세요! 지구과학을 공부하고 계시는군요..."),
    HumanMessage(content="태양계에서 가장 큰 행성은 무엇인가요?"),
    AIMessage(content="태양계에서 가장 큰 행성은 목성(Jupiter)입니다!..."),
    HumanMessage(content="그 행성의 위성은 몇 개나 되나요?")
]

```

### 주요 특징과 장점

1. 자동 컨텍스트 관리
   이전 모든 대화 내용이 자동으로 현재 프롬프트에 포함되어 연속적인 대화가 가능합니다.

2. 세션 기반 관리
  서로 다른 `seesion_id`를 사용하여 여러  사용자나 대화 스레드를 독립적으로 관리할 수 있습니다.

3. 간편한 구현
  복잡한 메모리 관리 로직 없이도 대화형 AI 를 구현할 수 있습니다.

---

### 프로덕션 환경에서의 고려사항

1. 영구 저장소 필요성
  `InMemoryChatMessageHistory` 대신 ㅔㄷ이터베이스 기반 저장소를 사용해야 합니다.

2. 토큰 제한 관리
  대화가 길어질수록 토큰 사용량이 증가하므로 히스토리 트리밍이 필요합니다.

3. LangGraph 마이그레이션
  더 복잡한 기능이 필요한 경우 LangGraph 의 메모리 시스템으로 전환을 고려해야 합니다.

  이 구현 방식은 LangChain 에서 대화형 AI 의 메모리 기능을 이해하고 구축하는데 효과적인 방법입니다.

# 7. 마치며

wiki docs 의 "랭체인(LangChain) 입문부터 응용까지" 의 LangChain 기초 부분에 있는 것들을 포스트에 작성하면서 LangChain 의 기초에 대한 공부를 진행해 보았습니다. 직접 따라해보면서 LangChain 이 대략적으로 무엇인지 알게 되었고, 또 wiki docs 에 있는 코드들 중에는 실행이 안되거나, 설명이 부족하거나, 실행결과가 다른 것들이 많았습니다. 그래서 기존 작성된 코드에 직접 수정을 하다보니 코드가 눈과 손에 자연스레 익숙해지기도 했습니다. 이제 다음부턴 LangChain 을 이용한 RAG 에 대한 공부를 진행하고자 합니다. 
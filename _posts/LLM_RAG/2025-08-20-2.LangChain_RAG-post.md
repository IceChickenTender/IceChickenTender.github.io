---
title: "[LLM/RAG] LangChain 을 이용한 RAG 기초"
categories:
  - LLM/RAG
  - LangChain

tags:
  - LLM/RAG
  - LangChain
  
use_math: true  
toc: true
toc_sticky: true
toc_label: "LangChain 을 이용한 RAG 기초"
---

이번엔 wiki docs 의 "랭체인(LangChain) 입문부터 응용까지" 를 참고해 LangChain 을 이용해 RAG 에 대해서 공부하고 구현까지 해보도록 하겠습니다.

# 1. RAG 개요

RAG(Retrieval-Augmented Generation) 기법은 기존의 대규모 언어 모델(LLM) 을 확장하여, 주어진 컨텍스트나 질문에 대해 더욱 정확하고 풍부한 정보를 제공하는 방법입니다. 모델이 학습 데이터에 포함되지 않은 외부 데이터를 실시간으로 검색(retrieval)하고, 이를 바탕으로 답변을 생성(generation) 하는 과정을 포함합니다. 특히 환각(생성된 내용이 사실이 아닌 것으로 오인되는 현상)을 방지하고, 모델이 최신 정보를 반영하거나 더 넓은 지식을 활용할 수 있게 합니다.

## 1.1 RAG 모델의 기본 구조

검색 단계(Retrieval Phase) : 사용자의 질문이나 컨텍스트를 입력으로 받아서, 이와 관련된 외부 데이터를 검색하는 단계입니다. 이 때 검색 엔진이나 데이터베이스 등 다양한 소스에서 필요한 정보를 찾아냅니다. 검색된 데이터는 질문에 대한 답변을 생성하는데 적합하고 상세한 정보를 포함하는 것을 목표로 합니다.

생성 단계(Generation Phase) : 검색된 데이터를 기반으로 LLM 모델이 사용자의 질문에 답변을 생성하는 단계입니다. 이 단계에서 모델은 검색된 정보와 기존의 지식을 결합하여, 주어진 질문에 대한 답변을 생성합니다.

## 1.2 RAG 모델의 장점

- 풍부한 정보 제공 : RAG 모델은 검색을 통해 얻은 외부 데이터를 활용하여, 보다 구체적이고 풍부한 정보를 제공할 수 있습니다.
- 실시간 정보 반영 : 최신 데이터를 검색하여 반영함으로써, 모델이 실시간으로 변화하는 정보에 대응할 수 있습니다.
- 환각 방지 : 검색을 통해 실제 데이터에 기반한 답변을 생성함으로써, 환각 현상이 발생할 위험을 줄이고 정확도를 높일 수 있습니다.

## 1.3 RAG 파이프라인 구조

RAG 파이프라인은 기존의 언어 모델에 검색 기능을 추가하여, 주어진 질문이나 문제에 대해 더 정확하고 풍부한 정보를 기반으로 답변을 생성할 수 있게 해줍니다. 이 파이프라인은 크게 데이터 로드, 텍스트 분할, 인덱싱, 검색, 생성의 다섯 단계로 구성됩니다.

### 1. 데이터 로드

RAG 에 사용할 데이터를 불러오는 단계입니다. 외부 데이터 소스에서 정보를 수집하고, 필요한 형식으로 변환하여 시스템에 로드합니다. 예를 들면 공개 데이터셋, 웹 크롤링을 통해 얻은 데이터, 또는 사전에 정리된 자료일 수 있습니다. 가져온 데이터는 검색에 사용될 지식이나 정보를 담고 있어야 합니다.

다음 예제는 `langchain_community.document_loaders` 모듈에서 `WebBaseLoader` 클래스를 사용하여 특정 웹페이지(위키피디아 정책과 지침)의 데이터를 가져오는 방법을 보여줍니다. 웹 크롤링을 통해 웹페이지의 텍스트 데이터를 추출하여 `Document` 객체의 리스트로 변환합니다.

우선 코드를 실행하기 위해선 추가적으로 langchain-community 라이브러리 설치가 필요합니다. 코랩에서 코드 실행 전 아래를 먼저 실행해주시길 바랍니다.

```python
!pip install langchain-community
```

```python
# Data Loader - 웹페이지 데이터 가져오기
from langchain_community.document_loaders import WebBaseLoader

# 위키피디아 정책과 지침
url = 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'
loader = WebBaseLoader(url)

# 웹페이지 텍스트 -> Documents
docs = loader.load()

print(len(docs))
print(len(docs[0].page_content))
print(docs[0].page_content[5000:6000])

```

```
실행 결과

1
13255
수단을 이용해야 합니다. 특히 정책 문서에 명시된 원칙을 지키지 않는 것은 대부분의 경우 다른 사용자에게 받아들여지지 않습니다 (다른 분들에게 예외 상황임을 설득할 수 있다면 가능하기는 하지만요). 이는 당신을 포함해서 편집자 개개인이 정책과 지침을 직접 집행 및 적용한다는 것을 의미합니다.
특정 사용자가 명백히 정책에 반하는 행동을 하거나 정책과 상충되는 방식으로 지침을 어기는 경우, 특히 의도적이고 지속적으로 그런 행위를 하는 경우 해당 사용자는 관리자의 제재 조치로 일시적, 혹은 영구적으로 편집이 차단될 수 있습니다. 영어판을 비롯한 타 언어판에서는 일반적인 분쟁 해결 절차로 끝낼 수 없는 사안은 중재위원회가 개입하기도 합니다.

문서 내용
정책과 지침의 문서 내용은 처음 읽는 사용자라도 원칙과 규범을 잘 이해할 수 있도록 다음 원칙을 지켜야 합니다.

명확하게 작성하세요. 소수만 알아듣거나 준법률적인 단어, 혹은 지나치게 단순한 표현은 피해야 합니다. 명확하고, 직접적이고, 모호하지 않고, 구체적으로 작성하세요. 지나치게 상투적인 표현이나 일반론은 피하세요. 지침, 도움말 문서 및 기타 정보문 문서에서도 "해야 합니다" 혹은 "하지 말아야 합니다" 같이 직접적인 표현을 굳이 꺼릴 필요는 없습니다.
가능한 간결하게, 너무 단순하지는 않게. 정책이 중언부언하면 오해를 부릅니다. 불필요한 말은 생략하세요. 직접적이고 간결한 설명이 마구잡이식 예시 나열보다 더 이해하기 쉽습니다. 각주나 관련 문서 링크를 이용하여 더 상세히 설명할 수도 있습니다.
규칙을 만든 의도를 강조하세요. 사용자들이 상식대로 행동하리라 기대하세요. 정책의 의도가 명료하다면, 추가 설명은 필요 없죠. 즉 규칙을 '어떻게' 지키는지와 더불어 '왜' 지켜야 하는지 확실하게 밝혀야 합니다.
범위는 분명히, 중복은 피하기. 되도록 앞부분에서 정책 및 지침의 목적과 범위를 분명하게 밝혀야 합니다. 독자 대부분은 도입부 초반만 읽고 나가버리니까요. 각 정책 문서의 내용은 해당 정책의 범위 내에서만 서
```

실행 결과에서 변환된 문서 객체가 유일하게 한 개만 존재하고, 문자열의 글자 개수는 13255 글자임을 알 수 있습니다.

---

### 2. 텍스트 분할(Text Split)

불러온 데이터를 작은 크기의 단위(Chunk)로 분할하는 과정입니다. 자연어 처리(NLP) 기술을 활용하여 큰 문서를 처리가 쉽도록 문단, 문장 또는 구 단위로 나누는 작업입니다. 검색 효율성을 높이기 위한 중요한 과정입니다.

다음 코드는 `RecursiveCharacterTextSplitter` 라는 텍스트 분할 도구를 사용하고 있습니다. (이 도구에 대해서는 Text Splitter 챕터에서 상세하게 다룰 예정입니다.) 간략하게 설명하면 13255 개의 문자로 이루어진 긴 문장을 최대 1000글자 단위로 분할하는 것입니다. 200 글자는 각 분할 마다 겹치게 하여 문맥이 잘려나가지 않고 유지되게 합니다.

LLM 모델이나 API 의 입력 크기에 대한 제한이 있기 때문에, 제한에 걸리지 않도록 적정한 크기로 텍스트의 길이를 줄일 필요가 있습니다. 그리고 프롬프트가 지나치게 길어질 경우 중요한 정보가 상대적으로 희석되는 문제가 있을 수도 있습니다. 따라서, 적정한 크기로 텍스트를 분할하는 과정이 필요합니다.

```python
# Text Split (Documents -> small chunks: Documents)
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

print(len(splits))
print(splits[10])

```

```
실행 결과

19
page_content='제안과 채택
 백:아님 § 관료주의  문서를 참고하십시오. 단축백:제안
제안 문서란 정책과 지침으로 채택하자고 의견을 묻는 문서이나 아직 위키백과 내에 받아들여지는 원칙으로 확립되지는 않은 문서입니다. {{제안}} 틀을 붙여 공동체 내에서 정책이나 지침으로 채택할 지 의견을 물을 수 있습니다. 제안 문서는 정책과 지침이 아니므로 아무리 실제 있는 정책이나 지침을 요약하거나 인용해서 다른 문서에 쓴다고 해도 함부로 정책이나 지침 틀을 붙여서는 안 됩니다.
'제안'은 완전 새로운 원칙이라기보다, 기존의 불문율이나 토론 총의의 문서를 통한 구체화에 가깝습니다. 많은 사람들이 쉽게 제안을 받아들이도록 하기 위해서는, 기초적인 원칙을 우선 정하고 기본 틀을 짜야 합니다. 정책과 지침의 기본 원칙은 "왜 지켜야 하는가?", "어떻게 지켜야 하는가?" 두 가지입니다. 특정 원칙을 정책이나 지침으로 확립하기 위해서는 우선 저 두 가지 물음에 성실하게 답하는 제안 문서를 작성해야 합니다.
좋은 아이디어를 싣기 위해 사랑방이나 관련 위키프로젝트에 도움을 구해 피드백을 요청할 수 있습니다. 이 과정에서 공동체가 어느 정도 받아들일 수 있는 원칙이 구체화됩니다. 많은 이와의 토론을 통해 공감대가 형성되고 제안을 개선할 수 있습니다.
정책이나 지침은 위키백과 내의 모든 편집자들에게 적용되는 원칙이므로 높은 수준의 총의가 요구됩니다. 제안 문서가 잘 짜여졌고 충분히 논의되었다면, 더 많은 공동체의 편집자와 논의를 하기 위해 승격 제안을 올려야 합니다. 제안 문서 맨 위에 {{제안}}을 붙여 제안 안건임을 알려주고, 토론 문서에 {{의견 요청}}을 붙인 뒤 채택 제안에 관한 토론 문단을 새로 만들면 됩니다. 많은 편집자들에게 알리기 위해 관련 내용을 {{위키백과 소식}}에 올리고 사랑방에 이를 공지해야 하며, 합의가 있을 경우 미디어위키의 sitenotice(위키백과 최상단에 노출되는 구역)에 공지할 수도 있습니다.' metadata={'source': 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8', 'title': '위키백과:정책과 지침 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}
```

`page_content` 속성에는 분할된 텍스트 조각이 들어 있습니다. `metadata` 속성을 통해 원본 문서의 정보를 포함하는 메타데이터를 출력하여 확인합니다.

---

### 3. 인덱싱(Indexing)

분할된 텍스트를 검색 가능한 형태로 만드는 단계입니다. 인덱싱은 검색 시간을 단축시키고, 검색의 정확도를 높이는 데 중요한 역할을 합니다. LangChain 라이브러리를 사용하여 텍스트를 임베딩으로 변환하고, 이를 저장한 후, 저장된 임베딩을 기반으로 유사성 검색을 수행하는 과정을 보여줍니다.

간략하게 설명하면 OpenAI 의 임베딩 모델을 사용하여 텍스트를 벡터로 변환하고, 이를 Chroma 벡터 저장소에 저장합니다. `vectorstore.similarity_search` 메소드는 주어진 쿼리 문자열("격하 과정에 대해서 설명해주세요.")에 대해 저장된 문서들 중에서 가장 유사한 문서들을 찾아냅니다. 이 때 유사성은 임베딩 간의 거리(또는 유사도)로 계산됩니다. 아래 코드를 실행하면 4개의 문서가 반환되는데, 가장 유사도가 높은 첫 번째 문서를 출력하여 확인합니다.

```python
# Indexing (Texts -> Embedding -> Store)
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(documents=splits,
                                    embedding=OpenAIEmbeddings())

docs = vectorstore.similarity_search("격하 과정에 대해서 설명해주세요.")
print(len(docs))
print(docs[0].page_content)

```

```
실행 결과

4
격하
특정 정책이나 지침이 편집 관행이나 공동체 규범이 바뀌며 쓸모없어질 수 있고, 다른 문서가 개선되어 내용이 중복될 수 있으며, 불필요한 내용이 증식할 수도 있습니다. 이 경우 편집자들은 정책을 지침으로 격하하거나, 정책 또는 지침을 보충 설명, 정보문, 수필 또는 중단 문서로 격하할 것을 제안할 수 있습니다. 
격하 과정은 채택 과정과 비슷합니다. 일반적으로 토론 문서 내 논의가 시작되고 프로젝트 문서 상단에 {{새로운 토론|문단=진행 중인 토론 문단}} 틀을 붙여 공동체의 참여를 요청합니다. 논의가 충분히 이루어진 후, 제3의 편집자가 토론을 종료하고 평가한 후 상태 변경 총의가 형성되었는지 판단해야 합니다. 폐지된 정책이나 지침은 최상단에 {{중단}} 틀을 붙여 더 이상 사용하지 않는 정책/지침임을 알립니다.
소수의 공동체 인원만 지지하는 수필, 정보문 및 기타 비공식 문서는 일반적으로 주된 작성자의 사용자 이름공간으로 이동합니다. 이러한 논의는 일반적으로 해당 문서의 토론란에서 이루어지며, 간혹 위키백과:의견 요청을 통해 처리되기도 합니다.

같이 보기
위키백과:위키백과의 정책과 지침 목록
위키백과:의견 요청
수필

위키백과:제품, 절차, 정책
위키백과:위키백과 공동체의 기대와 규범
기타 링크
```

---

### 4. 검색(Retrieval)

사용자의 질문이나 주어진 컨텍스트에 가장 관련된 정보를 찾아내는 과정입니다. 사용자의 입력을 바탕으로 쿼리를 생성하고, 인덱싱된 데이터에서 가장 관련성 높은 정보를 검색합니다. LangChain 의 Retriever 메소드를 사용합니다.

---

### 5. 생성(Generation)

검색된 정보를 바탕으로 사용자의 질문에 답변을 생성하는 최종 단계입니다. LLM 모델에 검색 결과와 함께 사용자의 입력을 전달합니다. 모델은 사전 학습된 지식과 검색 결과를 결합하여 주어진 질문에 가장 적절한 답변을 생성합니다.

검색과 생성 단계를 수행하는 다음 코드를 살펴보겠습니다. `vectorstore.as_retriever()` 메소드는 `Chroma` 벡터 스토어를 검색기로 사용하여 사용자의 질문과 관련된 문서를 검색합니다. `format_docs` 함수는 검색된 문서들을 하나의 문자열로 반환합니다. RAG 체인을 구성하고, 주어진 질문에 대한 답변을 생성합니다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate

# Prompt

template = '''Answer the question based only on the following context:
{context}

Question: {question}
'''

prompt = ChatPromptTemplate.from_template(template)

# LLM
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# RAG
retriever = vectorstore.as_retriever()

print("연관 문서 출력")
# Print relevant documents
for doc in retriever.get_relevant_documents("격하 과정에 대해서 설명해주세요."):
    print(doc.page_content)
print("\n\n")

# Combine Documents
def format_docs(docs):
  return '\n\n'.join(doc.page_content for doc in docs)

# 일반 답변 연결
simple_sentence_chain = model | StrOutputParser()

simple_sentence_response = simple_sentence_chain.invoke("격하 과정에 대해서 설명해주세요.")

print("단문으로 LLM 에 요청했을 때의 답변")
print(simple_sentence_response)
print("\n\n")

# RAG Chain 연결
rag_chain = (
    {'context' : retriever | format_docs, 'question' : RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

rag_chain_response = rag_chain.invoke("격하 과정에 대해서 설명해 주세요.")

print("RAG 를 적용한 답변")
print(rag_chain_response)
```

```
실행 결과

연관 문서 출력
격하
특정 정책이나 지침이 편집 관행이나 공동체 규범이 바뀌며 쓸모없어질 수 있고, 다른 문서가 개선되어 내용이 중복될 수 있으며, 불필요한 내용이 증식할 수도 있습니다. 이 경우 편집자들은 정책을 지침으로 격하하거나, 정책 또는 지침을 보충 설명, 정보문, 수필 또는 중단 문서로 격하할 것을 제안할 수 있습니다. 
격하 과정은 채택 과정과 비슷합니다. 일반적으로 토론 문서 내 논의가 시작되고 프로젝트 문서 상단에 {{새로운 토론|문단=진행 중인 토론 문단}} 틀을 붙여 공동체의 참여를 요청합니다. 논의가 충분히 이루어진 후, 제3의 편집자가 토론을 종료하고 평가한 후 상태 변경 총의가 형성되었는지 판단해야 합니다. 폐지된 정책이나 지침은 최상단에 {{중단}} 틀을 붙여 더 이상 사용하지 않는 정책/지침임을 알립니다.
소수의 공동체 인원만 지지하는 수필, 정보문 및 기타 비공식 문서는 일반적으로 주된 작성자의 사용자 이름공간으로 이동합니다. 이러한 논의는 일반적으로 해당 문서의 토론란에서 이루어지며, 간혹 위키백과:의견 요청을 통해 처리되기도 합니다.

같이 보기
위키백과:위키백과의 정책과 지침 목록
위키백과:의견 요청
수필

위키백과:제품, 절차, 정책
위키백과:위키백과 공동체의 기대와 규범
기타 링크
채택 과정
한국어 위키백과에서 오랫동안 확립되어 온 정책과 지침의 대다수는, 영어 위키백과 설립 시 토대가 된 원칙에서 발전된 것들입니다. 물론 타 언어 위키백과의 원칙을 가져오는 것 말고도, 정책과 지침을 일반적인 문제와 문서 훼손 행위의 대응책으로 한국어 위키백과 내 공동체에서 자발적으로 세우기도 했습니다. 정책과 지침 대부분은 전례 없이 바로 받아들여지기보다, 공동체의 강력한 지지를 바탕으로 세워집니다. 정책과 지침의 제정 방법으로는 제안을 통한 수립, 기존의 수필 또는 지침의 정책화, 기존의 정책과 지침의 분할 또는 합병을 통한 재구성 등의 여러 방법이 있습니다.
정책과 지침이 아닌 위키백과 내 운영과 관련된 문서에는 {{수필}}, {{정보문}}, {{위키백과 사용서}} 등을 붙여 구분해야 합니다.
현재 정책이나 지침으로 제안된 문서는 분류:위키백과 제안에 모여 있습니다. 총의를 통해 채택이 거부된 제안은 분류:위키백과 거부된 제안을 참조하세요. 여러분들의 참여를 환영합니다.
명확하게 작성하세요. 소수만 알아듣거나 준법률적인 단어, 혹은 지나치게 단순한 표현은 피해야 합니다. 명확하고, 직접적이고, 모호하지 않고, 구체적으로 작성하세요. 지나치게 상투적인 표현이나 일반론은 피하세요. 지침, 도움말 문서 및 기타 정보문 문서에서도 "해야 합니다" 혹은 "하지 말아야 합니다" 같이 직접적인 표현을 굳이 꺼릴 필요는 없습니다.
가능한 간결하게, 너무 단순하지는 않게. 정책이 중언부언하면 오해를 부릅니다. 불필요한 말은 생략하세요. 직접적이고 간결한 설명이 마구잡이식 예시 나열보다 더 이해하기 쉽습니다. 각주나 관련 문서 링크를 이용하여 더 상세히 설명할 수도 있습니다.
규칙을 만든 의도를 강조하세요. 사용자들이 상식대로 행동하리라 기대하세요. 정책의 의도가 명료하다면, 추가 설명은 필요 없죠. 즉 규칙을 '어떻게' 지키는지와 더불어 '왜' 지켜야 하는지 확실하게 밝혀야 합니다.
범위는 분명히, 중복은 피하기. 되도록 앞부분에서 정책 및 지침의 목적과 범위를 분명하게 밝혀야 합니다. 독자 대부분은 도입부 초반만 읽고 나가버리니까요. 각 정책 문서의 내용은 해당 정책의 범위 내에서만 서술하세요. 한 도움말 문서가 다루는 범위가 다른 것과 겹친다면, 중복되는 부분을 최소화하세요. 한 정책 문서에서 다른 정책을 설명할 시, 최대한 간단명료하게만 작성해야 합니다.
과도한 링크는 피하세요. 타 정책, 지침, 수필, 기타 문서는 설명 또는 문맥상 필요할 시에만 링크해야 합니다. 다른 도움말 문서로 링크하는 것은 의도였든 아니든 해당 문서에 지나친 권위를 부여할 수 있습니다. 링크해도 되는 것과 안 되는 것을 명확히 해 주세요.
차후 공지가 불충분했다는 이의 제기를 피하려면, 위의 링크를 이용하여 공지하세요. 공지에 비중립적인 단어를 사용하는 등의 선전 행위는 피하세요.
토론이 끝났다면 선언과 함께 {{토론보존}} 틀을 이용하여 닫습니다. 총의 판단은 여타 토론과 마찬가지로  분쟁 해결 정책에서 갈음해 처리합니다. 토론을 통해 정책이나 지침 채택 여부를 논의하며, 이 과정에서 제안 문서가 크게 수정될 수도 있습니다. 토론 중 제안을 정식 정책/지침으로 채택하자는 합의로 모이고 나서 2주 (정확히 14일. 이후 내용은 모두 같습니다) 간 제안을 대폭 수정해야 하는 변경안 제시나 명확한 근거가 존재하는 반대가 나오지 않는다면 정책이나 지침으로 정식으로 채택됩니다. 반대로 토론자들 사이에서 채택을 거부한다는 합의가 모아져서 2주간 명확한 근거가 존재하는 반대 의견이 나오지 않는다면 채택안 거부 총의가 모아졌다고 보아 기각됩니다. 주요한 총의 판단 기준은 다음과 같습니다.


단문으로 LLM 에 요청했을 때의 답변
격하 과정(降下過程, Deposition Process)은 대기 중의 수증기가 응결하여 구름을 형성한 후, 이 구름에서 물방울이나 얼음 결정이 지표면으로 떨어지는 과정을 의미합니다. 이 과정은 주로 비, 눈, 우박 등의 형태로 나타납니다. 격하 과정은 다음과 같은 단계로 이루어집니다.

1. **응결**: 대기 중의 수증기가 냉각되어 미세한 물방울이나 얼음 결정으로 변하는 과정입니다. 이 과정은 대기 중의 온도가 이슬점 이하로 떨어질 때 발생합니다. 응결핵(먼지, 소금 등) 위에 수증기가 응결하여 물방울이 형성됩니다.

2. **구름 형성**: 응결된 물방울이 모여 구름을 형성합니다. 구름은 수많은 미세한 물방울이나 얼음 결정으로 이루어져 있으며, 이들은 대기 중에서 부유합니다.

3. **물방울 성장**: 구름 속의 물방울은 서로 충돌하고 합쳐지면서 점점 커집니다. 이 과정에서 물방울이 충분히 커지면 중력에 의해 떨어지기 시작합니다.

4. **격하**: 물방울이나 얼음 결정이 지표면으로 떨어지는 과정입니다. 이때 물방울의 크기와 대기 중의 온도, 바람의 세기 등에 따라 비, 눈, 우박 등의 형태로 나타납니다.

5. **지표면 도달**: 격하된 물방울이나 얼음 결정은 지표면에 도달하여 물이나 얼음의 형태로 존재하게 됩니다.

격하 과정은 기후와 날씨에 중요한 영향을 미치며, 수자원의 순환에도 중요한 역할을 합니다.


RAG 를 적용한 답변
격하 과정은 특정 정책이나 지침이 더 이상 유효하지 않거나 필요하지 않다고 판단될 때 진행됩니다. 이 과정은 채택 과정과 유사하게 진행되며, 일반적으로 다음과 같은 단계로 이루어집니다:

1. **논의 시작**: 편집자들은 정책이나 지침의 격하를 제안하고, 해당 문서의 토론란에서 논의를 시작합니다. 이때, 프로젝트 문서 상단에 {{새로운 토론|문단=진행 중인 토론 문단}} 틀을 붙여 공동체의 참여를 요청합니다.

2. **충분한 논의**: 논의가 충분히 이루어진 후, 제3의 편집자가 토론을 종료하고, 상태 변경에 대한 총의가 형성되었는지 판단합니다.

3. **상태 변경**: 만약 격하가 결정되면, 폐지된 정책이나 지침에는 최상단에 {{중단}} 틀을 붙여 더 이상 사용하지 않는 정책/지침임을 알립니다.

4. **소수의 지지**: 소수의 공동체 인원만 지지하는 수필, 정보문 및 기타 비공식 문서는 일반적으로 주된 작성자의 사용자 이름공간으로 이동됩니다.

이 과정은 공동체의 의견을 반영하여 정책이나 지침의 유효성을 평가하고, 필요에 따라 조정하는 중요한 절차입니다.
```

실행 결과를 보면 RAG 를 적용하지 않고 단순히 "격하 과정에 대해서 알려주세요" 라고 LLM 에 물어보았을 때는 `격하 과정`이 학습 데이터에 거의 없거나 존재하지 않는지 환각 현상을 보이고 있습니다. 하지만 RAG 를 적용한 답변은 연관 문서들을 통해 `격하 과정`에 대한 정확한 답변을 주는 것을 확인할 수 있습니다.

---

# 2. RAG - Document Loader

LangChain 에서 Document Loader 는 다양한 소스에서 문서를 부러오고 처리하는 과정을 담당합니다. 특히 사전 지식이 필요한 지식 기반의 태스크, 정보 검색, 데이터 처리 작업 등을 처리할 때 반드시 필요합니다. Document Loader 의 주요 목적은 효율적으로 문서 데이터를 수집하고, 사용 가능한 형식으로 변환하는 것입니다.

	1. 다양한 소스 지원 : 웹 페이지, PDF 파일, 데이터베이스 등 다양한 소스에서 문서를 불러올 수 있습니다.
	2. 데이터 변환 및 정제 : 불러온 문서 데이터를 분석하고 처리하여, 랭체인의 다른 모듈이나 알고리즘이 처리하기 쉬운 형태로 변환합니다. 불필요한 데이터를 제거하거나, 구조를 변경할 수도 있습니다.
	3. 효율적인 데이터 관리 : 대량의 문서 데이터를 효율적으로 관리하고, 필요할 때 쉽게 접근할 수 있도록 합니다. 이를 통해 검색 속도를 향상시키고, 전체 시스템의 성능을 높일 수 있습니다.

## 2.1 웹 문서(WebBaseLoader)

### WebBaseLoader 이용하여 웹 페이지 데이터 가져오기

`WebBaseLoader` 는 특정 웹 페이지의 내용을 로드하고 파싱하기 위해 설계된 클래스입니다.

`web_paths` 매개변수는 로드할 웹 페이지의 URL 을 단일 문자열 또는 여러 개의 URL 시퀀스 배열로 지정할 수 있습니다. 여기서는 파이썬 튜플 형태로 2개의 URL 을 사용하고 있습니다.

`bs_kwargs` 매개변수는 BeautifulSoup 을 사용하여 HTML 을 파싱할 때 사용되는 인자들을 딕셔너리 형태로 제공합니다. 예제에서는 `bs4.SoupStrainer` 를 사용하여 특정 클래스 이름을 가진 HTML 요소만 파싱하도록 지정하고 있습니다. `article-header`, `article-title` 클래스를 가진 요소만 선택하여 파싱합니다.

`docs` 변수에는 로드된 문서들의 배열이 할당됩니다. 각 페이지별로 별도의 Document 객체로 변환되어 2개의 문서가 생성됩니다.

```python
# WebBaseLloader

import bs4
from langchain_community.document_loaders import WebBaseLoader

# 여러 개의 url 지정 가능

url1 = "https://blog.langchain.dev/customers-replit/"
url2 = "https://blog.langchain.dev/langgraph-v0-2/"

loader = WebBaseLoader(
    web_paths = (url1, url2),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("article-header", "article-content")
        )
    ),
)

docs = loader.load()

# 문서 개수 출력
print(f"docs num : {len(docs)}")

# 첫 번째 문서 출력
docs[0]
```

```
실행 결과

docs num : 2
Document(metadata={'source': 'https://blog.langchain.dev/customers-replit/'}, page_content='\nReplit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow which enables a highly custom agentic workflow with a high-degree of control and parallel execution. By using LangSmith, Replit gained deep visibility into their agent interactions to debug tricky issues.\xa0The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context.\xa0Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what’s happening, including at the inputs and outputs of each step, in order to understand the agent’s decision-making.\xa0Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning, creating dev environments, installing dependencies, and deploying applications for users.\xa0As a result, Replit’s traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith’s frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern – searching within traces – was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit’s time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,\xa0 Replit’s agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces.\xa0To solve this, LangSmith’s thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial.\xa0ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development.\t\n')
```

---

## 2.2 텍스트 문서

### TextLoader 이용하여 텍스트 파일 데이터 가져오기

`langchain_community` 라이브러리의 `document_loaders` 모듈에는 다양한 Document Loader 함수를 지원하고 있습니다. 이 중에서 `TextLoader` 를 사용하여 텍스트 파일을 불러올 수 있습니다. 그리고 텍스트 파일의 내용을 랭체인의 Document 객체로 변환하고 이를 리스트 형태로 반환합니다.

실습데이터 : [history.txt](https://github.com/tsdata/langchain-study/blob/main/data/history.txt)

현재 저는 구글 코랩에서 실습을 진행하고 있기 때문에 제 구글 드라이브와 연동해서 history.txt 파일을 loader 하도록 하였습니다.

```python
# Text 파일 Loader

from langchain_community.document_loaders import TextLoader

file_path = "/content/drive/MyDrive/LangChain/history.txt"

loader = TextLoader(file_path)
data = loader.load()

print(type(data))
print(len(data))

data
```

출력해보면 리스트 안에 Document 객체가 담겨 있는 것을 볼 수 있습니다.

```
실행 결과 

<class 'list'>
1
[Document(metadata={'source': '/content/drive/MyDrive/LangChain/history.txt'}, page_content='한국의 역사는 수천 년에 걸쳐 이어져 온 긴 여정 속에서 다양한 문화와 전통이 형성되고 발전해 왔습니다. 고조선에서 시작해 삼국 시대의 경쟁, 그리고 통일 신라와 고려를 거쳐 조선까지, 한반도는 많은 변화를 겪었습니다.\n\n고조선은 기원전 2333년 단군왕검에 의해 세워졌다고 전해집니다. 이는 한국 역사상 최초의 국가로, 한민족의 시원이라 할 수 있습니다. 이후 기원전 1세기경에는 한반도와 만주 일대에서 여러 소국이 성장하며 삼한 시대로 접어듭니다.\n\n4세기경, 고구려, 백제, 신라의 삼국이 한반도의 주요 세력으로 부상했습니다. 이 시기는 삼국이 각각 문화와 기술, 무력을 발전시키며 경쟁적으로 성장한 시기로, 한국 역사에서 중요한 전환점을 마련했습니다. 특히 고구려는 북방의 강대국으로 성장하여 중국과도 여러 차례 전쟁을 벌였습니다.\n\n7세기 말, 신라는 당나라와 연합하여 백제와 고구려를 차례로 정복하고, 한반도 최초의 통일 국가인 통일 신라를 건립합니다. 이 시기에 신라는 불교를 국교로 채택하며 문화와 예술이 크게 발전했습니다.\n\n그러나 10세기에 이르러 신라는 내부의 분열과 외부의 압력으로 쇠퇴하고, 이를 대체하여 고려가 성립됩니다. 고려 시대에는 과거제도의 도입과 더불어 청자 등 고려 고유의 문화가 꽃피었습니다.\n\n조선은 1392년 이성계에 의해 건국되어, 1910년까지 이어졌습니다. 조선 초기에는 세종대왕이 한글을 창제하여 백성들의 문해율을 높이는 등 문화적, 과학적 성취가 이루어졌습니다. 그러나 조선 후기에는 내부적으로 실학의 발전과 함께 사회적 변화가 모색되었으나, 외부로부터의 압력은 점차 커져만 갔습니다.\n\n19세기 말부터 20세기 초에 걸쳐 한국은 제국주의 열강의 침략을 받으며 많은 시련을 겪었습니다. 1910년, 한국은 일본에 의해 강제로 병합되어 35년간의 식민 지배를 받게 됩니다. 이 기간 동안 한국인들은 독립을 위한 다양한 운동을 전개했으며, 이는 1945년 일본의 패망으로 이어지는 독립으로 결실을 맺었습니다.\n\n해방 후 한반도는 남북으로 분단되어 각각 다른 정부가 수립되었고, 1950년에는 한국전쟁이 발발하여 큰 피해를 입었습니다. 전쟁 후 남한은 빠른 경제 발전을 이루며 오늘날에 이르렀습니다.\n\n한국의 역사는 오랜 시간 동안 수많은 시련과 도전을 겪으며 형성된 깊은 유산을 지니고 있습니다. 오늘날 한국은 그 역사적 배경 위에서 세계적으로 중요한 역할을 하고 있으며, 과거의 역사가 현재와 미래에 어떻게 영향을 미치는지를 이해하는 것은 매우 중요합니다.')]
```

Document 객체에는 `page_content` 필드와 `metadata` 필드가 들어 있습니다. `page_content` 는 텍스트로 변환된 문자열이 들어 있습니다.

---

## 2.3 디렉토리 폴더(DirectoryLoader)

### DirectoryLoader 이용하여 특정 폴더의 모든 파일을 가져오기

`DirectoryLoader` 를 사용하여 디렉토리 내의 모든 문서를 로드할 수 있습니다. `DirectoryLoader` 인스턴스를 생성할 때 문서가 있는 디렉토리의 경로와 해당 문서를 식별할 수 있는 glob 패턴을 지정합니다.

실습데이터1 : [history.txt](https://github.com/tsdata/langchain-study/blob/main/data/history.txt)
실습데이터2 : [places.txt](https://github.com/tsdata/langchain-study/blob/main/data/places.txt)

```python
# DirectoryLoader 이용

from langchain_community.document_loaders import DirectoryLoader

loader = DirectoryLoader(path="/content/drive/MyDrive/LangChain", glob="*.txt", loader_cls=TextLoader)

data = loader.load()

print("첫 번째 데이터")
print(data[0].page_content)
print("\n\n")

print("두 번째 데이터")
print(data[1].page_content)

```

아래와 같이 두 개의 txt 파일을 읽어오는 것을 확인할 수 있습니다.

```
첫 번째 데이터
한국의 역사는 수천 년에 걸쳐 이어져 온 긴 여정 속에서 다양한 문화와 전통이 형성되고 발전해 왔습니다. 고조선에서 시작해 삼국 시대의 경쟁, 그리고 통일 신라와 고려를 거쳐 조선까지, 한반도는 많은 변화를 겪었습니다.

고조선은 기원전 2333년 단군왕검에 의해 세워졌다고 전해집니다. 이는 한국 역사상 최초의 국가로, 한민족의 시원이라 할 수 있습니다. 이후 기원전 1세기경에는 한반도와 만주 일대에서 여러 소국이 성장하며 삼한 시대로 접어듭니다.

4세기경, 고구려, 백제, 신라의 삼국이 한반도의 주요 세력으로 부상했습니다. 이 시기는 삼국이 각각 문화와 기술, 무력을 발전시키며 경쟁적으로 성장한 시기로, 한국 역사에서 중요한 전환점을 마련했습니다. 특히 고구려는 북방의 강대국으로 성장하여 중국과도 여러 차례 전쟁을 벌였습니다.

7세기 말, 신라는 당나라와 연합하여 백제와 고구려를 차례로 정복하고, 한반도 최초의 통일 국가인 통일 신라를 건립합니다. 이 시기에 신라는 불교를 국교로 채택하며 문화와 예술이 크게 발전했습니다.

그러나 10세기에 이르러 신라는 내부의 분열과 외부의 압력으로 쇠퇴하고, 이를 대체하여 고려가 성립됩니다. 고려 시대에는 과거제도의 도입과 더불어 청자 등 고려 고유의 문화가 꽃피었습니다.

조선은 1392년 이성계에 의해 건국되어, 1910년까지 이어졌습니다. 조선 초기에는 세종대왕이 한글을 창제하여 백성들의 문해율을 높이는 등 문화적, 과학적 성취가 이루어졌습니다. 그러나 조선 후기에는 내부적으로 실학의 발전과 함께 사회적 변화가 모색되었으나, 외부로부터의 압력은 점차 커져만 갔습니다.

19세기 말부터 20세기 초에 걸쳐 한국은 제국주의 열강의 침략을 받으며 많은 시련을 겪었습니다. 1910년, 한국은 일본에 의해 강제로 병합되어 35년간의 식민 지배를 받게 됩니다. 이 기간 동안 한국인들은 독립을 위한 다양한 운동을 전개했으며, 이는 1945년 일본의 패망으로 이어지는 독립으로 결실을 맺었습니다.

해방 후 한반도는 남북으로 분단되어 각각 다른 정부가 수립되었고, 1950년에는 한국전쟁이 발발하여 큰 피해를 입었습니다. 전쟁 후 남한은 빠른 경제 발전을 이루며 오늘날에 이르렀습니다.

한국의 역사는 오랜 시간 동안 수많은 시련과 도전을 겪으며 형성된 깊은 유산을 지니고 있습니다. 오늘날 한국은 그 역사적 배경 위에서 세계적으로 중요한 역할을 하고 있으며, 과거의 역사가 현재와 미래에 어떻게 영향을 미치는지를 이해하는 것은 매우 중요합니다.



두 번째 데이터
경복궁
서울의 중심에 위치한 경복궁은 조선 시대의 왕궁으로, 한국의 역사와 전통 문화를 체험할 수 있는 대표적인 명소입니다. 광활한 궁궐 안에는 경회루, 근정전 등 다양한 전통 건축물이 있으며, 정기적으로 궁궐 경비 교대식과 전통 공연이 열립니다.

남산 서울타워
서울의 스카이라인을 대표하는 남산 서울타워는 서울 시내를 한눈에 볼 수 있는 최고의 전망대입니다. 타워 주변의 남산 공원은 산책과 휴식을 즐기기에 적합하며, 연인들의 자물쇠 벽도 유명합니다.

부산 해운대 해수욕장
부산의 해운대 해수욕장은 국내외 관광객에게 사랑받는 한국 최대의 해수욕장 중 하나입니다. 넓은 백사장과 도심 속의 접근성이 좋은 위치로, 여름철에는 수많은 피서객으로 붐빕니다.

제주도
한국의 남쪽에 위치한 제주도는 화산섬으로, 아름다운 자연 풍경과 독특한 문화를 자랑합니다. 세계 자연 유산에 등재된 한라산, 청정 해변, 용두암 등 다양한 자연 명소와 함께 특색 있는 음식과 문화가 관광객을 맞이합니다.

경주
신라 천년의 고도 경주는 한국의 역사적인 도시 중 하나로, 불국사, 석굴암, 첨성대 등 수많은 유적지와 문화재가 있습니다. 신라의 역사와 문화를 체험할 수 있는 최적의 장소입니다.

인사동
서울의 인사동은 전통 찻집, 공예품 가게, 갤러리가 즐비한 문화 예술의 거리입니다. 한국의 전통 문화와 현대 예술이 공존하는 이곳에서는 다양한 기념품을 구입하고 전통 차를 맛볼 수 있습니다.

한강
서울을 가로지르는 한강은 도시의 휴식처로, 한강공원, 자전거 도로, 피크닉 장소 등을 제공합니다. 야경이 아름다운 한강에서는 다양한 레저 활동과 행사가 열리며, 여름에는 불꽃놀이 축제가 인기입니다.

순천만 국가정원
전라남도 순천에 위치한 순천만 국가정원은 다양한 식물과 아름다운 정원이 조화를 이루는 곳으로, 자연과 함께하는 힐링의 시간을 제공합니다. 인근의 순천만 습지는 천연 기념물로 지정된 생태 관광지입니다.
```

---

## 2.4 CSV 문서(CSVLoader)

### CSVLoader 이용하여 CSV 파일 데이터 가져오기

`langchain_community` 라이브러리의 `document_loaders` 모듈의 `CSVLoader` 클래스를 사용하여 CSV 파일에서 데이터를 로드합니다. CSV 파일의 각 행을 추출하여 서로 다른 Document 객체로 변환합니다. 이들 문서 객체로 이루어진 리스트 형태로 반환합니다.

다음 코드는 `CSVLoader` 클래스의 인스턴스를 이용하여 주택금융관련 지수 데이터를 담고 있는 CSV 파일을 로드하고 있습니다. 인코딩 방식은 `cp949` 를 사용하였습니다.

실습 데이터 : [한국주택금융공사_주택금융관련_지수_20160101.cvs](https://github.com/tsdata/langchain-study/blob/main/data/%E1%84%92%E1%85%A1%E1%86%AB%E1%84%80%E1%85%AE%E1%86%A8%E1%84%8C%E1%85%AE%E1%84%90%E1%85%A2%E1%86%A8%E1%84%80%E1%85%B3%E1%86%B7%E1%84%8B%E1%85%B2%E1%86%BC%E1%84%80%E1%85%A9%E1%86%BC%E1%84%89%E1%85%A1_%E1%84%8C%E1%85%AE%E1%84%90%E1%85%A2%E1%86%A8%E1%84%80%E1%85%B3%E1%86%B7%E1%84%8B%E1%85%B2%E1%86%BC%E1%84%80%E1%85%AA%E1%86%AB%E1%84%85%E1%85%A7%E1%86%AB_%E1%84%8C%E1%85%B5%E1%84%89%E1%85%AE_20160101.csv)

```python
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(file_path="/content/drive/MyDrive/LangChain/csv_sample.csv", encoding='cp949')
data = loader.load()

data[0]
```

```
실행 결과

Document(metadata={'source': '/content/drive/MyDrive/LangChain/csv_sample.csv', 'row': 0}, page_content='연도: 2004-01-01\n전국소득대비 주택가격 비율: 4.21\n서울소득대비 주택가격 비율: 4.89\n부산소득대비 주택가격 비율: 3.95\n대구소득대비 주택가격 비율: 3.73\n인천소득대비 주택가격 비율: 4.65\n광주소득대비 주택가격 비율: 2.81\n대전소득대비 주택가격 비율: 4.68\n울산소득대비 주택가격 비율: 2.66\n세종소득대비 주택가격 비율: 0\n경기소득대비 주택가격 비율: 4.17\n강원소득대비 주택가격 비율: 2.49\n충북소득대비 주택가격 비율: 2.62\n충남소득대비 주택가격 비율: 2.17\n전북소득대비 주택가격 비율: 3.12\n전남소득대비 주택가격 비율: 2.12\n경북소득대비 주택가격 비율: 2.12\n경남소득대비 주택가격 비율: 3.81\n제주소득대비 주택가격 비율: 2.99\n전국평균 대출금액  평균 연소득: 2.36\n서울평균 대출금액  평균 연소득: 2.61\n부산평균 대출금액  평균 연소득: 2.35\n대구평균 대출금액  평균 연소득: 2.24\n인천평균 대출금액  평균 연소득: 2.7\n광주평균 대출금액  평균 연소득: 1.6\n대전평균 대출금액  평균 연소득: 2.26\n울산평균 대출금액  평균 연소득: 1.67\n세종평균 대출금액  평균 연소득: 0\n경기평균 대출금액  평균 연소득: 2.42\n강원평균 대출금액  평균 연소득: 1.44\n충북평균 대출금액  평균 연소득: 1.53\n충남평균 대출금액  평균 연소득: 1.21\n전북평균 대출금액  평균 연소득: 1.9\n전남평균 대출금액  평균 연소득: 1.42\n경북평균 대출금액  평균 연소득: 1.31\n경남평균 대출금액  평균 연소득: 2.06\n제주평균 대출금액  평균 연소득: 1.28')
```

### 데이터 출처 정보를 특정 필드(열, column)로 지정

`CSVLoader` 를 사용하여 CSV 파일을 로드할 대, `source_column` 속성에 데이터의 출처 정보(`source`)로 사용될 열의 이름을 지정할 수 있습니다. 다음 예제에서는 "연도" 열이 각 행 데이터의 출처 정보로 사용됩니다. `source` 속성을 확인해 보면 `2004-01-01` 와 같이 해당 행의 "연도" 열에 있는 값이 적용된 것을 알 수 있습니다.

```python
loader = CSVLoader(file_path="/content/drive/MyDrive/LangChain/csv_sample.csv", encoding='cp949',
                   source_column='연도'
                   )
data = loader.load()

data[0]
```

```
실행 결과

Document(metadata={'source': '2004-01-01', 'row': 0}, page_content='연도: 2004-01-01\n전국소득대비 주택가격 비율: 4.21\n서울소득대비 주택가격 비율: 4.89\n부산소득대비 주택가격 비율: 3.95\n대구소득대비 주택가격 비율: 3.73\n인천소득대비 주택가격 비율: 4.65\n광주소득대비 주택가격 비율: 2.81\n대전소득대비 주택가격 비율: 4.68\n울산소득대비 주택가격 비율: 2.66\n세종소득대비 주택가격 비율: 0\n경기소득대비 주택가격 비율: 4.17\n강원소득대비 주택가격 비율: 2.49\n충북소득대비 주택가격 비율: 2.62\n충남소득대비 주택가격 비율: 2.17\n전북소득대비 주택가격 비율: 3.12\n전남소득대비 주택가격 비율: 2.12\n경북소득대비 주택가격 비율: 2.12\n경남소득대비 주택가격 비율: 3.81\n제주소득대비 주택가격 비율: 2.99\n전국평균 대출금액  평균 연소득: 2.36\n서울평균 대출금액  평균 연소득: 2.61\n부산평균 대출금액  평균 연소득: 2.35\n대구평균 대출금액  평균 연소득: 2.24\n인천평균 대출금액  평균 연소득: 2.7\n광주평균 대출금액  평균 연소득: 1.6\n대전평균 대출금액  평균 연소득: 2.26\n울산평균 대출금액  평균 연소득: 1.67\n세종평균 대출금액  평균 연소득: 0\n경기평균 대출금액  평균 연소득: 2.42\n강원평균 대출금액  평균 연소득: 1.44\n충북평균 대출금액  평균 연소득: 1.53\n충남평균 대출금액  평균 연소득: 1.21\n전북평균 대출금액  평균 연소득: 1.9\n전남평균 대출금액  평균 연소득: 1.42\n경북평균 대출금액  평균 연소득: 1.31\n경남평균 대출금액  평균 연소득: 2.06\n제주평균 대출금액  평균 연소득: 1.28')
```

---

### CSV 파싱 옵션 지정

`CSVLoader` 클래스를 사용할 때 추가적인 CSV 관련 설정을 `csv_args` 매개변수를 통해 지정할 수 있습니다. `csv_args` 는 파이썬 표준 라이브러리인 `csv` 모듈에 전달될 추가 인자들을 담는 딕셔너리입니다. 다음 예제는 CSV 파일의 구분자(`delimiter`)로 줄바꿈 문자(`\n`)를 지정하고 있습니다. 줄바꿈 문자를 기준으로 각 필드를 구분하기 때문에, 기본 값인 콤마(`,`)를 적용했을 경우와 파싱된 결과에 차이가 있습니다.

```python
loader = CSVLoader(file_path="/content/drive/MyDrive/LangChain/csv_sample.csv", encoding='cp949',
                   csv_args={
                       'delimiter':'\n',
                   }
                   )
data = loader.load()

data[0]
```

## 2.5 PDF 문서

`langchain_community` 라이브러리의 `document_loaders` 모듈에는 PDF 문서에서 텍스트를 추출하여 파일 내용을 Document 객체로 변환하는 다양한 유형의 Document Loader 를 제공합니다.

### 2.5.1 PDF 문서 페이지별 로드(PyPDFLoader)

#### PyPDFLoader 이용해 PDF 파일 데이터 가져오기

`langchain_community` 패키지에서 제공하는 `PyPDFLoader` 를 사용해 PDF 파일에서 텍스트를 추출합니다. 이 명령을 사용하려면 pypdf 라이브러리를 먼저 설치해야 합니다.

```python
!pip install -q pypdf
```

`PyPDFLoader` 인스턴스를 사용해 지정된 PDF 파일을 열고, Document 객체의 개수를 출력해봅니다.

실습 데이터 : [000660_SK_2023.pdf](https://github.com/tsdata/langchain-study/blob/main/data/000660_SK_2023.pdf)

```python
from langchain_community.document_loaders import PyPDFLoader

pdf_filepath = "/content/drive/MyDrive/LangChain/000660_SK_2023.pdf"
loader = PyPDFLoader(pdf_filepath)
pages = loader.load()

len(pages)
```

```
실행 결과

21
```

10번째에 있는 Document 객체를 출력해 봅니다.

```
실행 결과

Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.2 (Macintosh)', 'creationdate': '2023-06-26T16:16:31+09:00', 'moddate': '2023-06-26T17:21:06+09:00', 'trapped': '/False', 'source': '/content/drive/MyDrive/LangChain/000660_SK_2023.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='2120\nESG Special Report 2023\n투자 검토 단계\nPre-Acquisition (인수 전)\n01\n포트폴리오 ESG 관리 체계\n장기적 관점에서 기업가치 제고를 실현하기 위해 핵심자산인 \n투자 포트폴리오의 경제적 가치와 함께 ESG 가치를 \n통합적으로 관리하기 위한 체계를 구축하고 있습니다.\n투자 검토 시점부터 인수 후, 회수 시점까지 투자\nLife Cycle에 걸쳐 적용되는 체계적인 ESG 관리를 \n기반으로 내부적으로는 ESG를 고려한 합리적인 투자의사 \n결정을 이행하고, 시장에서는 포트폴리오의 기업가치가 \n시장에서 제대로 평가받으며 나아가 사회·환경에 미치는 \n파급력을 높일 수 있도록 노력하겠습니다.\n포트폴리오 ESG 관리 원칙\nSK주식회사 투자회사\n기업가치 관점의\nESG 중점관리 \n항목 도출\n자사 ESG \n관리전략\nESG 성과 \n데이터 관리\n기업가치와\nESG 성과 \n연계성 분석\n포트폴리오 \nESG 관리전략 \nUpgrade\n성장단계\n산업특성\nESG Divestment 전략 검토\n       ESG Exit 리포트 발간\n ·    인수 이후 ESG Value-up 기반 Exit 전략 도출\n ·    중대 ESG 리스크/기회 현황 및   \nESG 관리·공시 수준 확인\n셀사이드(Sell-side) 점검사항 관리\n       중대 ESG 이슈 존재 여부 검토\n ·    매각 대상 시장 내 ESG 규제 준수 여부 확인\n ·    ESG 우수 영역에 대한 정보공개 및    \n기회 확대 방안 제시\n ·    국내외 책임투자 기준 부합 여부 확인\n ·    우수 관리 영역 정보공개 및   \n이해관계자 커뮤니케이션\n매각/투자 회수 단계\n03\nExit (투자 회수)\n정기 ESG 점검\n       투자회사 분류 \n ·   전체 포트폴리오를 16개 업종, 기업 규모에 따라 3개 그룹으로 구분\n       ESG 중점관리 항목 도출 \n  ·   ESG 외부평가 및 주가 상관관계 상위 영역 분석에 따라   \n산업 핵심관리 영역 도출\n       정기 평가 실시 \n   ·   연 2회 ESG 실적점검 실시,     \n중요 ESG 리스크/기회 관련 이슈 식별\n보유 단계\n02\nValue-up Period (보유 기간)\nESG 기반 주주 소통\n        주주 소통 대상 안건\n  ·   해당 산업 ESG 중점관리 항목과 연관된 리스크가 식별되었거나,  \n연중 중대한 ESG Controversy Issue(예상하지 못한 우려) 발생 시\n       투자회사 유형별 관여 방식\n ·    이사회 기타비상무이사를 통한 소통\n ·    ESG 점검 리포트 또는 주주서한 발송\n기후 리스크 관리\n        전환 리스크\n ·    탄소 규제/가격 변동에 따른 재무영향 점검  \n[수익성] 매출액/영업이익 대비 탄소 비용 추이 고려  \n[경제성] 투자 대비 감축 수단 효과 검토  \n[시장성] 경쟁사 대비 속도/수준 감안 대응전략 점검\n       물리적 리스크\n ·    자산 소재 지역의 취약한 이상기후 요인 식별\n ·    고위험 기업 대상 관리방안 마련 권고\nESG 실사\n       ESG 실사 수행\n ·   실사 대상 기업 산업 및 규모에 따른 \n체크리스트 생성\n ·   점검항목은 유형에 따라   \nESG 리스크와 관리체계로 구분\n ·   서면진단 및 현장실사 실시\n        시사점 도출\n ·    Valuation 반영을 통한   \n인수가액 조정\n ·   PMI(Post-Merger Integration, \n인수 합병 후 통합 과정)   \n개선 과제 제시\nESG는 의사결정과 행동에 반드시 \n반영되어야 하는 필수 요소이자 \n기회입니다. SK는 기업가치의 건강한 \n성장과 이해관계자의 지속가능한 행복을 \n위해 경영의사결정의 DNA로서 \nESG(환경/사회/지배구조)를 \n내재화하고 있습니다.\nESG 관리의 단계적 고도화\n포트폴리오 ESG 관리 역량 축적, \n글로벌 Top-tier 수준 ESG 관리체계 확보\n성장단계별 관리 차별화\n투자기업 성숙도(기업의 Life Cycle)에\n따라 적합한 ESG Value-up 실현\n산업별 중점관리항목 체계화\n산업별 기업가치에 큰 영향을 주는 \nESG 이슈 집중 관리\nWhere we are heading    |     How we get there    |     What we are preparing')
```

---

### 2.5.2 PDF 문서의 메타 데이터를 상세하게 추출하기(PyMuPDFLoader)

#### PyMuPDFLoader 이용하여 PDF 파일 데이터 가져오기

`langchain_community.document_loaders` 모듈의 PyMuPDFLoader 클래스는 PyMuPDF 를 사용하여 PDF 파일의 페이지를 로드하고, 각 페이지를 개별 `document` 객체로 추출합니다. 특히 PDF 문서의 자세한 메타데이터를 추출하는데 강점이 있습니다.

실습 데이터 : [000660_SK_2023_2023.pdf](https://github.com/tsdata/langchain-study/blob/main/data/000660_SK_2023.pdf)

`PyMuPDFLoader` 클래스를 사용하려면, 개발환경에 `PyMuPDF` 라이브러리를 설치해야합니다.

```python
!pip install pymupdf
```

`PyMuPDF` 인스턴스를 사용하여 지정된 PDF 파일을 로드하면, 각 페이지가 하나의 Document 객체로 일대일로 변환됩니다.

```python
from langchain_community.document_loaders import PyMuPDFLoader

pdf_filepath = "/content/drive/MyDrive/LangChain/000660_SK_2023.pdf"
loader = PyMuPDFLoader(pdf_filepath)

pages = loader.load()

print(len(pages))

```

```
실행 결과

21
```

첫 번째 Document 객체의 내용(`page_content`)을 출력해서 확인해 봅니다.

```python
from langchain_community.document_loaders import PyMuPDFLoader

pdf_filepath = "/content/drive/MyDrive/LangChain/000660_SK_2023.pdf"
loader = PyMuPDFLoader(pdf_filepath)

pages = loader.load()

pages[0].page_content
```

```
실행 결과

1
Where we are heading    |     How we get there    |     What we are preparing
ESG Special Report
2023 
NAVIGATING 
UNCERTAINTIES TO ENSURE  
SUSTAINABLE 
GROWTH
```

---

### 2.5.3 온라인 PDF 문서 로드(OnlinePDFLoader)

#### OnlinePDFLoader 이용하여 온라인 PDF 파일의 데이터를 가져오기

"Transformers" 논문 (`https://arxiv.org/pdf/1706.03762.pdf`)을 로드하여 페이지 내용을 추출하고, 로드된 페이지 수와 첫 페이지의 내용 일부를 출력하는 과정을 처리합니다. `langchain_community` 라이브러리의 `OnlinePDFLoader` 클래스를 사용합니다.

실습 데이터 : [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

구글 코랩 환경에서 실행하기 위해선 다음 라이브러리를 설치해 주어야 합니다.

```python
!pip install unstructured-pytesseract
```

```python
from langchain_community.document_loaders import OnlinePDFLoader

# Transformers 논문 로드
loader = OnlinePDFLoader("https://arxiv.org/pdf/1706.03762")
pages = loader.load()

print(len(pages))
```

```
실행 결과

1
```

로드된 문서 객체의 내용을 출력하여 확인합니다. 첫 페이지의 텍스트 내용 중 처음 1000 자를 출력합니다.

```python
from langchain_community.document_loaders import OnlinePDFLoader

# Transformers 논문 로드
loader = OnlinePDFLoader("https://arxiv.org/pdf/1706.03762")
pages = loader.load()

pages[0].page_content[:1000]
```

```
실행 결과

1706.03762v7 [cs.CL] 2 Aug 2023

arXiv

Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.

Attention Is All You Need

Ashish Vaswani* Noam Shazeer* Niki Parmar* Jakob Uszkoreit* Google Brain Google Brain Google Research Google Research avaswani@google.com noam@google.com nikip@google.com usz@google.com

Llion Jones* Aidan N. Gomez* ¢ Lukasz Kaiser* Google Research University of Toronto Google Brain llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com

Illia Polosukhin* + illia.polosukhin@gmail.com

Abstract

The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing 
```

---

### 2.5.4 특정 폴더의 모든 PDF 문서 로드 (PyPDFDirectoryLoader)

#### PyPDFDirectoryLoader 이용하여 특정 폴더의 모든 PDF 파일 가져오기

`langchain_community.document_loaders` 모듈의 `PyPDFDirectoryLoader` 클래스는 지정된 디렉토리에서 모든 PDF 문서를 한 번에 가져옵니다.

실습 데이터1 : [00660_SK_2023.pdf](https://github.com/tsdata/langchain-study/blob/main/data/000660_SK_2023.pdf)
실습 데이터2 : [300720_한일시멘트_2023.pdf](https://github.com/tsdata/langchain-study/blob/main/data/300720_%E1%84%92%E1%85%A1%E1%86%AB%E1%84%8B%E1%85%B5%E1%86%AF%E1%84%89%E1%85%B5%E1%84%86%E1%85%A6%E1%86%AB%E1%84%90%E1%85%B3_2023.pdf)

`PyPDFDirectoryLoader` 의 인스턴스를 생성하고, load 메소드를 호출하여 해당 디렉토리의 모든 PDF 문서를 로드하고, data 변수에 할당합니다. `len(data)` 는 로드된 문서 객체의 총 개수를 반환합니다. `PyPDFDirectoryLoader` 가 디렉토리 내의 PDF 파일들을 가져와서 페이지별로 문서 객체로 변환하게 됩니다.

```python
from langchain_community.document_loaders import PyPDFDirectoryLoader

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/LangChain/")
data = loader.load()

len(data)
```

```
실행 결과

100
```

```python
from langchain_community.document_loaders import PyPDFDirectoryLoader

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/LangChain/")
data = loader.load()

print(data[0].metadata)
print("\n")
print(data[-1].metadata)
```

첫 번째 문서에는 SK 보고서가 두 번째 문서에는 한일시멘트 보고서가 들어간 것을 확인할 수 있습니다.

```
실행 결과

{'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.2 (Macintosh)', 'creationdate': '2023-06-26T16:16:31+09:00', 'moddate': '2023-06-26T17:21:06+09:00', 'trapped': '/False', 'source': '/content/drive/MyDrive/LangChain/000660_SK_2023.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}

{'producer': 'iLovePDF', 'creator': 'Adobe InDesign 16.4 (Macintosh)', 'creationdate': '2023-07-28T10:47:36+09:00', 'moddate': '2023-07-28T11:18:40+09:00', 'trapped': '/False', 'source': '/content/drive/MyDrive/LangChain/300720_한일시멘트_2023.pdf', 'total_pages': 79, 'page': 78, 'page_label': '79'}
```

---

# 3. RAG - Text Splitter

LangChain 은 긴 문서를 작은 단위인 청크(chunk)로 나누는 텍스트 분리 도구를 다양하게 지원합니다. 텍스트를 분리하는 작업을 청킹(chunking)이라고 부르기도 합니다. 이렇게 문서를 작은 조각으로 나누는 이유는 LLM 모델의 입력 토큰의 개수가 정해져 있기 때문입니다. 허용 한도를 넘는 텍스트는 모델에서 입력으로 처리할 수 없기 때문이죠 한편, 텍스트가 너무 긴 경우에는 핵심 정보 이외에 불필요한 정보들이 많이 포함될 수 있어서 RAG 품질이 낮아지는 요인이 될 수도 있습니다. 핵심 정보가 유지될 수 있는 적절한 크기로 나누는 것이 매우 중요합니다.

LangChain 이 지원하는 다양한 텍스트 분리기(Text Splitter)는 분할하려는 텍스트 유형과 사용 사례에 맞춰 선택할 수 있는 다양한 옵션이 제공됩니다. 크게 두 가지 차원에서 검토가 필요합니다.

1. 텍스트가 어떻게 분리되는지

	텍스트를 나눌 때 각 청크가 독립적으로 의미를 갖도록 나눠야 합니다. 이를 위해 문장, 구절, 단락 등 문서 구조를 기준으로 나눌 수 있습니다.

2. 청크 크기가 어떻게 측정 되는지

	각 청크의 크기를 직접 조정할 수 있습니다. LLM 모델의 입력 크기와 비용 등을 종합적으로 고려하여 애플리케이션에 적합한 최적 크기를 결정하는 기준입니다. 예를 들면 단어 수, 문자 수 등을 기준으로 나눌 수 있습니다.

## 3.1 CharacterTextSplitter

### 1. 문서를 개별 문자 단위로 나누기(`separator=""`)

`CharacterTextSplitter` 클래스는 주어진 텍스트를 문자 단위로 분할하는데 사용됩니다. Python 의 split 함수라고 생각하시면 됩니다. 다음 코드에서 적용된 주요 매개변수는 다음과 같습니다.

- `separator` : 분할된 각 청크를 구분할 때 기준이 되는 문자열입니다. 여기서는 빈 문자열('')을 사용하므로, 각 글자를 기준으로 분할합니다.

- `chunk_size` : 각 청크의 최대 길이입니다. 여기서는 '500'으로 설정되어 있으므로, 최대 500자까지의 텍스트가 하나의 청크에 포함됩니다.

- `chunk_overlap` : 인접한 청크 사이에 중복으로 포함될 문자의 수입니다. 여기서는 '100'으로 설정되어 있으므로, 각 청크들은 연결 부분에서 100자가 중복됩니다.

- `length_fucntion` : 청크의 길이를 계산하는 함수입니다. 여기서는 `len` 함수가 사용되었으므로, 문자열의 길이를 기반으로 청크의 길이를 계산합니다.

`split_text` 메소드는 주어진 텍스트를 위에서 설정한 매개변수에 따라 분할하고, 분할된 청크의 리스트를 반환합니다. `len(texts)` 는 분할된 청크의 총 수를 나타냅니다. 여기서는 3개의 청크로 분할됩니다.

여기서 중요한 것은 각 청크의 크기가 `chunk_size` 를 초과하지 않으며, 인접한 청크 사이에는 `chunk_overlap` 만큼의 문자가 중복되어 있음을 이해하는 것입니다. 이렇게 함으로써, 텍스트의 의미적 연속성을 유지하면서도 큰 데이터를 더 작은 단위로 분할할 수 있습니다.

사용한 데이터는 이전에 TextLoader 클래스를 설명할 때 사용한 'history.txt' 파일에 있는 데이터를 사용했습니다.

```python
# 각 문자를 구분하여 분할

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter

# 구글 드라이브에서 history.txt 파일을 로드
file_path = "/content/drive/MyDrive/LangChain/history.txt"

loader = TextLoader(file_path)
data = loader.load()

# 텍스트 분리 진행
text_splitter = CharacterTextSplitter(
    separator = '',
    chunk_size = 500,
    chunk_overlap = 100,
    length_function = len,
)

texts = text_splitter.split_text(data[0].page_content)

print(len(texts))
```

```
실행 결과

3
```

분할된 텍스트 조각 중에서 첫 번째 청크의 길이를 확인해보면 정확하게 500자임을 알 수 있습니다.

```python
# 각 문자를 구분하여 분할

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter

# 구글 드라이브에서 history.txt 파일을 로드
file_path = "/content/drive/MyDrive/LangChain/history.txt"

loader = TextLoader(file_path)
data = loader.load()

# 텍스트 분리 진행
text_splitter = CharacterTextSplitter(
    separator = '',
    chunk_size = 500,
    chunk_overlap = 100,
    length_function = len,
)

texts = text_splitter.split_text(data[0].page_content)

print(len(texts[0]))
```

```
실행 결과

500
```

---

### 2. 문서를 특정 문자열 기준으로 나누기 (`separator="문자열"`)

`CharacterTextSplitter` 클래스의 `separator` 매개변수를 줄바꿈 문자로 설정하는 예제입니다. 이렇게 하면 각 청크를 나누는 기준을 줄바꿈 문자로 설정하는 것입니다.

우선 이전에 `separator` 를 지정하지 않았을 경우 각 청크들의 길이가 어떻게 되는지 출력을 해보면 다음과 같습니다.

```
실행 결과

500, 499 434
```

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter

file_path = "/content/drive/MyDrive/LangChain/history.txt"

loader = TextLoader(file_path)
data = loader.load()

text_splitter = CharacterTextSplitter(
    separator = '\n',
    chunk_size = 500,
    chunk_overlap = 100,
    length_function = len,
)

texts = text_splitter.split_text(data[0].page_content)

print(len(texts))
print(len(texts[0]), len(texts[1]), len(texts[2]))
```

분할된 각 청크의 길이를 확인해보면 정확하게 500자 단위로 나누어지지 않았습니다. 이처럼 줄바꿈 문자를 기준으로 최대 500자를 맞출 수 있는 위치를 찾아서 분할하게 됩니다.

```
실행 결과

3
411 386 427
```

---

## 3.2 RecursiveCharacterTextSplitter

`RecursiveCharacterTextSplitter` 클래스는 텍스트를 재귀적으로 분할하여 의미적으로 관련 있는 텍스트 조각들이 같이 있도ㅗㄺ 하는 목적으로 설계되었습니다. 이 과정에서 문자 리스트(`['\n\n', '\n', ' ', '']`)의 문자를 순서대로 사용하여 텍스트를 분할하며, 분할된 청크들이 설정된 `chunk_size` 보다 작아질 때까지 이 과정을 반복합니다. 여기서 `chunk_overlap` 은 분할된 텍스트 조각들 사이에서 중복으로 포함될 문자 수를 정의합니다. `length_function = len` 코드는 분할의 기준이 되는 길이를 측정하는 함수로 문자열의 길이를 반환하는 `len` 함수를 사용한다는 의미입니다.

`texts = text_splitter.split_text(data[0].page_content)` 코드는 `data[0].page_content` 에서 첫 번째 문서의 내용을 `RecursiveCharacterTextSplitter` 를 사용하여 분할하고, 결과를 `texts` 변수에 할당합니다. `data` 리스트에서 첫 번째 문서의 내용을 기반으로 분할 작업을 수행하게 됩니다. `len(texts)` 는 분할된 텍스트 조각들의 총 수를 반환합니다.

사용한 데이터는 `history.txt` 파일을 사용하였습니다.

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

file_path = "/content/drive/MyDrive/LangChain/history.txt"

loader = TextLoader(file_path)
data = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 500,
    chunk_overlap = 100,
    length_function = len,
)

texts = text_splitter.split_text(data[0].page_content)

print(len(texts))
len(texts[0]), len(texts[1]), len(texts[2])
```

```
실행 결과

3
413, 388, 429
```

## 3.3 토큰 수를 기준으로 텍스트 분할(Tokenizer 활용)

대규모 언어 모델(LLM)을 사용할 때 모델이 처리할 수 있는 토큰 수에는 한계가 있습니다. 입력 데이터를 모델의 제한을 초과하지 않도록 적절히 분할하는 것이 중요합니다. 이 때 LLM 모델에 적용되는 토크나이저를 기준으로 텍스트를 토큰으로 분할하고, 이 토큰들의 수를 기준으로 텍스트를 청크로 나누면 모델 입력 토큰 수를 조절할 수 있습니다.

OpenAI API의 경우 `tiktoken` 라이브러리를 통해 해당 모델에서 사용하는 토크나이저를 기준으로 분할할 수 있습니다. `CharacterTextSplitter.from_tiktoken_encoder` 메서드는 글자 수 기준으로 분할할 때 `tiktoken` 토크나이저를 기준으로 글자수를 계산하여 분할합니다. 여기서 `encoding_name='cl100k_base`는 텍스트를 토큰으로 변환하는 인코딩 방식을 나타냅니다.

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter

file_path = "/content/drive/MyDrive/LangChain/history.txt"

loader = TextLoader(file_path)
data = loader.load()

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size = 600,
    chunk_overlap = 200,
    encoding_name = 'cl100k_base'
)

docs = text_splitter.split_documents(data)

print(len(docs))

print(len(docs[0].page_content))
print(docs[0].page_content)
```

```
실행 결과

3
525
한국의 역사는 수천 년에 걸쳐 이어져 온 긴 여정 속에서 다양한 문화와 전통이 형성되고 발전해 왔습니다. 고조선에서 시작해 삼국 시대의 경쟁, 그리고 통일 신라와 고려를 거쳐 조선까지, 한반도는 많은 변화를 겪었습니다.

고조선은 기원전 2333년 단군왕검에 의해 세워졌다고 전해집니다. 이는 한국 역사상 최초의 국가로, 한민족의 시원이라 할 수 있습니다. 이후 기원전 1세기경에는 한반도와 만주 일대에서 여러 소국이 성장하며 삼한 시대로 접어듭니다.

4세기경, 고구려, 백제, 신라의 삼국이 한반도의 주요 세력으로 부상했습니다. 이 시기는 삼국이 각각 문화와 기술, 무력을 발전시키며 경쟁적으로 성장한 시기로, 한국 역사에서 중요한 전환점을 마련했습니다. 특히 고구려는 북방의 강대국으로 성장하여 중국과도 여러 차례 전쟁을 벌였습니다.

7세기 말, 신라는 당나라와 연합하여 백제와 고구려를 차례로 정복하고, 한반도 최초의 통일 국가인 통일 신라를 건립합니다. 이 시기에 신라는 불교를 국교로 채택하며 문화와 예술이 크게 발전했습니다.
```

# 4. RAG Embedding

임베딩(Embedding)은 텍스트 데이터를 숫자로 이루어진 벡터로 변환시킨 것을 말합니다. 이러한 벡터 표현을 사용하면, 텍스트 데이터를 벡터 공간 내에서 수학적으로 다룰 수 있게 되며, 이를 통해 텍스트 간의 유사성을 계산하거나, 텍스트 데이터를 기반으로 하는 다양한 머신러닝 및 자연어 처리 작업을 수행할 수 있습니다. 임베딩 과정은 텍스트의 의미적인 정보를 보존하도록 설계 되어 있어, 벡터 공간에서 가까이 위치한 텍스트 조각들은 의미적으로도 유사한 것으로 간주됩니다.

임베딩의 주요 활용 사례

- 의미 검색(Semantic Search) : 벡터 표현을 활용하여 의미적으로 유사한 텍스트를 검색하는 과정으로, 사용자가 입력한 쿼리에 대해 가장 관련성 높은 문서나 정보를 찾아내는 데 사용됩니다.

- 문서 분류(Document Classification) : 임베딩된 텍스트 벡터를 사용하여 문서를 특정 카테고리나 주제에 할당하는 분류 작업에 사용됩니다.

- 텍스트 유사도 계산(Text Similarity Calculation) : 두 텍스트 벡터 사이의 거리를 계산하여, 텍스트 간의 유사성 정도를 정량적으로 평가합니다.

임베딩 모델 제공자
- OpenAI : GPT 와 같은 언어 모델을 통해 텍스트의 임베딩 벡터를 생성할 수 있는 API 를 제공합니다.
- Hugging Face : Transformers 라이브러리를 통해 다양한 오픈소스 임베딩 모델을 제공합니다.
- Google : Gemini, Gemma 등 언어 모델에 적용되는 임베딩 모델을 제공합니다.

임베딩 메서드
- embed_documents : 이 메소드는 문서 객체의 집합을 입력으로 받아, 각 문서를 벡터 공간에 임베딩합니다. 주로 대량의 텍스트 데이터를 배치 단위로 처리할 때 사용됩니다.
- embed_query : 이 메소드는 단일 텍스트 쿼리를 입력으로 받아, 쿼리를 벡터 공간에 임베딩합니다. 주로 사용자의 검색 쿼리를 임베딩하여, 문서 집합 내에서 해당 쿼리와 유사한 내용을 찾아내는 데 사용됩니다.

임베딩은 텍스트 데이터를 머신러닝 모델이 이해할 수 있는 형태로 변환하는 핵심 과정입니다. 다양한 자연어 처리 작업의 기반이 되는 중요한 작업입니다.

## 4.1 OpenAIEmbeddings

`OpenAIEmbeddings` 클래스는 OpenAI 의 API 를 활용하여, 각 문서를 대응하는 임베딩 벡터로 변환합니다. `langchain_openai` 라이브러리에서 `OpenAIEmbeddings` 클래스를 직접 임포트합니다.

아래 코드에서 `embed_documents` 메소드는 입력 받은 5개의 문서 객체를 각각 별도의 벡터로 임베딩합니다. `embeddings` 변수에는 각 텍스트에 대한 벡터 표현을 담고 있는 리스트가 할당됩니다. `len(embeddings)` 는 입력된 텍스트 리스트의 개수와 동일하며, 이는 임베딩 과정을 거친 문서의 총 수를 나타냅니다.

`len(embeddings[0])` 는 첫 번째 문서의 벡터 표현의 차원을 나타냅니다. 일반적으로 이 차원 수는 선택된 모델에 따라 정해지며, 모든 임베딩 벡터는 동일한 차원을 가집니다. OpenAI 의 임베딩 모델을 사용할 경우 임베딩 벡터의 차원은 1536 이라는 것을 확인할 수 있습니다.

```python
from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()

embeddings = embeddings_model.embed_documents(
    [
        '안녕하세요!',
        '어! 오랜만이에요',
        '이름이 어떻게 되세요?',
        '날씨가 추워요',
        'Hello LLM!'
    ]
)

len(embeddings), len(embeddings[0])
```

```
실행 결과

(5, 1536)
```

첫 번째 문서의 변환된 임베딩 벡터를 출력해 봅니다. 1536 차원 중에서 앞에서 20차원에 해당하는 원소만을 출력합니다. 이처럼 각 문서를 임베딩으로 변환하면 숫자를 원소로 갖는 긴 벡터 형태를 갖게 됩니다.

```python
print(embeddings[0][:20])
```

```
실행 결과

[-0.010458921082317829, -0.013548481278121471, -0.006539991125464439, -0.01863865926861763, -0.018246132880449295, 0.016625380143523216, -0.009211701340973377, 0.0039442540146410465, -0.007413678336888552, 0.01007272582501173, 0.011775783263146877, -0.006723592057824135, -0.02538757584989071, -0.022538594901561737, -0.004830603487789631, -0.021804191172122955, 0.025286277756094933, -0.017651012167334557, 0.007939157076179981, -0.017840944230556488]
```

`embed_query` 메소드는 단일 쿼리 문자열을 받아 이를 벡터 공간에 임베딩합니다. 주로 검색 쿼리나 질문 같은 단일 텍스트를 임베딩할 때 유용하며, 생성된 임베딩을 사용해 유사한 문서나 답변을 찾을 수 있습니다.

```python
from langchain_openai import OpenAIEmbeddings

embedded_query = embeddings_model.embed_query('첫인사를 하고 이름을 물어봤나요?')
embedded_query[:5]
```

```
실행 결과

[0.003640108974650502,
 -0.024275783449411392,
 0.010910888202488422,
 -0.04110145568847656,
 -0.004543057177215815]
```

코사인 유사도는 두 벡터 간의 코사인 각을 이용하여 유사성을 측정하는 방법입니다. 두 벡터의 바향이 완전히 동일하면 코사인 유사도는 1이 됩니다. 90도로 수직이면 0, 반대 방향이면 -1이 됩니다. 이는 텍스트 임베딩과 같이 고차원에서도 벡터 간 유사도를 측정하는 데 유용하게 사용됩니다.

주어진 cos_sim 함수는 두 벡터 A와 B 사이의 코사인 유사도를 계산합니다. `dot(A, B)` 는 두 벡터의 내적을, `norm(A)`와 `norm(B)`는 각각 벡터 A와 B의 노름(크기)을 계산합니다. 이 함수는 내적 값과 두 벡터 크기의 곱으로 나누 값으로 코사인 유사도를 계산합니다.

다음 예시는 앞에서 임베딩 변환한 문서들(embeddings)과 하나의 임베딩된 쿼리(embedded_query) 사이의 코사인 유사도를 계산하여 출력합니다. 각 문서 임베딩에 대해 cos_sim 함수를 호출하여, 해당 문서가 쿼리와 얼마나 유사한지를 숫자로 나타냅니다. 유사도가 높은 문서일수록 쿼리와 더 관련이 깊다고 볼 수 있습니다.

```python

# 코사인 유사도

import numpy as np
from numpy import dot
from numpy.linalg import norm
from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()

embeddings = embeddings_model.embed_documents(
    [
        '안녕하세요!',
        '어! 오랜만이에요',
        '이름이 어떻게 되세요?',
        '날씨가 추워요',
        'Hello LLM!'
    ]
)

embedded_query = embeddings_model.embed_query('첫인사를 하고 이름을 물어봤나요?')

def cos_sim(A, B):
  return dot(A, B) / (norm(A)*norm(B))

for embedding in embeddings:
  print(cos_sim(embedding, embedded_query))

```

다음은 임베딩된 모든 문서와 쿼리 사이의 코사인 유사도를 출력한 결과입니다. "인사"와 "이름"이라는 두 가지 토픽에 대해서 상대적으로 관련성이 높은 문서들이 유사도가 높은 것을 확인할 수 있습니다. 이러한 방식으로 문서 검색, 추천 시스템 등 다양한 자연어 처리 작업에서 유사도 기반 필터링이나 정렬을 수행할 수 있습니다.

```
실행 결과

0.8347781524360807
0.8153837322339593
0.8843960106566056
0.7899011862340304
0.7468198077293927
```

## 4.2 HuggingFaceEmbeddings

`sentence-transformers` 라이브러리를 사용하면 HuggingFace 모델에서 사용된 사전 훈련된 임베딩 모델을 다운로드 받아서 적용할 수 있습니다. OpenAI 임베딩 모델을 사용할 때는 API 사용료가 부과되지만, HuggingFace 의 오픈소스 기반의 임베딩 모델을 사용하면 요금이 부과되지 않습니다.

먼저 `sentence-transformers` 라이브러리를 설치합니다.

```
!pip install -U sentence-trainsformers
```

HuggingFaceEmbeddings 클래스는 Hugging Face 의 트랜스포머 모델을 사용하여 문서 또는 문장을 임베딩하는 데 사용됩니다. 다음은 주요 매개변수의 설정 값을 설명합니다.

- `model_name` : 사용할 모델을 지정합니다. 여기서는 한국어 자연어 추론(Natural Language Inference, NLI)에 최적화된 ko-sroberta 모델을 사용합니다.

- `model-kwargs` : 모델이 cpu 에서 실행되도록 설정합니다. gpu 를 사용할 수 있는 환경이라면 cuda 로 설정할 수도 있습니다.

- `encode_kwargs` : 임베딩을 정규화하여 모든 벡터가 같은 범위의 값을 갖도록 합니다. 이는 유사도 계산 시 일관성을 높여줍니다.

`embeddings_model` 을 출력해보면 `Pooling` 레이어의 `word_embedding_dimension` 값에서 임베딩 벡터의 크기를 확인할 수 있습니다. 768 차원의 벡터라는 것을 알 수 있습니다.

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings_model = HuggingFaceEmbeddings(
    model_name = "jhgan/ko-sroberta-nli",
    model_kwargs = {'device' : 'cuda'},
    encode_kwargs = {'normalize_embeddings':True}
)

embeddings_model
```

```python
실행 결과

HuggingFaceEmbeddings(client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'RobertaModel'})
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
), model_name='jhgan/ko-sroberta-nli', cache_folder=None, model_kwargs={'device': 'cuda'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)
```

`embed_documents` 메소드는 주어진 5개의 문장으로 구성된 텍스트 리스트를 임베딩합니다. 임베딩 벡터는 768ㅊ원으로 확인됩니다.

```python
embeddings = embeddings_model.embed_documents(
    [
        '안녕하세요!',
        '어! 오랜만이에요',
        '이름이 어떻게 되세요?',
        '날씨가 추워요',
        'Hello LLM!'
    ]
)
len(embeddings), len(embeddings[0])

```

```
실행 결과

(5, 768)
```

`embed_query` 메소드는 단일 쿼리 문장을 임베딩합니다. 이렇게 생성된 임베딩은 cos_sim 함수를 사용하여 쿼리와 각 문서 간의 코사인 유사도를 계산합니다. 이 유사도 점수를 통해 쿼리와 가장 관련이 깊은 문서를 파악할 수 있습니다.

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
  return dot(A, B) / (norm(A)*norm(B))

embeddings_model = HuggingFaceEmbeddings(
    model_name = "jhgan/ko-sroberta-nli",
    model_kwargs = {'device' : 'cuda'},
    encode_kwargs = {'normalize_embeddings':True}
)

#embeddings_model

embeddings = embeddings_model.embed_documents(
    [
        '안녕하세요!',
        '어! 오랜만이에요',
        '이름이 어떻게 되세요?',
        '날씨가 추워요',
        'Hello LLM!'
    ]
)
len(embeddings), len(embeddings[0])

embedded_query = embeddings_model.embed_query('첫인사를 하고 이름을 물어봤나요?')

for embedding in embeddings:
    print(cos_sim(embedding, embedded_query))

```

```
실행 결과

0.5899016046274541
0.41826309410089174
0.7240604881408343
0.05702663569167117
0.4316417573777297
```

---

## 4.3 GoogleGenerativeAIEmbeddings

`langchain_google_genai` 라이브러리와 `GoogleGenerativeAIEmbeddings` 클래스를 사용하면 Google 의 생성형 AI 모델을 활용하여 문서나 문장을 임베딩할 수 있습니다.

먼저 `langchain_google_genai` 라이브러리를 설치합니다. `-q` 플래그는 로그 출력을 최소화합니다.

```
!pip install -q langchain_google_genai
```

Google API 사용을 위해선 환경 변수에 API 키를 설정합니다. `GOOGLE_API_KEY`는 실제 사용자의 API 키로 대체합니다.

```python
import os

os.environ['GOOGLE_API_KEY'] = 'GOOGLE_API_KEY'
```

`GoogleGenerativeAIEmbeddings(model='models/embedding-001')`을 사용하여 임베딩 모델의 인스턴스를 생성합니다. 이 때, 사용할 Google 의 생성형 AI 모델을 `model` 인자를 통해 지정합니다. 여기서는 `models/embedding-001` 을 사용하도록 지정하고 있습니다.

`embed-documents` 메소드를 호출하여, 주어진 텍스트 리스트를 임베딩합니다. 이 메소드는 각 문서 또는 문장을 벡터 공간에 매핑된 임베딩으로 변환합니다. 

```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings_model = GoogleGenerativeAIEmbeddings(model='models/embedding-001')

embeddings = embeddings_model.embed_documents(
    [
        '안녕하세요!',
        '어! 오랜만이에요',
        '이름이 어떻게 되세요?',
        '날씨가 추워요',
        'Hello LLM!'
    ]
)
len(embeddings), len(embeddings[0])
```

```
실행 결과

(5, 768)
```

이번엔 쿼리에 대한 임베딩을 생성하고, 코사인 유사도를 구해보도록 하겠습니다.

```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings_model = GoogleGenerativeAIEmbeddings(model='models/embedding-001')

embeddings = embeddings_model.embed_documents(
    [
        '안녕하세요!',
        '어! 오랜만이에요',
        '이름이 어떻게 되세요?',
        '날씨가 추워요',
        'Hello LLM!'
    ]
)
len(embeddings), len(embeddings[0])

embedded_query = embeddings_model.embed_query('첫인사를 하고 이름을 물어봤나요?')

for embedding in embeddings:
    print(cos_sim(embedding, embedded_query))

```

```
실행 결과

0.9483334221872353
0.9563804554562886
0.9988387578259003
0.9707770904712603
0.8775081156240261
```

---

# 5. RAG Vector Store

벡터 저장소(Vector Store)는 벡터 형태로 표현된 데이터, 즉 임베딩 벡터들을 효율적으로 저장하고 검색할 수 있는 시스템이나 데이터베이스를 의미합니다. 자연어 처리(NLP), 이미지 처리, 그리고 기타 다양한 머신러닝 응용 분야에서 생성된 고차원 벡터 데이터를 관리하기 위해 설계되었습니다. 벡터 저장소의 핵심 기능은 대규모 벡터 데이터셋에서 빠른 속도로 가장 유사한 항목을 찾아내는 것입니다.

- **벡터 저장**
	임베딩 벡터는 텍스트, 이미지, 소리 등 다양한 형태의 데이터를 벡터 공간에 매핑한 것으로, 데이터의 의미적, 시각적, 오디오적 특성을 수치적으로 표현합니다. 이러한 벡터를 효율적으로 저장하기 위해서는 고차원 벡터를 처리할 수 있도록 최적화된 데이터 저장 구조가 필요합니다.

- **벡터 검색**
	저장된 벡터들 중에서 사용자의 쿼리에 가장 유사한 벡터를 빠르게 찾아내는 과정입니다. 이를 위해 코사인 유사도, 유클리드 거리, 맨해튼 거리 등 다양한 유사도 측정 방법을 사용할 수 있습니다. 코사인 유사도는 방향성을 기반으로 유사도를 측정하기 때문에 텍스트 임베딩 검색에 특히 자주 사용됩니다.

- **결과 반환**
	사용자의 쿼리에 대해 계산된 유사도 점수를 기반으로 가장 유사한 항목들을 순서대로 사용자에게 반환합니다. 이 과정에서는 유사도 점수뿐만 아니라 , 검색 결과의 관련성, 다양성, 신뢰도 등 다른 요소들을 고려할 수도 있습니다.
	벡터 저장소는 Faiss(Facebook AI Similarity Search), Chroma, Elasticsearch, Pinecone 등 다양한 오픈 소스 및 상용 솔루션이 있으며, 각각의 특성과 성능이 다르기 때문에 사용 목적에 따라 적합한 도구를 선택해야 합니다.

## 5.1 Chroma

Chroma 는 임베딩 벡터를 저장하기 위한 오픈소스 소프트웨어로, LLM 앱 구축을 용이하게 하는 핵심 기능을 수행합니다. Chroma 의 주요 특징은 다음과 같습니다.

- 임베딩 및 메타데이터 저장 : 대규모의 임베딩 데이터와 이와 관련된 메타데이터를 효율적으로 저장할 수 있습니다.
- 문서 및 쿼리 임베딩 : 텍스트 데이터를 벡터 공간에 매핑하여 임베딩을 생성할 수 있으며, 이를 통해 검색 작업이 가능합니다.
- 임베딩 검색 : 사용자 쿼리에 기반하여 가장 관련성 높은 임베딩을 찾아내는 검색 기능을 제공합니다.

Apache 2.0 라이선스 하에 배포되는 오픈소스 프로젝트로 자유롭게 사용, 수정, 배포할 수 있습니다.

---

### 5.1.1 유사도 기반 검색(Similarity search)

`Chroma` 벡터 저장소를 사용하여 임베딩된 텍스트 데이터를 저장하고 검색하는 방법을 설명합니다. `Chroma` 벡터 저장소를 사용하여 대규모 텍스트 데이터셋에서 빠르고 효율적으로 유사도 기반 검색(Similarity search)을 수행할 수 있습니다. 구체적인 단계는 다음과 같습니다.

1. 텍스트 데이터 로드
	- `TextLoader` 클래스를 사용해 텍스트 파일에서 텍스트 데이터를 로드합니다.
	- 로드된 데이터는 `data` 변수에 저장됩니다.

2. 테스트 분할
	- `RecuresiveCharacterTextSplitte` 를 사용하여 로드된 텍스트를 여러 개의 작은 조각으로 분할합니다.
	- 분할된 텍스트 조각들을 `texts` 변수에 저장됩니다.

3. 임베딩 모델 초기화
	- OpenAIEmbeddings 를 사용하여 OpenAI 임베딩 모델의 인스턴스를 생성합니다. 이 단계에서 HuggingFace 또는 다른 임베딩 모델을 사용할 수 있습니다.

4. Chroma 벡터 저장소 생성
	- Chroma.from_texts 메소드를 사용하여 분할된 텍스트들을 임베딩하고, 이 임베딩을 `Chroma` 벡터 저장소에 저장합니다.
	- 저장소는 `collection_name`으로 구분되며, 여기서는 `history` 라는 이름을 사용합니다.
	- 저장된 데이터는 `./db/chromadb` 디렉토리에 저장됩니다.
	- `collection_metadata`에서 `'hnsw:space' : 'cosine'` 을 설정하여 유사도 계산에 코사인 유사도를 사용합니다.

5. 유사도 기반 검색 수행
	- `query` 변수에 검색 쿼리를 정의합니다.
	- `db.similarity_search` 메소드를 사용하여 저장된 데이터 중에서 쿼리와 가장 유사한 문서를 찾습니다.
	- 검색 결과를 `docs` 변수에 저장하고, 가장 유사한 문서의 내용은 `docs[0]_page_content` 를 통해 확인합니다.

이 과정을 통해, 주어진 쿼리('`누가 한글을 창제했나요?`')에 대해 가장 관련성 높은 텍스트 조각을 찾아내고 있습니다.

```python
# Choma 유사도 기반 검색
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

file_path = "/content/drive/MyDrive/LangChain/history.txt"

loader = TextLoader(file_path)
data = loader.load()

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size = 250,
    chunk_overlap = 50,
    encoding_name='cl100k_base'
)

texts = text_splitter.split_text(data[0].page_content)

embeddings_model = OpenAIEmbeddings()

db = Chroma.from_texts(
    texts = texts,
    embedding = embeddings_model,
    collection_name = 'history',
    persist_directory = "/content/drive/MyDrive/LangChain/db/chromadb",
    collection_metadata = {'hnsw:space': 'cosine'},

)

query = "누가 한글을 창제했나요?"
docs = db.similarity_search(query)
print(docs[0].page_content)
```

실행 결과로 쿼리와 가장 유사한 문서가 추출되는 것을 확인할 수 있습니다.

```
실행 결과

조선은 1392년 이성계에 의해 건국되어, 1910년까지 이어졌습니다. 조선 초기에는 세종대왕이 한글을 창제하여 백성들의 문해율을 높이는 등 문화적, 과학적 성취가 이루어졌습니다. 그러나 조선 후기에는 내부적으로 실학의 발전과 함께 사회적 변화가 모색되었으나, 외부로부터의 압력은 점차 커져만 갔습니다.
```

---

### 5.1.2 MMR (Maximum marginal relevance search)

최대 한계 관련성(Maximum Marginal Relevance, MMR) 검색 방식은 유사성과 다양성의 균형을 맞추어 검색 결과의 품질을 향상시키는 알고리즘입니다. 이 방식은 검색 쿼리에 대한 문서들의 관련성을 최대화하는 동시에, 검색된 문서들 사이의 중복성을 최소화하여, 사용자에게 다양하고 풍부한 정보를 제공하는 것을 목표로 합니다.

#### MMR 의 작동 원리

MMR 은 쿼리에 대한 전체 문서에서 각 문서의 유사성 점수와 이미 선택된 문서들과의 다양성(또는 차별성) 점수를 조합하여, 각 문서의 최종 점수를 계산합니다. 이 최종 점수에 기반하여 문서를 선택합니다. MMR 은 다음과 같이 정의될 수 있습니다.
핵심 아이디어는 **질의와의 유사도는 높게, 이미 선택된 것들과의 유사도는 낮게** 입니다.

$$
\operatorname{MMR}(d;Q,D') \;=\; 
\lambda \,\operatorname{Sim}(d, Q)
\;-\;
(1-\lambda)\,\max_{d' \in D'} \operatorname{Sim}(d, D')
$$

- ($\operatorname{Sim}(d, Q$) 는 전체 문서 ($d$) 와 쿼리 ($Q$) 사이의 유사성을 나타냅니다. 그리고 이 값은 MMR 수식 계산 전에 모든 문서에 대해서 계산을 해놓습니다.
- ($\max_{d' \in D'} \operatorname{Sim}(d, d')$) 는 문서 ($d$) 와 이미 선택된 문서 집합 ($D'$) 중 가장 유사한 문서와의 유사성을 나타냅니다.
- ($\lambda$) 는 유사성과 다양성의 상대적 중요도를 조절하는 매개변수입니다.
- MMR 점수는 뽑고자 하는 문서 개수 k 가 충족될 때까지 매번 모든 문서에 대해서 MMR 점수 계산을 진행합니다.
- 시간복잡도는 전체 문서를 $D$ 뽑고자 하는 문서의 개수가 $k$ 라면 $O(D \cdot k)$ 입니다. 하지만 실제 최적화는 쿼리와의 유사도가 높은 상위 100 ~ 200개 문서를 가지고 와서 진행합니다.

#### MMR 의 주요 매개변수

- query : 사용자로부터 입력받은 검색 쿼리입니다.
- k : 최종적으로 선택할 문서의 수 입니다. 이 매개변수는 반환할 문서의 총 개수를 결정합니다.
- fetch_k : MMR 알고리즘을 수행할 때 고려할 상위 문서의 수입니다. 이는 초기 후보 문서 집합의 크기를 의미하며, 이 중에서 MMR 에 의해 최종 문서가 k 개 만큼 선택됩니다.
- lambda_mult : 쿼리와의 유사성과 선택된 문서 간의 다양성 사이의 균형을 조절합니다. ($\lambda = 1$) 은 유사성만을 고려하며, ($\lambda = 0$) 은 다양성만을 고려합니다. 실무 권장 범위는 보통 0.5 ~ 0.8 에서 검증하여 결정합니다.

MMR 방식을 사용하면, 검색 결과로 얻은 문서들이 쿼리와 관련성이 높으면서도 서로 다른 측면이나 정보를 제공하도록 할 수 있습니다. 이는 특히 정보 검색이나 추천 시스템에서 사용자에게 더 풍부하고 만족스러운 결과를 제공하는 데 도움이 됩니다.

#### 코드 예제

다음 코드 예제를 통해 유사도 기반 검색과 MMR 방식의 검색 결과를 비교해봅니다.

먼저 `PyMuPDFLoader` 를 사용하여 PDF 파일에서 텍스트 데이터를 로드합니다. 이 클래스는 PyMuPDF 라이브러리를 사용하여 PDF 문서의 내용을 추출합니다. 필요한 경우 `!pip install pymupdf` 명령어로 라이브러리를 설치합니다.

실습 데이터 : [카카오뱅크 2022 지속가능경영보고서.pdf](https://kind.krx.co.kr/external/2023/06/29/000059/20230628001879/%EC%B9%B4%EC%B9%B4%EC%98%A4%EB%B1%85%ED%81%AC%202022%20%EC%A7%80%EC%86%8D%EA%B0%80%EB%8A%A5%EA%B2%BD%EC%98%81%EB%B3%B4%EA%B3%A0%EC%84%9C.pdf)

`RecursiveCharacterTextSplitter` 를 사용하여 문서를 텍스트 조각으로 분할하는 인스턴스를 생성하고 `text_splitter.split_documents(data)` 를 호출하여 문서 객체를 여러 개의 청크로 분할합니다.

```python
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

loader = PyMuPDFLoader("/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf")
data = loader.load()
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000,
    chunk_overlap=200,
    encoding_name='cl100k_base'
)

documents = text_splitter.split_documents(data)
```

다음은 `OpenAIEmbedding` 클래스를 사용하여 임베딩 모델의 인스턴스를 생성합니다. `Chroma.from_documents` 메소드를 사용하여 분할된 문서들을 임베딩하고, 이 임베딩들을 `Chroma` 벡터 저장소에 저장합니다. 여기서는 `esg` 라는 컬렉션 이름을 사용하며, `collection_metadata` 를 통해 유사도 검색에 사용될 공간을 `cosine` 으로 지정하여, 코사인 유사도를 사용합니다.

```python
# Embedding -> Upload to Vectorstore
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings()
db2 = Chroma.from_documents(
    documents, 
    embeddings_model,
    collection_name = 'esg',
    persist_directory = './db/chromadb',
    collection_metadata = {'hnsw:space': 'cosine'}, # l2 is the default
)

db2
```

이제 일반 유사도 검색과 MMR 검색을 비교해 보겠습니다.

##### 1. 일반적인 유사도 기반 검색

쿼리 `카카오뱅크의 환경목표와 세부추진 내용을 알려줘?`를 사용하여 유사성 검색을 수행합니다.

```python
query = '카카오뱅크의 환경목표와 세부추진내용을 알려줘?'
docs = db2.similarity_search(query)
print(len(docs))
print(docs[0].page_content)

```

```
실행 결과

4
더 나은 세상을 만들어 나가는데 앞장서겠습니다.
이에 따라 카카오뱅크는 아래와 같은 환경방침을 수립하여 운영합니다.
•   전사 환경경영 정책 수립
•   녹색 구매 지침 수립
•   환경 지표 설정 및 성과 관리
•   용수, 폐기물, 에너지 등
     자원 사용량 관리
•   기후변화를 포함한
     환경 리스크 관리체계 마련
•   Scope 1&2&3 온실가스
     배출량 모니터링
•   탄소 가격 도입을 통한
     환경 비용 관리
•   신재생 에너지 사용 확대
•   녹색채권 발행 기반 마련
단기
중기
장기
•   환경경영 조직 및
     관리 체계 구축
•   환경영향평가 체계 구축
환경경영
관리체계
구축
환경
리스크 및
성과 관리
환경
영향
저감
환경경영체계 구축
카카오뱅크 환경방침
Protect Environment
카카오뱅크 2022 지속가능경영보고서

```

다음은 검색 결과 중 가장 유사도가 낮은 문서의 내용을 출력해 봅니다.

```python
print(docs[-1].page_content)

```

유사도 기반 검색 결과의 경우 중복 방지가 따로 되어 있지 않아 검색된 4개의 문서가 모두 같은 내용인 것을 확인할 수 있습니다.

```
실행 결과

더 나은 세상을 만들어 나가는데 앞장서겠습니다.
이에 따라 카카오뱅크는 아래와 같은 환경방침을 수립하여 운영합니다.
• 전사 환경경영 정책 수립
• 녹색 구매 지침 수립
• 환경 지표 설정 및 성과 관리
• 용수, 폐기물, 에너지 등
   자원 사용량 관리
• 기후변화를 포함한
   환경 리스크 관리체계 마련
• Scope 1&2&3 온실가스
   배출량 모니터링
• 탄소 가격 도입을 통한
   환경 비용 관리
• 신재생 에너지 사용 확대
• 녹색채권 발행 기반 마련
단기
중기
장기
• 환경경영 조직 및
   관리 체계 구축
• 환경영향평가 체계 구축
환경경영
관리체계
구축
환경
리스크 및
성과 관리
환경
영향
저감
환경경영체계 구축
카카오뱅크 환경방침
Protect Environment
카카오뱅크 2022 지속가능경영보고서
```

##### 2. MMR 검색

동일한 쿼리를 사용하여 MMR 검색을 수행합니다. 여기서는 `k=4` 와 `fetch_k=100` 을 설정하여, 상위 10개의 유사한 문서 중에서 서로 다른 정보를 제공하는 4개의 문서를 선택합니다.

```python
mmr_docs = db2.max_marginal_relevance_search(query, k=4, fetch_k=100)
print(len(mmr_docs))
print(mmr_docs[0].page_content)

```

유사도 기반 검색과 동일한 문서가 출력되는 것을 확인할 수 있습니다.

```
실행 결과

4
더 나은 세상을 만들어 나가는데 앞장서겠습니다.
이에 따라 카카오뱅크는 아래와 같은 환경방침을 수립하여 운영합니다.
•   전사 환경경영 정책 수립
•   녹색 구매 지침 수립
•   환경 지표 설정 및 성과 관리
•   용수, 폐기물, 에너지 등
     자원 사용량 관리
•   기후변화를 포함한
     환경 리스크 관리체계 마련
•   Scope 1&2&3 온실가스
     배출량 모니터링
•   탄소 가격 도입을 통한
     환경 비용 관리
•   신재생 에너지 사용 확대
•   녹색채권 발행 기반 마련
단기
중기
장기
•   환경경영 조직 및
     관리 체계 구축
•   환경영향평가 체계 구축
환경경영
관리체계
구축
환경
리스크 및
성과 관리
환경
영향
저감
환경경영체계 구축
카카오뱅크 환경방침
Protect Environment
카카오뱅크 2022 지속가능경영보고서

```

```python
print(mmr_docs[-1].page_content)
```

유사도 기반 검색과 달리 MMR 검색의 경우 가장 낮은 순위의 문서 내용이 순위가 가장 높은 순위의 문서 내용과 다른 것을 확인할 수 있습니다.

```
실행 결과

니다. 또한 운영에 많은 에너지가 필요한 오프라인 지점 대신 챗봇으로 고객과 소통
하는 것도 환경 중심 경영에 도움이 되고 있습니다. 비대면 계좌 개설, 대출 서류 전자
서식 도입 등 페이퍼리스 정책은 카카오뱅크가 선도해 온, 카카오뱅크만의 자랑입니
다. 카카오뱅크는 국제표준 환경경영시스템 ISO 14001을 취득했습니다. 카카오뱅
크의 모든 임직원이 환경적 지속가능성의 중요성에 공감하며, 환경을 먼저 고려하는 
방안을 경영 전략에 담고 실천하겠습니다. 사업과 업무 단계에서 발생하는 온실가스 
배출량을 측정하고, 절감하는 방안도 모색하고 있습니다.
2,000만 고객의 주거래은행 : 계속되는 포용금융
카카오뱅크의 2022년 말 기준 고객 수는 2,042만 명입니다. 경제활동인구 대비 
71%에 달하는 수준입니다. 카카오뱅크의 출범의 이유와 성장 동력은 모두 ‘고객’
입니다. 카카오뱅크는 IT 기술로 절감한 비용을 2,000만 명이 넘는 고객들에게 돌
려드리기 위해 최선을 다하고 있습니다. 카카오뱅크의 독자적인 대안신용평가모형
(CSS) ‘카카오뱅크 스코어’를 기반으로 한 중저신용대출 공급액은 출범 이후 2022
년 12월까지 누적 7조 1,106억 원에 달합니다.
중도상환해약금 면제 금액도 지난해 말까지 992억 원에 달하며, 2,494억 원에 달
하는 ATM 이용 수수료도 받지 않고 고객들에 돌려드렸습니다.
금리인하요구권을 통한 고객 이자 절감 규모는 누적 169억 원입니다. 카카오뱅크는 
고객들이 상품과 서비스를 이용하며 마주할 수 있는 문제를 해결하는 데 최선을 다하
겠습니다. 지난해 시니어 고객 대상으로 무료로 금융안심보험에 가입할 수 있도록 했
고, 빅데이터와 디지털 기술을 활용하여 보이스피싱을 예방하는 프로그램도 진행하
고 있습니다. 올해도 더 안전하고 더 편리하게 금융 서비스를 이용할 수 있도록 ‘모바
일 금융 안전망 강화’에 연구 역량과 자원을 투입하고 지원하겠습니다.
```

---

## 5.2 FAISS

FAISS(Facebook AI Similarity Search) 는 Facebook AI Research 에 의해 개발된 라이브러리로, 대규모 벡터 데이터셋에서 유사도 검색을 빠르고 효율적으로 수행할 수 있게 해줍니다. FAISS 는 특히 벡터의 압축된 표현을 사용하여 메모리 사용량을 최소화하면서도 검색 속도를 극대화하는 특징이 있습니다.

### 5.2.1 FAISS 기반 유사도 기반 검색(Similarity search)

FAISS 기반의 벡터 스토어를 생성하고 Huggingface 에서 한국어 임베딩 모델을 다운로드 받아서 검색하는 과정을 살펴보겠습니다. 먼저 `faiss-cpu` 와 `sentence-transformers` 패키지를 설치합니다. FAISS 는 CPU 만 사용하는 버전 (`faiss-cpu`)과 GPU 를 지원하는 버전(`faiss-gpu`)으로 나뉘는데, 여기서는 CPU 버전을 설치하는 방법으로 설명합니다. `sentence-transformers` 는 임베딩 모델을 허깅페이스에서 다운로드 받기 위해서 설치합니다.

```
!pip install faiss-cpu sentence-transformers
```

`HuggingFaceEmbeddings` 클래스를 사용하여 사전 학습된 임베딩 모델(`jhgan/ko-sbert-nli`)을 로드하고, `FAISS.from_documents` 메소드를 사용하여 문서 객체를 임베딩 벡터로 변환하여 벡터 저장소에 저장합니다. 여기서 distance_strategy 는 벡터간 거리(또는 유사도)를 측정하는 방법을 결정합니다. DisatanceStrategy.CONSINE은 문서 간의 유사도를 측정할 때 코사인 유사도를 사용하겠다는 것을 의미합니다.

단, 문서 객체를 임베딩 벡터로 변환하여 벡터 저장소에 저장할 때, 모델의 입력 길이 제한을 고려해야 합니다. `jhgan/ko-sbert-nli` 모델의 경우 최대 시퀀스 길이는 128 토큰입니다. (일반적인 BERT 기반 모델은 최대 시퀀스 길이가 512 토큰입니다.) 최대 시퀀스 길이를 초과하는 입력 문장은 잘리거나 패딩 처리됩니다. 사용한 데이터는 이전 Chroma 예제에서 사용했던 "카카오뱅크 2022 지속가능경영보고서.pdf" 데이터를 사용했습니다.

```python
# 벡터스토어 db 인스턴스를 생성
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings_model = HuggingFaceEmbeddings(
    model_name='jhgan/ko-sbert-nli',
    model_kwargs={'device':'cpu'},
    encode_kwargs={'normalize_embeddings':True},
)


vectorstore = FAISS.from_documents(documents,
                                   embedding = embeddings_model,
                                   distance_strategy = DistanceStrategy.COSINE
                                  )
vectorstore

```

벡터 저장소의 유사도 측정 기준을 출력해 보면, 앞서 설정한 'COSINE'을 확인할 수 있습니다.

```python
vectorstore.distance_strategy
```

```
실행 결과

<DistanceStrategy.COSINE: 'COSINE'>

```

다음 코드는 주어진 쿼리에 대해 유사도 검색을 수행하고, 결과로 얻어진 문서들의 리스트를 `docs` 변수에 저장합니다. 그리고 검색 결과의 길이와 첫 번째 문서의 내용을 출력합니다.

`vectorestore.similarity_search` 메서드는 주어진 쿼리 문자열에 대해 벡터 스토어 내의 문서들 중에서 가장 유사한 문서들을 찾아내는 작업을 다음 단계에 따라 수행합니다.

1. 쿼리 인코딩 : 주어진 쿼리 문자열을 벡터로 변환합니다. 이 과정은 `vectorstore` 를 생성할 때 사용된 임베딩 모델(`embeddings_model`)을 사용하여 수행됩니다.
2. 유사도 검색 : 변환된 쿼리 벡터와 벡터 스토어에 저장된 문서 벡터들 간의 유사도를 계산하여, 가장 유사한 문서들을 찾아냅니다. 이 때, 유사도 계산 방법은 `vectorstore` 생성 시 지정된 `distance_strategy`에 따라 결정됩니다.
3. 결과 반환 : 검색 결과로 얻어진 문서들을 유사도 순으로 정렬하여 반환합니다.

```python
query = '카카오뱅크가 중대성 평가를 통해 도출한 6가지 중대 주제는 무엇인가?'
docs = vectorstore.similarity_search(query)
print(len(docs))
print(docs[0].page_content)

```

```
실행 결과

4
카카오뱅크는 중대성 평가를 통해 도출된 6가지 중대 주제에 대한 활동을 공개하고 성과를 평가하며 지속가능한 경영 보고 체계를 수립하고 있습니다.
Management Approach
구분
ESG 경영 이행
환경경영체계
구축 및
운영 내재화
인권경영 확대
이사회 건전성 강화
포용적 금융을 통한
경제 및 사회적
가치 창출
정보보안 및
고객정보 관리
주요 성과
• 2022 MSCI 평가등급 BBB
• 2022 ESG 펀드 신규 투자
• 2022 ESG위원회 5회 개최
• 2023 ISO 14001 획득
• 2022 UN Global Compact 가입
• 2023 인권영향평가 시행
• 2022 ESG위원회 신설
• 2022 여성 사외이사 신규 선임
• 2022 중신용대출 취급 확대,
     개인사업자 신용대출 출시
• 2023 사회적 가치 측정 첫 시도
• 2022 정보보호 관련 인증 유지
     (ISMS, ISO 27001) 
• 개인정보 관련 인증 최초 획득
     (ISO 27701)
중대 주제 선정 이유 및 영향
카카오뱅크는 설립 단계부터 비즈니스의 환경적, 사회적 가치 창출을 고려하였습니다.
카카오뱅크는 기술과 혁신을 통해 기존 금융권에서 소외되어 있던 계층을 포함한 모든 사회
구성원의 금융 접근성을 향상했습니다.
카카오뱅크는 ‘더 나은 세상을 위해 환경을 지키는 것’을 환경경영의 목표로 삼고, 기업활동
전반에 걸쳐 발생하는 환경영향을 최소화하여 지속가능한 미래에 기여하고자 합니다.
특히, 카카오뱅크의 비즈니스가 지속적으로 성장함에 따라 온실가스 배출량을 비롯한 환경에의
영향 확대가 불가피한 현실에서 환경 리스크를 전사 차원의 주요 리스크로 인식하고 관리해야
함을 인지하고 있습니다.
카카오뱅크는 임직원 뿐만 아니라 협력사, 고객, 지역사회와 같은 모든 이해관계자의 인권이
마땅히 존중받는 ‘사람 중심 경영’을 실천하고자 노력합니다. 인권헌장이 바탕이 된 인권경영과
이해관계자의 참여 및 적극적인 리스크 관리로 진정성 있는 인권존중을 실현하고자 하며,

```

---

### 5.2.2 FAISS 기반 MMR(Maximum marginal relevance search)

아래 코드에서는 `vectorstore.max_marginal_relevacne` 메서드를 통해 구현합니다.

1. 주어진 쿼리에 대해 유사도가 높은 문서들을 `fetch_k` 만큼 검색합니다.
2. 이 초기 문서 집합에서, 첫 번째 문서를 선택하고, 나머지 문서들에 대해 각각의 관련성 점수와 이미 선택된 문서들과의 유사도를 기반으로 MMR 점수를 계산합니다.
3. MMR 점수가 가장 높은 문서를 다음으로 선택하고, 이 과정을 `k` 개의 문서를 선택할 때까지 반복합니다.

`query` 에 대한 유사도 검색을 수행하고, 그 결과로부터 관련성은 높지만 내용이 중복되지 않는 문서들을 선택합니다. 결과적으로 `query`에 대해 최대한 다양하면서도 관련성 높은 4개의 문서를 선택하여 반환합니다. `mmr_docs` 는 이 4개의 문서를 포함하는 리스트이며, `mmr_docs[0].page_content` 는 이 중 첫 번째 문서의 내용을 출력합니다.

```python
mmr_docs = vectorstore.max_marginal_relevance_search(query, k=4, fetch_k=10)
print(len(mmr_docs))
print(mmr_docs[0].page_content)

```

```
실행 결과

4
카카오뱅크는 중대성 평가를 통해 도출된 6가지 중대 주제에 대한 활동을 공개하고 성과를 평가하며 지속가능한 경영 보고 체계를 수립하고 있습니다.
Management Approach
구분
ESG 경영 이행
환경경영체계
구축 및
운영 내재화
인권경영 확대
이사회 건전성 강화
포용적 금융을 통한
경제 및 사회적
가치 창출
정보보안 및
고객정보 관리
주요 성과
• 2022 MSCI 평가등급 BBB
• 2022 ESG 펀드 신규 투자
• 2022 ESG위원회 5회 개최
• 2023 ISO 14001 획득
• 2022 UN Global Compact 가입
• 2023 인권영향평가 시행
• 2022 ESG위원회 신설
• 2022 여성 사외이사 신규 선임
• 2022 중신용대출 취급 확대,
     개인사업자 신용대출 출시
• 2023 사회적 가치 측정 첫 시도
• 2022 정보보호 관련 인증 유지
     (ISMS, ISO 27001) 
• 개인정보 관련 인증 최초 획득
     (ISO 27701)
중대 주제 선정 이유 및 영향
카카오뱅크는 설립 단계부터 비즈니스의 환경적, 사회적 가치 창출을 고려하였습니다.
카카오뱅크는 기술과 혁신을 통해 기존 금융권에서 소외되어 있던 계층을 포함한 모든 사회
구성원의 금융 접근성을 향상했습니다.
카카오뱅크는 ‘더 나은 세상을 위해 환경을 지키는 것’을 환경경영의 목표로 삼고, 기업활동
전반에 걸쳐 발생하는 환경영향을 최소화하여 지속가능한 미래에 기여하고자 합니다.
특히, 카카오뱅크의 비즈니스가 지속적으로 성장함에 따라 온실가스 배출량을 비롯한 환경에의
영향 확대가 불가피한 현실에서 환경 리스크를 전사 차원의 주요 리스크로 인식하고 관리해야
함을 인지하고 있습니다.
카카오뱅크는 임직원 뿐만 아니라 협력사, 고객, 지역사회와 같은 모든 이해관계자의 인권이
마땅히 존중받는 ‘사람 중심 경영’을 실천하고자 노력합니다. 인권헌장이 바탕이 된 인권경영과
이해관계자의 참여 및 적극적인 리스크 관리로 진정성 있는 인권존중을 실현하고자 하며,

```

---

### 5.2.3 FAISS DB 를 로컬에 저장하기 

벡터 스토어 DB 를 저장하고 불러오는 과정을 통해 생성된 벡터 인덱스를 재사용할 수 있습니다. 이는 특히 대규모 데이터셋을 다루는 경우 시간과 자원을 절약하는데 큰 도움이 됩니다.

#### 벡터 스토어 저장하기

`vectorstore.save_local` 메서드를 사용하여 로컬 파일 시스템에 벡터 스토어를 저장할 수 있습니다. 이 메서드는 벡터 스토어의 상태를 지정된 경로에 파일로 저장합니다. 저장된 파일은 나중에 불러와서 동일한 벡터 스토어 상태를 재구성하는 데 사용될 수 있습니다. 저는 이 `vectorstore` 의 상태를 제 구글 드라이브의 `LangChain/db/faiss` 에 저장해보도록 하겠습니다.

```python
# save faiss db
vectorstore.save_local('./db/faiss')
```

#### 벡터 스토어 불러오기

저장된 벡터 스토어를 다시 불러오기 위해서는 `FAISS.load_local` 클래스 메소드를 사용합니다. 이 메소드는 저장된 파일 경로와 임베딩 모델을 인자로 받아, 해당 파일로부터 벡터 스토어를 재구성합니다. 여기서 `embeddings_model`은 벡터 스토어와 함께 사용될 임베딩 모델 인스턴스입니다.

```python
# load faiss db
db3 = FAISS.load_local('./db/faiss', embeddings_model)
```

#### 주의 사항

벡터 스토어를 불러올 때는 저장할 때 사용된 임베딩 모델과 동일한 모델을 사용해야 합니다. 벡터 스토어가 임베딩 벡터를 바탕으로 구축되기 때문에 다른 종류의 임베딩 모델을 사용하면 검색 결과를 보증하기 어렵습니다.

# 6. RAG Retriever

RAG (Retrieval Augmented Generation)에서 검색도구(Retrievers)는 벡터 저장소에서 문서를 검색하는 도구입니다. LangChain 은 간단한 의미 검색도구부터 성능 향상을 위해 고려된 다양한 검색 알고리즘을 지원합니다. 이번 챕터에서는 LangChain 에서 제공하는 다양한 검색도구에 대해서 알아보겠습니다.

## 6.1 Vector Store Retriver

벡터스토어 검색도구(Vector Store Retriever)를 사용하면 대량의 텍스트 데이터에서 관련 정보를 효율적으로 검색할 수 있습니다. 다음 코드에서는 LangChain 의 벡터 스토어와 임베딩 모델을 사용하여 문서들의 임베딩을 생성하고, 그 후 저장된 임베딩들을 기반으로 검색 쿼리에 가장 관련 있는 문서들을 검색하는 방법을 설명합니다.

### 사전 준비

이전 Chroma 와 FAISS 를 공부하면서 진행했던 pdf 문서를 로드하고, 텍스트 분리를 진행해 chunking 을 진행하고, 임베딩 모델을 이용해 문서 임베딩을 벡터스토어에 저장합니다. 이전에 진행했던 것과 같이 데이터는 "카카오뱅크 2022 지속가능경영보고서.pdf" 데이터를 사용하며, 임베딩 모델은 `HuggingFaceEmbeddings` 의 `jhgan/ko-sbert-nli` 를 사용하며, 벡터DB 는 FAISS 를 사용합니다. 전체 소스코드는 다음과 같습니다.

```python
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# Load data -> Text split

loader = PyMuPDFLoader("/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf")
data = loader.load()
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000,
    chunk_overlap=200,
    encoding_name='cl100k_base'
)

documents = text_splitter.split_documents(data)

# 벡터스토어 db 인스턴스를 생성
embeddings_model = HuggingFaceEmbeddings(
    model_name = "jhgan/ko-sroberta-multitask",
    model_kwargs = {'device' : 'cuda'},
    encode_kwargs = {'normalize_embeddings':True}
)

vectorstore = FAISS.from_documents(documents,
                                    embedding = embeddings_model,
                                    distance_strategy = DistanceStrategy.COSINE
                                    )

```

### 단일 문서 검색

검색 쿼리를 정의한 후, `as_retriever` 메소드를 사용하여 벡터스토어에서 Retriever 객체를 생성합니다. `search_kwargs`에서 `k:1`을 설정하여 가장 유사도가 높은 하나의 문서를 검색합니다.

```python
# 검색 쿼리
query = '카카오뱅크의 환경목표와 세부추진내용을 알려줘'

# 가장 유사도가 높은 문장을 하나만 추출
retriever = vectorstore.as_retriever(search_kwargs={'k': 1})

docs = retriever.get_relevant_documents(query)
docs[0]

```

```
실행 결과

/tmp/ipython-input-2101203039.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.
  docs = retriever.get_relevant_documents(query)
Document(id='46c68410-503b-436f-9285-736d729386d4', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 21}, page_content='카카오뱅크는 중대성 평가를 통해 도출된 6가지 중대 주제에 대한 활동을 공개하고 성과를 평가하며 지속가능한 경영 보고 체계를 수립하고 있습니다.\nManagement Approach\n구분\nESG 경영 이행\n환경경영체계\n구축 및\n운영 내재화\n인권경영 확대\n이사회 건전성 강화\n포용적 금융을 통한\n경제 및 사회적\n가치 창출\n정보보안 및\n고객정보 관리\n주요 성과\n• 2022 MSCI 평가등급 BBB\n• 2022 ESG 펀드 신규 투자\n• 2022 ESG위원회 5회 개최\n• 2023 ISO 14001 획득\n• 2022 UN Global Compact 가입\n• 2023 인권영향평가 시행\n• 2022 ESG위원회 신설\n• 2022 여성 사외이사 신규 선임\n• 2022 중신용대출 취급 확대,\n\t 개인사업자 신용대출 출시\n• 2023 사회적 가치 측정 첫 시도\n• 2022 정보보호 관련 인증 유지\n\t (ISMS, ISO 27001) \n• 개인정보 관련 인증 최초 획득\n\t (ISO 27701)\n중대 주제 선정 이유 및 영향\n카카오뱅크는 설립 단계부터 비즈니스의 환경적, 사회적 가치 창출을 고려하였습니다.\n카카오뱅크는 기술과 혁신을 통해 기존 금융권에서 소외되어 있던 계층을 포함한 모든 사회\n구성원의 금융 접근성을 향상했습니다.\n카카오뱅크는 ‘더 나은 세상을 위해 환경을 지키는 것’을 환경경영의 목표로 삼고, 기업활동\n전반에 걸쳐 발생하는 환경영향을 최소화하여 지속가능한 미래에 기여하고자 합니다.\n특히, 카카오뱅크의 비즈니스가 지속적으로 성장함에 따라 온실가스 배출량을 비롯한 환경에의\n영향 확대가 불가피한 현실에서 환경 리스크를 전사 차원의 주요 리스크로 인식하고 관리해야\n함을 인지하고 있습니다.\n카카오뱅크는 임직원 뿐만 아니라 협력사, 고객, 지역사회와 같은 모든 이해관계자의 인권이\n마땅히\xa0존중받는 ‘사람 중심 경영’을 실천하고자 노력합니다. 인권헌장이 바탕이 된 인권경영과\n이해관계자의 참여 및 적극적인 리스크 관리로 진정성 있는 인권존중을 실현하고자 하며,')
```

### MMR 검색

다양성을 고려한 MMR 검색을 사용하여 상위 5개 문서를 검색합니다. 여기서 `fetch_k : 50`는 후보 집합으로 선택되는 문서의 수를 의미하고, `k:5` 는 최종적으로 반환되는 문서의 수입니다. `lambda_mult : 0.5` 설정은 유사도와 다양성 사이에서 적용될 수준을 의미합니다. 0.5를 사용하면 중립적으로 적용하게 됩니다.

```python
# MMR - 다양성 고려 (lambda_mult = 0.5)
retriever = vectorstore.as_retriever(
    search_type='mmr',
    search_kwargs={'k': 5, 'fetch_k': 50}
)

docs = retriever.get_relevant_documents(query)
print(len(docs))
docs[0]

```

```
실행 결과

5
Document(id='46c68410-503b-436f-9285-736d729386d4', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 21}, page_content='카카오뱅크는 중대성 평가를 통해 도출된 6가지 중대 주제에 대한 활동을 공개하고 성과를 평가하며 지속가능한 경영 보고 체계를 수립하고 있습니다.\nManagement Approach\n구분\nESG 경영 이행\n환경경영체계\n구축 및\n운영 내재화\n인권경영 확대\n이사회 건전성 강화\n포용적 금융을 통한\n경제 및 사회적\n가치 창출\n정보보안 및\n고객정보 관리\n주요 성과\n• 2022 MSCI 평가등급 BBB\n• 2022 ESG 펀드 신규 투자\n• 2022 ESG위원회 5회 개최\n• 2023 ISO 14001 획득\n• 2022 UN Global Compact 가입\n• 2023 인권영향평가 시행\n• 2022 ESG위원회 신설\n• 2022 여성 사외이사 신규 선임\n• 2022 중신용대출 취급 확대,\n\t 개인사업자 신용대출 출시\n• 2023 사회적 가치 측정 첫 시도\n• 2022 정보보호 관련 인증 유지\n\t (ISMS, ISO 27001) \n• 개인정보 관련 인증 최초 획득\n\t (ISO 27701)\n중대 주제 선정 이유 및 영향\n카카오뱅크는 설립 단계부터 비즈니스의 환경적, 사회적 가치 창출을 고려하였습니다.\n카카오뱅크는 기술과 혁신을 통해 기존 금융권에서 소외되어 있던 계층을 포함한 모든 사회\n구성원의 금융 접근성을 향상했습니다.\n카카오뱅크는 ‘더 나은 세상을 위해 환경을 지키는 것’을 환경경영의 목표로 삼고, 기업활동\n전반에 걸쳐 발생하는 환경영향을 최소화하여 지속가능한 미래에 기여하고자 합니다.\n특히, 카카오뱅크의 비즈니스가 지속적으로 성장함에 따라 온실가스 배출량을 비롯한 환경에의\n영향 확대가 불가피한 현실에서 환경 리스크를 전사 차원의 주요 리스크로 인식하고 관리해야\n함을 인지하고 있습니다.\n카카오뱅크는 임직원 뿐만 아니라 협력사, 고객, 지역사회와 같은 모든 이해관계자의 인권이\n마땅히\xa0존중받는 ‘사람 중심 경영’을 실천하고자 노력합니다. 인권헌장이 바탕이 된 인권경영과\n이해관계자의 참여 및 적극적인 리스크 관리로 진정성 있는 인권존중을 실현하고자 하며,')
```

### 유사도 점수 임계값 기반 검색

이 방식은 설정한 `score_threshold` 유사도 점수 이상인 문서만을 대상으로 추출합니다. 여기서 임계값은 0.3으로 설정되어 있습니다. 이는 쿼리 문장과 최소한 0.3 이상의 유사도를 가진 문서만을 검색 결과로 반환하게 됩니다. 따라서 유사도가 높은 문서만 필터링하고 싶을 때 유용합니다.

```python
# Similarity score threshold (기준 스코어 이상인 문서를 대상으로 추출)
retriever = vectorstore.as_retriever(
    search_type='similarity_score_threshold',
    search_kwargs={'score_threshold': 0.3}
)

docs = retriever.get_relevant_documents(query)
print(len(docs))
docs[0]
```

실행 결과로 하나의 문서만 검색된 것을 확인할 수 있습니다.

```
실행 결과

1
Document(id='46c68410-503b-436f-9285-736d729386d4', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 21}, page_content='카카오뱅크는 중대성 평가를 통해 도출된 6가지 중대 주제에 대한 활동을 공개하고 성과를 평가하며 지속가능한 경영 보고 체계를 수립하고 있습니다.\nManagement Approach\n구분\nESG 경영 이행\n환경경영체계\n구축 및\n운영 내재화\n인권경영 확대\n이사회 건전성 강화\n포용적 금융을 통한\n경제 및 사회적\n가치 창출\n정보보안 및\n고객정보 관리\n주요 성과\n• 2022 MSCI 평가등급 BBB\n• 2022 ESG 펀드 신규 투자\n• 2022 ESG위원회 5회 개최\n• 2023 ISO 14001 획득\n• 2022 UN Global Compact 가입\n• 2023 인권영향평가 시행\n• 2022 ESG위원회 신설\n• 2022 여성 사외이사 신규 선임\n• 2022 중신용대출 취급 확대,\n\t 개인사업자 신용대출 출시\n• 2023 사회적 가치 측정 첫 시도\n• 2022 정보보호 관련 인증 유지\n\t (ISMS, ISO 27001) \n• 개인정보 관련 인증 최초 획득\n\t (ISO 27701)\n중대 주제 선정 이유 및 영향\n카카오뱅크는 설립 단계부터 비즈니스의 환경적, 사회적 가치 창출을 고려하였습니다.\n카카오뱅크는 기술과 혁신을 통해 기존 금융권에서 소외되어 있던 계층을 포함한 모든 사회\n구성원의 금융 접근성을 향상했습니다.\n카카오뱅크는 ‘더 나은 세상을 위해 환경을 지키는 것’을 환경경영의 목표로 삼고, 기업활동\n전반에 걸쳐 발생하는 환경영향을 최소화하여 지속가능한 미래에 기여하고자 합니다.\n특히, 카카오뱅크의 비즈니스가 지속적으로 성장함에 따라 온실가스 배출량을 비롯한 환경에의\n영향 확대가 불가피한 현실에서 환경 리스크를 전사 차원의 주요 리스크로 인식하고 관리해야\n함을 인지하고 있습니다.\n카카오뱅크는 임직원 뿐만 아니라 협력사, 고객, 지역사회와 같은 모든 이해관계자의 인권이\n마땅히\xa0존중받는 ‘사람 중심 경영’을 실천하고자 노력합니다. 인권헌장이 바탕이 된 인권경영과\n이해관계자의 참여 및 적극적인 리스크 관리로 진정성 있는 인권존중을 실현하고자 하며,')
```

### 메타데이터 필터링을 사용한 검색

메타데이터의 특정 필드에 대해서 기준(예:`'format', 'PDF 1.4'`)을 설정하고 조건을 충족하는 문서만을 필터링하여 검색합니다. 특정 형식이나 조건을 만족하는 문서를 검색할 때 유용합니다.

```python
# 문서 객체의 metadata 를 이용한 필터링

retriever = vectorstore.as_retriever(
    search_kwargs={'filter' : {'format':'PDF 1.4'}}
)

docs = retriever.get_relevant_documents(query)
print(len(docs))
docs[0]
```

```
실행 결과

4
Document(id='46c68410-503b-436f-9285-736d729386d4', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 21}, page_content='카카오뱅크는 중대성 평가를 통해 도출된 6가지 중대 주제에 대한 활동을 공개하고 성과를 평가하며 지속가능한 경영 보고 체계를 수립하고 있습니다.\nManagement Approach\n구분\nESG 경영 이행\n환경경영체계\n구축 및\n운영 내재화\n인권경영 확대\n이사회 건전성 강화\n포용적 금융을 통한\n경제 및 사회적\n가치 창출\n정보보안 및\n고객정보 관리\n주요 성과\n• 2022 MSCI 평가등급 BBB\n• 2022 ESG 펀드 신규 투자\n• 2022 ESG위원회 5회 개최\n• 2023 ISO 14001 획득\n• 2022 UN Global Compact 가입\n• 2023 인권영향평가 시행\n• 2022 ESG위원회 신설\n• 2022 여성 사외이사 신규 선임\n• 2022 중신용대출 취급 확대,\n\t 개인사업자 신용대출 출시\n• 2023 사회적 가치 측정 첫 시도\n• 2022 정보보호 관련 인증 유지\n\t (ISMS, ISO 27001) \n• 개인정보 관련 인증 최초 획득\n\t (ISO 27701)\n중대 주제 선정 이유 및 영향\n카카오뱅크는 설립 단계부터 비즈니스의 환경적, 사회적 가치 창출을 고려하였습니다.\n카카오뱅크는 기술과 혁신을 통해 기존 금융권에서 소외되어 있던 계층을 포함한 모든 사회\n구성원의 금융 접근성을 향상했습니다.\n카카오뱅크는 ‘더 나은 세상을 위해 환경을 지키는 것’을 환경경영의 목표로 삼고, 기업활동\n전반에 걸쳐 발생하는 환경영향을 최소화하여 지속가능한 미래에 기여하고자 합니다.\n특히, 카카오뱅크의 비즈니스가 지속적으로 성장함에 따라 온실가스 배출량을 비롯한 환경에의\n영향 확대가 불가피한 현실에서 환경 리스크를 전사 차원의 주요 리스크로 인식하고 관리해야\n함을 인지하고 있습니다.\n카카오뱅크는 임직원 뿐만 아니라 협력사, 고객, 지역사회와 같은 모든 이해관계자의 인권이\n마땅히\xa0존중받는 ‘사람 중심 경영’을 실천하고자 노력합니다. 인권헌장이 바탕이 된 인권경영과\n이해관계자의 참여 및 적극적인 리스크 관리로 진정성 있는 인권존중을 실현하고자 하며,')
```

### 답변 생성

이번에는 실제로 사용자 쿼리(`카카오뱅크의 환경목표와 세부추진내용을 알려줘`)에 대한 답변을 생성해보겠습니다. 벡터 저장소에서 문서를 검색한 다음, 이를 기반으로 ChatGPT 모델에 쿼리를 수행하는 end-to-end 프로세스를 구현합니다. 이 과정을 통해 사용자의 질문에 대한 의미적으로 관련이 있는 답변을 생성할 수 있습니다.

1. 검색(Retrieval) : `vectorstore.as_retriever` 를 사용하여 MMR 검색 방식으로 문서를 검색합니다. `search_kwargs`에 `k:5` 와 `lambda_mult:0.15` 를 설정하여 상위 5개의 관련성이 높으면서도 다양한 문서를 선택합니다.

2. 프롬프트 생성(Prompt) : `ChatPromptTemplate` 를 사용하여 쿼리에 대한 답변을 생성하기 위한 템플릿을 정의합니다. 여기서 `{context}`는 검색된 문서의 내용이고, `{question}`은 사용자의 쿼리입니다.

3. 모델(Model) : `ChatOpenAI`를 사용하여 OpenAI의 GPT 모델을 초기화합니다. 사용하는 모델은 `gpt-4o-mini`를 사용하며, `temperature` 를 0으로 설정하여 결정론적인 응답을 생성하고, `max_tokens`를 500으로 설정하여 응답의 길이를 제한합니다.

4. 문서 포맷팅(Formatting Docs) : 검색된 문서(`docs`)를 포맷팅하는 `format_docs` 함수를 정의합니다. 이 함수는 각 문서의 `page_content` 를 가져와 두 개의 문단 사이에 두 개의 줄바꿈을 삽입하여 문자열로 결합합니다.

5. 체인 실행(Chain Execution) : `prompt | llm | StrOutputParser()` 를 사용하여 LLM 체인을 구성하고, 실행합니다. 프롬프트를 통해 정의된 쿼리를 모델에 전달하고, 모델의 응답을 문자열로 파싱합니다.

6. 실행(Run) : `chain.invoke` 메소드를 사용하여 체인을 실행합니다. `context` 로는 포맷팅된 문서 내용이고, `question`은 사용자의 쿼리입니다. 최종 응답은 `response` 변수에 저장됩니다.

위 과정을 진행하기 전에 우선 단순히 LLM 에 "카카오뱅크의 환경목표와 세부추진내용을 알려줘" 라는 질문을 해보고 답변을 받아보도록 하겠습니다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    max_tokens=500,
    )

chain = llm | StrOutputParser()

response = chain.invoke("카카오뱅크의 환경목표와 세부추진내용을 알려줘")
print(response)
```

단순히 gpt-4o-mini 에게 물어보면 아래와 같은 답변을 얻을 수 있습니다.

```
실행 결과

카카오뱅크는 환경 지속 가능성을 중요시하며, 다양한 환경목표와 세부 추진내용을 수립하고 있습니다. 카카오뱅크의 환경목표와 관련된 구체적인 내용은 다음과 같습니다.

### 환경목표
1. **탄소중립 달성**: 카카오뱅크는 운영 과정에서 발생하는 탄소 배출량을 줄이고, 최종적으로 탄소중립을 목표로 하고 있습니다.
2. **친환경 금융 상품 확대**: 환경 친화적인 금융 상품 및 서비스 제공을 통해 지속 가능한 투자와 대출을 촉진하고 있습니다.
3. **자원 효율성 증대**: 운영 과정에서 자원의 효율성을 높이고, 폐기물을 줄이는 방향으로 운영 방침을 설정하고 있습니다.

### 세부 추진 내용
1. **온실가스 감축 활동**: 에너지 효율성을 개선하고 재생 가능 에너지를 활용하여 온실가스 배출량을 지속적으로 모니터링하고 관리합니다.
2. **ESG 투자 및 대출**: 기업의 환경, 사회, 거버넌스(ESG) 지표를 반영한 금융 상품 개발을 통해 지속 가능한 프로젝트에 대한 투자를 장려합니다.
3. **디지털 소통 강화**: 비대면 서비스와 디지털 플랫폼을 통해 종이 사용을 줄이고, 고객과의 소통에서 환경친화적인 방법을 도입합니다.
4. **사회 공헌 프로그램**: 환경 보호 활동과 연계한 다양한 사회 공헌 프로그램을 운영하여 지역사회와 함께 지속 가능한 환경을 만들어가는 데 기여합니다.

### 결론
카카오뱅크는 지속 가능한 금융을 실현하기 위해 다양한 노력을 기울이고 있으며, 이를 통해 환경보호와 경제적 가치 창출을 동시에 추구하고 있습니다. 더 구체적인 내용은 카카오뱅크의 공식 홈페이지나 지속 가능성 보고서를 통해 확인할 수 있습니다.
```

```python

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Retrieval
retriever = vectorstore.as_retriever(
    search_type = 'mmr',
    search_kwargs={'k':5, 'lambda_mult':0.15}
)

query = "카카오뱅크의 환경목표와 세부추진내용을 알려줘"

docs = retriever.get_relevant_documents(query)

# Prompt
template = '''Answer the question based only on the following context:
{context}

Question: {question}
'''

prompt = ChatPromptTemplate.from_template(template)

# Model
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    max_tokens=1000,
)

def format_docs(docs):
  return '\n\n'.join([d.page_content for d in docs])

# Chain
chain = prompt | llm | StrOutputParser()

# Run

response = chain.invoke({'context':(format_docs(docs)), 'question' : query})
print(response)
```

RAG 를 적용할 경우 이전에 문서 검색을 할 때 출력되던 문서의 내용들이 포함되어 좀 더 구체적인 정보들이 많이 포함된 것을 확인할 수 있습니다.

```
실행 결과

카카오뱅크의 환경목표는 '더 나은 세상을 위해 환경을 지키는 것'으로, 기업활동 전반에 걸쳐 발생하는 환경영향을 최소화하여 지속가능한 미래를 위해 노력하는 것입니다. 

세부 추진 내용은 다음과 같습니다:

1. **환경경영 관리체계 구축**: 환경경영 전략과 정책을 수립하고 이를 이행하기 위한 기반을 마련합니다.
2. **환경 리스크 및 성과 관리**: 환경 리스크를 관리하고 성과를 모니터링합니다.
3. **단기, 중기, 장기 과제 도출 및 이행**: 환경영향 저감을 위한 다양한 과제를 단계적으로 이행합니다.
4. **ESG팀 및 환경 TF 구성**: 환경경영 이행을 전담하는 조직을 구성하여 환경경영 업무를 고도화하고 혁신 과제를 발굴합니다.
5. **환경 방침 수립**: 전사 환경경영 정책, 녹색 구매 지침, 환경 지표 설정 및 성과 관리, 자원 사용량 관리, 기후변화 포함 환경 리스크 관리체계를 마련합니다.
6. **온실가스 배출 관리**: Scope 1, 2, 3 온실가스 배출량을 관리하고 저감하기 위한 노력을 지속합니다.

이러한 목표와 추진 내용을 통해 카카오뱅크는 환경문제에 적극적으로 대응하고 지속 가능한 경영을 실현하고자 합니다.
```

---

## 6.2 Multi Query Retriever

멀티 쿼리 검색도구(MultiQueryRetriever)는 벡터스토어 검색도구(Vector Store Retriever)의 한계를 극복하기 위해 고안된 방법입니다. 사용자가 입력한 쿼리의 의미를 다각도로 포착하여 ㄱ머색 효율성을 높이고, LLM 을 활용하여 사용자에게 보다 관련성 높고 정확한 정보를 제공하는 것을 목표로 합니다.

단일 쿼리의 의미를 다양한 관점으로 확장하여 멀티 쿼리를 자동 생성하고, 이러한 모든 쿼리에 대한 검색 결과를 결합하여 처리합니다. 다양한 문장을 생성하기 위하여 LLM 을 사용하여 사용자의 입력 문장을 다양한 관점으로 패러프레이징(Paraphrasing)하는 방식으로 구현됩니다.

다음 코드는 `MultiQueryRetriever` 클래스를 사용하여 여러 쿼리에 기반한 문서 검색 과정을 설정하고 실행하는 방법을 보여줍니다.

1. MultiQueryRetriever 설정 : `from_llm` 메서드를 통해, 기존 벡터 저장소 검색도구(vectorstore.as_retriever())와 LLM 모델을 결합하여 `MultiQueryRetriever` 인스턴스를 생성합니다. 이때 LLM은 다양한 관점의 쿼리를 생성하는 데 사용됩니다.

2. 로깅 설정 : 로깅을 설정하여 `MultiQueryRetriever`에 의해 생성되고 실행되는 쿼리들에 대한 정보를 로그로 기록하고 확인할 수 있습니다. 검색 과정에서 어떤 쿼리들이 생성되고 사용되었는지 이해하는 데 도움이 됩니다.

3. 문서 검색 실행 : `get_relevant_documents` 메서드를 사용하여 주어진 사용자 쿼리(`question`)에 대해 멀티 쿼리 기반의 문서 검색을 실행합니다. 생성된 모든 쿼리에 대해 문서를 검색하고, 중복을 제거하여 고유한 문서들만을 결과로 반환합니다.

4. 결과 확인 : 검색을 통해 반환된 고유 문서들의 수를 확인합니다. 멀티 쿼리 접근 방식을 통해 얼마나 많은 관련 문서가 검색되었는지를 나타냅니다.

```python
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

# Load data

loader = PyMuPDFLoader("/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf")
data = loader.load()
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000,
    chunk_overlap=200,
    encoding_name = 'cl100k_base'
)

documents = text_splitter.split_documents(data)

embeddings_model = HuggingFaceEmbeddings(
    model_name = "jhgan/ko-sroberta-multitask",
    model_kwargs = {'device' : 'cpu'},
    encode_kwargs = {'normalize_embeddings' : True}
)

vectorstore = FAISS.from_documents(documents,
                                   embedding = embeddings_model,
                                   distance_strategy = DistanceStrategy.COSINE
                                   )

# 멀티 쿼리 생성
question = "카카오뱅크의 최근 영업실적을 알려줘."

llm = ChatOpenAI(
    model = "gpt-4o-mini",
    temperature=0,
    max_tokens=500,
)

retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(), llm=llm
)

import logging

logging.basicConfig()
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

unique_docs = retriever_from_llm.get_relevant_documents(query=question)
len(unique_docs)

```

```
실행 결과

INFO:langchain.retrievers.multi_query:Generated queries: ['카카오뱅크의 최신 재무 성과에 대한 정보를 제공해줄 수 있나요?  ', '최근 카카오뱅크의 영업 실적에 대한 업데이트가 있나요?  ', '카카오뱅크의 최근 분기 영업 실적은 어떻게 되나요?']
6
```

앞에서 정의한 MultiQueryRetriever(`retriever_from_llm`)를 활용하여 여러 쿼리를 생성하고 검색된 문서를 기반으로 사용자 질문에 답변하는 과정을 살펴봅니다. 우선 LLM 에 RAG 를 적용하지 않고 단순히 `query` 를 던져 답을 받아 보도록 하겠습니다.

```python
from langchain_core.output_parsers import StrOutputParser

chain = llm | StroutputParser

response = chain.invoke(query)

print(response)

```

RAG 를 적용하지 않고 LLM 에게 질문을 던지면 정보가 없어 답변을 해주지 못하는 것을 확인할 수 있습니다.

```
실행 결과

죄송하지만, 2023년 10월 이후의 카카오뱅크의 영업 실적에 대한 구체적인 정보를 제공할 수 없습니다. 하지만 카카오뱅크는 일반적으로 디지털 뱅킹 서비스와 관련된 다양한 금융 상품을 제공하며, 최근 몇 년간 빠른 성장세를 보였습니다. 최신 영업 실적이나 재무 정보는 카카오뱅크의 공식 웹사이트나 금융 관련 뉴스 매체를 통해 확인하실 수 있습니다.
```

```python
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

# Prompt
template = '''Answer the question based only on the following context:
{context}

Question: {question}
'''

prompt = ChatPromptTemplate.from_template(template)

def format_docs(docs):
  return '\n\n'.join([d.page_content for d in docs])

# Chain
chain = (
    {'context' : retriever_from_llm | format_docs, 'question' : RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Run
response = chain.invoke("카카오뱅크의 최근 영업실적을 요약해서 알려주세요")
print(response)
```

놀랍게도 LLM 에 질문만 했을 때에는 정보가 없어 전혀 답변을 하지 못했지만 구체적인 정보를 포함해주니 문서의 내용을 토대로 답변 해주는 것을 확인할 수 있습니다.

```
실행 결과

카카오뱅크는 2022년 말 기준으로 총 고객 수가 2,042만 명에 달하며, 수신 규모는 33.1조 원, 여신 규모는 27.9조 원을 기록했습니다. 특히, 차별적인 수신 상품을 통해 자금조달의 경쟁우위를 확보하였고, 주택담보대출 상품의 성공적인 안착으로 리테일 뱅킹 포트폴리오를 완성했습니다. 또한, 개인사업자 뱅킹 서비스 출시로 기업 금융의 발판을 마련하였으며, 제휴를 통한 연계 서비스도 성장하여 증권계좌개설 실적이 누적 614만 좌, 연계 대출이 누적 5.7조 원에 달했습니다. 청소년을 위한 카카오뱅크 mini는 2022년 말 기준으로 누적 가입 고객 수가 160만 명 이상에 이르렀습니다.
```

---

## 6.3 Contextual compression

컨텍스트 압축 기법은 검색된 문서 중에서 쿼리와 관련된 정보만을 추출하여 반환하는 것을 목표로 합니다. 쿼리와 무관한 정보를 제거하는 방식으로 답변의 품질을 높이고 비용을 줄일 수 있습니다. 먼저 기본 검색기를 정의합니다.

### 기본 검색기(Base Retriever) 정의

1. 기본 검색기 설정

  `vectorstore.as_retriever` 함수를 사용하여 기본 검색기를 설정합니다. 여기서 `search_type='mmr'` 와 `search_kwargs={'k':7, 'fetch_k':20}`는 검색 방식을 설정합니다. `mmr` 검색 방식은 다양성을 고려한 검색 결과를 제공하여, 단순히 가장 관련성 높은 문서만ㅁ 반환하는 대신 다양한 관점에서 관련된 문서들을 선택합니다.

2. 쿼리 처리 및 문서 검색

  `base_retriever.get_relevant_documents(question)` 함수를 사용하여 주어진 쿼리에 대한 관련 문서를 검색합니다. 이 함수는 쿼리와 관련성 높은 문서들을 반환합니다.

3. 결과 출력

  `print(len(docs))`를 통해 검색된 문서의 수를 출력합니다.

```python
# 기본 검색기

question = "카카오뱅크의 최근 영업실적을 알려줘"

llm = ChatOpenAI(
    model = "gpt-4o-mini",
    temperature=0,
    max_tokens=500,
)

base_retriever = vectorstore.as_retriever(
    search_type='mmr',
    search_kwargs={'k':7, 'fetch_k':20}
)

docs = base_retriever.get_relevant_documents(question)
print(len(docs))
```

```
실행 결과

7
```

### 문서 압축기의 구성과 작동 방식

문서 압축기는 기본 검색기로부터 얻은 문서들을 더욱 효율적으로 압축하여, 쿼리와 가장 관련이 깊은 내용만을 추려내는 것을 목표로 합니다. `LLMChainExtractor` 와 `ContextualCompressionRetriever` 클래스를 사용합니다.

1. LLMChainExtractor 설정

  `LLMChainExtractor.from_llm(llm)`를 사용하여 문서 압축기를 설정합니다. LLM 을 사용하여 문서 내용을 압축합니다.

2. ContextualCompressionRetriever 설정

  `ContextualCompressionRetriever` 인스턴스를 생성할 때, `base_compressor` 와 `base_retriever` 를 인자로 제공합니다. `base_compressor` 는 앞서 설정한 `LLMChainExtractor` 인스턴스이며, `base_retriever` 는 기본 검색기 인스턴스입니다 . 이 두 구성 요소를 결합하여 검색된 문서들을 압축하는 과정을 처리합니다.

3. 압축된 문서 검색

  `compression_retriever.get_relevant_documnets(question)` 함수를 사용하여 주어진 쿼리에 대한 압축된 문서들을 검색합니다. 기본 검색기를 통해 얻은 문서들을 문서 압축기를 사용하여 내용을 압축하고, 쿼리와 가장 관련된 내용만을 추려냅니다.

4. 결과 출력

  `print(len(compressed_docs))`를 통해 압축된 문서의 수를 출력합니다.

```python
# 문서 압축기를 연결하여 구성

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=base_retriever
)

compressed_docs = compression_retriever.get_relevant_documents(question)
print(len(compressed_docs))

```

저는 wiki docs 와는 다르게 `카카오뱅크의 최근 영업실적을 알려줘` 라는 쿼리를 사용했을 때 문서 압축기로 추출되는 문서가 없어 쿼리를 `카카오뱅크가 생각하는 환경문제에 대해서 알려줘`로 바꾸어서 진행해 보았습니다.

```
실행 결과

6
```

```python
compressed_docs
```

최종적으로 압축된 문서의 내용을 출력하여 확인합니다. 이 방식은 효율적인 정보 검색과 내용 압축을 통해 RAG 답변의 품질을 높일 수 있는 유용한 접근법입니다. 기본 검색기로부터 유사도 기반으로 추출된 문서들 중에서 실제로 사용자의 쿼리와 관련된 정보만을 압축하여, 정보를 더욱 집약적으로 제공하는 것이 목적입니다.

```
실행 결과

[Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 25}, page_content='카카오뱅크는 아래와 같은 환경방침을 수립하여 운영합니다.\n• 전사 환경경영 정책 수립\n• 녹색 구매 지침 수립\n• 환경 지표 설정 및 성과 관리\n• 용수, 폐기물, 에너지 등 자원 사용량 관리\n• 기후변화를 포함한 환경 리스크 관리체계 마련\n• Scope 1&2&3 온실가스 배출량 모니터링\n• 탄소 가격 도입을 통한 환경 비용 관리\n• 신재생 에너지 사용 확대\n• 녹색채권 발행 기반 마련\n• 환경경영 조직 및 관리 체계 구축\n• 환경영향평가 체계 구축'),
 Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 14}, page_content='카카오뱅크는 무점포 영업구조를 가진 모바일 전문 은행으로 혁신적인 금융과 기술을 통해 출범 시점부터 저탄소 경영과 환경보호 등 친환경 가치를 실현하고 있습니다.  \n탄소발자국  \nZERO  \n• 영업점 非 구축 및 非 운영  \n• 모바일 only 정책으로 고객의 은행 방문 無  \n• 현금 보관 및 현수송에 소요되는 에너지 無  \n종이 사용  \nZERO  \n• 100% 모바일 통장 운영 및 디지털명세서 발급  \n• 대출 신청, 심사, 결과 안내까지 전과정 디지털화  \n• 수표와 어음에 사용되는 종이가 필요 없는 전자송금 방식  \n그린 IT  \n• 에너지 효율성과 기후변화를 고려한 가상 서버  \n(Virtual Machine Server) 등의 혁신 기술 활용  \n환경경영  \n• ISO 14001(환경경영시스템) 인증 취득  \n• 금융배출량 포함 탄소배출량 측정 및 공개  \n• 친환경 투자 확대'),
 Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 94}, page_content='카카오뱅크는 기후리스크가 주요 리스크임을 인지하고 포트폴리오의 배출량을 산정 및 공개하고 있고 내·외부의 ESG 리스크 및 기회를 식별하기 위해 이중 중대성 평가를 시행하고 있습니다. 카카오뱅크는 투자의사결정 시 환경, 사회, 지배구조 요소가 충분히 고려되도록 투자 가이드라인에 ESG 요소를 포함합니다. 카카오뱅크는 2022년 친환경 건축물 인증제도(LEED)를 받은 판교테크원에 입주하여 용수 사용량을 저감하고 고효율 LED 및 자동점등 센서를 사용하여 에너지를 저감했습니다. 고객 대상으로는 교통카드 온라인 충전 서비스를 제공하여 종이 영수증 발행을 감소시켰으며, 추후에는 고객의 친환경 활동에 대해 우대금리를 제공하는 상품을 개발할 예정입니다. 카카오뱅크는 중요 리스크에 기후리스크를 포함한 바 있으며, 이를 근거로 향후 기업 대출 강화 등 포트폴리오 다변화에 따른 기후리스크 평가 시스템을 구축해 나갈 예정입니다. 연 1회 환경영향평가를 실시하여 이해관계자 요구사항을 반영한 주요 리스크와 기회를 식별 및 관리하고 있습니다. 도출된 이슈를 해결하기 위한 목표와 세부 추진계획을 세워 환경영향을 최소화하고 있으며, 관련 지표들을 모니터링하고 있습니다.'),
 Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 61}, page_content='•\t탄소 배출 절감 도시 숲 조성  \n•\t코트디부아르 재활용 플라스틱 벽돌 학교 지원 등'),
 Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 29}, page_content='카카오뱅크는 기후위기에 대응하고 임직원의 환경의식을 제고하기 위해 다양한 환경 교육과 캠페인을 실시하고 있습니다. 전자문서 보고체계를 구축하여 페이퍼리스 문화를 조성하였습니다. 또한 제안서를 메일로만 접수받고 제안 발표 자료 출력을 최소화하는 등 협력사의 환경영향까지 고려하고 있습니다. 카카오뱅크는 폐기물 발생량 최소화를 위해 분리배출 시스템을 운영하며 임직원들이 자원순환에 적극적으로 동참할 수 있도록 다양한 캠페인을 실시하고 있습니다. 또한 미사용 사무용 가구와 노트북을 사회복지시설에 기부하여 자원을 재활용하였습니다. 사내 일회용품 사용을 줄이기 위해 생분해성 친환경 컵과 빨대를 사용하고 있으며 텀블러 사용을 활성화하기 위해 세척기를 도입하였습니다. 또한, 사내 카페에서 텀블러 사용 시 할인 혜택을 제공하는 등 일회용품 사용 저감을 위한 다양한 활동을 운영하고 있습니다.'),
 Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Macintosh)', 'creationdate': '2023-06-21T17:01:54+09:00', 'source': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'file_path': '/content/drive/MyDrive/LangChain/카카오뱅크 2022 지속가능경영보고서.pdf', 'total_pages': 99, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-06-21T17:02:34+09:00', 'trapped': '', 'modDate': "D:20230621170234+09'00'", 'creationDate': "D:20230621170154+09'00'", 'page': 94}, page_content='연 1회 환경영향평가를 실시하여 이해관계자 요구사항을 반영한 주요 리스크와 기회를 식별 및 관리하고 있습니다. 도출된 이슈를 해결하기 위한 목표와 세부 추진계획을 세워 환경영향을 최소화하고 있으며, 관련 지표들을 모니터링하고 있습니다. 카카오뱅크는 의무적인 기후 또는 환경 규제(배출권 거래제 등)에 적용을 받지 않고, Scope 1&2로 배출되는 탄소배출량이 배출권 거래제에서 요구하는 법적 수치보다 현저히 적습니다. 그러나 환경영향을 정확하게 파악하고 개선하기 위해 탄소배출량을 모니터링하고 금융배출량을 포함한 업스트림, 다운스트림 Scope 3 탄소배출량을 산정하여 공개하고 있습니다. 카카오뱅크는 사업 확장으로 미래 배출량을 예측하기는 어려우나, 에너지 및 탄소배출량 집약도를 지속적으로 모니터링하고 있습니다. 기후변화의 영향도를 보다 면밀하게 분석하고 관리하기 위해 과거 Scope2 탄소배출량 공개 이외에 Scope 1과 3도 추가로 공개하는 등 관리 체계를 고도화 하고 있습니다. 카카오뱅크는 사업 확장으로 인해 예상되는 환경리스크 및 기회도 식별하며 사업을 추진할 계획입니다.')]
```

# 7. 마치며

본격적으로 RAG 적용 방법에 대해서 알아보았습니다. 실제 문서들을 Langchain 을 이용해 로드해보고, 로드한 문서들을 Chunking 하여 문서 임베딩으로 변환하고, 벡터DB에 저장한 다음 LLM 에 질문을 할 때 질문과 유사한 문서들을 벡터DB 에서 추출해서 질문과 함께 문서들을 묶어 LLM 에 질문을 하도록 해보았습니다. 이번 포스트를 작성하며 실제로 RAG 가 동작하는 것을 확인 했고, 전반적으로 Langchain 을 이용해서 어떻게 RAG 를 구축하면 되는지 어렴풋하게 알게 된 것 같습니다. 마치며, 본 포스트는 제가 개인적으로 공부하기 위해 작성한 포스트라 잘못된 내용이 있을 수 있습니다. 혹시나 잘못된 내용이 있다면 댓글 작성을 부탁드리며, 오타나 다른 질문 글도 환영입니다. 긴 긁 읽어주셔서 감사합니다.
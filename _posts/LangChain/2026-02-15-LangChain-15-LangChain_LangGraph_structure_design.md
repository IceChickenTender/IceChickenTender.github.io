---
title: "[LangChain] LangChain 15. LangGraphì˜ êµ¬ì¡° ì„¤ê³„"
categories:
  - LangChain
tags:
  - LangChain
  
use_math: true  
toc: true
toc_sticky: true
toc_label: "LangGraphì˜ êµ¬ì¡° ì„¤ê³„"
---

# 1. ê°œìš”

LangGraphë¥¼ ì´ìš©í•œ ê¸°ë³¸ì ì¸ RAG êµ¬ì¡°ë¶€í„° AI Agentë¥¼ í™œìš©í•œ ì—¬ëŸ¬ êµ¬ì¡° ì„¤ê³„ë“¤ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

# 2. LangGraphë¥¼ ì´ìš©í•œ Naive RAG êµ¬ì¡° ì„¤ê³„

ì˜ˆì œ ì½”ë“œ ì‹¤í–‰ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¶€í„° ì§„í–‰í•´ ì¤ë‹ˆë‹¤.

```bin
!pip install -U langchain-core langchain-community langchain-openai langchain-classic pdfplumber faiss-cpu langchain-teddynote
```

ìš°ì„  ì˜ˆì œ ì‹¤í–‰ì— í•„ìš”í•œ í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜í•´ ì¤ë‹ˆë‹¤.

```python
from langchain_core.prompts import load_prompt
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

from abc import ABC, abstractmethod
from operator import itemgetter
from langchain_classic import hub


class RetrievalChain(ABC):
    def __init__(self):
        self.source_uri = None
        self.k = 10

    @abstractmethod
    def load_documents(self, source_uris):
        """loaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        pass

    @abstractmethod
    def create_text_splitter(self):
        """text splitterë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        pass

    def split_documents(self, docs, text_splitter):
        """text splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤."""
        return text_splitter.split_documents(docs)

    def create_embedding(self):
        return OpenAIEmbeddings(model="text-embedding-3-small")

    def create_vectorstore(self, split_docs):
        return FAISS.from_documents(
            documents=split_docs, embedding=self.create_embedding()
        )

    def create_retriever(self, vectorstore):
        # MMRì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        dense_retriever = vectorstore.as_retriever(
            search_type="similarity", search_kwargs={"k": self.k}
        )
        return dense_retriever

    def create_model(self):
        return ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    def create_prompt(self):
        return hub.pull("teddynote/rag-prompt-chat-history")

    @staticmethod
    def format_docs(docs):
        return "\n".join(docs)

    def create_chain(self):
        docs = self.load_documents(self.source_uri)
        text_splitter = self.create_text_splitter()
        split_docs = self.split_documents(docs, text_splitter)
        self.vectorstore = self.create_vectorstore(split_docs)
        self.retriever = self.create_retriever(self.vectorstore)
        model = self.create_model()
        prompt = self.create_prompt()
        self.chain = (
            {
                "question": itemgetter("question"),
                "context": itemgetter("context"),
                "chat_history": itemgetter("chat_history"),
            }
            | prompt
            | model
            | StrOutputParser()
        )
        return self
```

```python
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import List, Annotated


class PDFRetrievalChain(RetrievalChain):
    def __init__(self, source_uri: Annotated[str, "Source URI"]):
        self.source_uri = source_uri
        self.k = 10

    def load_documents(self, source_uris: List[str]):
        docs = []
        for source_uri in source_uris:
            loader = PDFPlumberLoader(source_uri)
            docs.extend(loader.load())

        return docs

    def create_text_splitter(self):
        return RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
```

```python
def format_docs(docs):
    return "\n".join(
        [
            f"<document><content>{doc.page_content}</content><source>{doc.metadata['source']}</source><page>{int(doc.metadata['page'])+1}</page></document>"
            for doc in docs
        ]
    )


def format_searched_docs(docs):
    return "\n".join(
        [
            f"<document><content>{doc['content']}</content><source>{doc['url']}</source></document>"
            for doc in docs
        ]
    )


def format_task(tasks):
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    task_time_pairs = []

    # ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ê° í•­ëª©ì„ ì²˜ë¦¬
    for item in tasks:
        # ì½œë¡ (:) ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë¶„ë¦¬
        task, time_str = item.rsplit(":", 1)
        # 'ì‹œê°„' ë¬¸ìì—´ì„ ì œê±°í•˜ê³  ì •ìˆ˜ë¡œ ë³€í™˜
        time = int(time_str.replace("ì‹œê°„", "").strip())
        # í•  ì¼ê³¼ ì‹œê°„ì„ íŠœí”Œë¡œ ë§Œë“¤ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        task_time_pairs.append((task, time))

    # ê²°ê³¼ ì¶œë ¥
    return task_time_pairs
```

## 2.1 ê¸°ë³¸ PDF ê¸°ë°˜ Retrieval Chain ìƒì„±

ì´ë²ˆ ì˜ˆì œì—ì„œëŠ” PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Retrieval Chainì„ ìƒì„±í•©ë‹ˆë‹¤. ê°€ì¥ ë‹¨ìˆœí•œ êµ¬ì¡°ì˜ Retrieval Chainì…ë‹ˆë‹¤. ë‹¨, LangGraphì—ì„œëŠ” Retrieverì™€ Chainì„ ë”°ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ë˜ì•¼ ê° ë…¸ë“œë³„ë¡œ ì„¸ë¶€ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# pdf ë¬¸ì„œë¥¼ ë¡œë“œ í•©ë‹ˆë‹¤.
pdf = PDFRetrievalChain(["/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf"]).create_chain()

# retrieverì™€ chainì„ ìƒì„±í•©ë‹ˆë‹¤.
pdf_retriever = pdf.retriever
pdf_chain = pdf.chain
```

ë¨¼ì €, pdf_retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

```python
search_result = pdf_retriever.invoke("ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
search_result
```

```
Output:
[Document(id='45ac8bbf-d738-4751-a3bc-6afa8c85b05a', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 13, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='1. ì •ì±…/ë²•ì œ 2. ê¸°ì—…/ì‚°ì—… 3. ê¸°ìˆ /ì—°êµ¬ 4. ì¸ë ¥/êµìœ¡\nêµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— 20ì–µ ë‹¬ëŸ¬ íˆ¬ìë¡œ ìƒì„± AI í˜‘ë ¥ ê°•í™”\nKEY Contents\nn êµ¬ê¸€ì´ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ íˆ¬ìì— í•©ì˜í•˜ê³  5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí–ˆìœ¼ë©°, ì•¤ìŠ¤ë¡œí”½ì€\nêµ¬ê¸€ê³¼ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì‚¬ìš© ê³„ì•½ë„ ì²´ê²°\nn 3ëŒ€ í´ë¼ìš°ë“œ ì‚¬ì—…ìì¸ êµ¬ê¸€, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸, ì•„ë§ˆì¡´ì€ ì°¨ì„¸ëŒ€ AI ëª¨ë¸ì˜ ëŒ€í‘œ ê¸°ì—…ì¸\nì•¤ìŠ¤ë¡œí”½ ë° ì˜¤í”ˆAIì™€ í˜‘ë ¥ì„ í™•ëŒ€í•˜ëŠ” ì¶”ì„¸\nÂ£êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ íˆ¬ì í•©ì˜ ë° í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì œê³µ'),
 Document(id='59a28176-1366-4b68-b8b0-f46cdb93b5e2', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 13, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='Â£êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ íˆ¬ì í•©ì˜ ë° í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì œê³µ\nn êµ¬ê¸€ì´ 2023ë…„ 10ì›” 27ì¼ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í–ˆìœ¼ë©°, ì´ ì¤‘ 5ì–µ\në‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí•˜ê³  í–¥í›„ 15ì–µ ë‹¬ëŸ¬ë¥¼ ì¶”ê°€ë¡œ íˆ¬ìí•  ë°©ì¹¨\nâˆ™ êµ¬ê¸€ì€ 2023ë…„ 2ì›” ì•¤ìŠ¤ë¡œí”½ì— ì´ë¯¸ 5ì–µ 5,000ë§Œ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•œ ë°” ìˆìœ¼ë©°, ì•„ë§ˆì¡´ë„ ì§€ë‚œ 9ì›”\nì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 40ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ì ê³„íšì„ ê³µê°œ\nâˆ™ í•œí¸, 2023ë…„ 11ì›” 8ì¼ ë¸”ë£¸ë²„ê·¸ ë³´ë„ì— ë”°ë¥´ë©´ ì•¤ìŠ¤ë¡œí”½ì€ êµ¬ê¸€ì˜ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì‚¬ìš©ì„ ìœ„í•´\n4ë…„ê°„ 30ì–µ ë‹¬ëŸ¬ ê·œëª¨ì˜ ê³„ì•½ì„ ì²´ê²°'),
 Document(id='f97ead3b-f369-4cdd-8202-f2f76f42a19c', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 13, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='4ë…„ê°„ 30ì–µ ë‹¬ëŸ¬ ê·œëª¨ì˜ ê³„ì•½ì„ ì²´ê²°\nâˆ™ ì˜¤í”ˆAI ì°½ì—…ì ê·¸ë£¹ì˜ ì¼ì›ì´ì—ˆë˜ ë‹¤ë¦¬ì˜¤(Dario Amodei)ì™€ ë‹¤ë‹ˆì—˜ë¼ ì•„ëª¨ë°ì´(Daniela Amodei)\në‚¨ë§¤ê°€ 2021ë…„ ì„¤ë¦½í•œ ì•¤ìŠ¤ë¡œí”½ì€ ì±—GPTì˜ ëŒ€í•­ë§ˆ â€˜í´ë¡œë“œ(Claude)â€™ LLMì„ ê°œë°œ\nn ì•„ë§ˆì¡´ê³¼ êµ¬ê¸€ì˜ ì•¤ìŠ¤ë¡œí”½ íˆ¬ìì— ì•ì„œ, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ëŠ” ì°¨ì„¸ëŒ€ AI ëª¨ë¸ì˜ ëŒ€í‘œ ì£¼ìì¸ ì˜¤í”ˆ\nAIì™€ í˜‘ë ¥ì„ í™•ëŒ€\nâˆ™ ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ëŠ” ì˜¤í”ˆAIì— ì•ì„œ íˆ¬ìí•œ 30ì–µ ë‹¬ëŸ¬ì— ë”í•´ 2023ë…„ 1ì›” ì¶”ê°€ë¡œ 100ì–µ ë‹¬ëŸ¬ë¥¼'),
 Document(id='e9de9949-3336-4882-a2a2-7db6059e7019', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 13, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='íˆ¬ìí•˜ê¸°ë¡œ í•˜ë©´ì„œ ì˜¤í”ˆAIì˜ ì§€ë¶„ 49%ë¥¼ í™•ë³´í–ˆìœ¼ë©°, ì˜¤í”ˆAIëŠ” ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì˜ ì• ì €(Azure)\ní´ë¼ìš°ë“œ í”Œë«í¼ì„ ì‚¬ìš©í•´ AI ëª¨ë¸ì„ í›ˆë ¨\nÂ£êµ¬ê¸€, í´ë¼ìš°ë“œ ê²½ìŸë ¥ ê°•í™”ë¥¼ ìœ„í•´ ìƒì„± AI íˆ¬ì í™•ëŒ€\nn êµ¬ê¸€ì€ ìˆ˜ìµë¥ ì´ ë†’ì€ í´ë¼ìš°ë“œ ì»´í“¨íŒ… ì‹œì¥ì—ì„œ ì•„ë§ˆì¡´ê³¼ ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ë¥¼ ë”°ë¼ì¡ê³ ì ìƒì„± AIë¥¼\ní†µí•œ ê¸°ì—… ê³ ê°ì˜ í´ë¼ìš°ë“œ ì§€ì¶œ í™•ëŒ€ë¥¼ ìœ„í•´ AI íˆ¬ìë¥¼ ì§€ì†\nâˆ™ êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ ì™¸ì—ë„ AI ë™ì˜ìƒ ì œì‘ ë„êµ¬ë¥¼ ê°œë°œí•˜ëŠ” ëŸ°ì›¨ì´(Runway)ì™€ ì˜¤í”ˆì†ŒìŠ¤ ì†Œí”„íŠ¸ì›¨ì–´\nê¸°ì—… í—ˆê¹… í˜ì´ìŠ¤(Hugging Face)ì—ë„ íˆ¬ì'),
 Document(id='46c47637-f00e-4dad-80b5-72edb180544f', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='â–¹ ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·10\nâ–¹ êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— 20ì–µ ë‹¬ëŸ¬ íˆ¬ìë¡œ ìƒì„± AI í˜‘ë ¥ ê°•í™” Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·11\nâ–¹ IDC, 2027ë…„ AI ì†Œí”„íŠ¸ì›¨ì–´ ë§¤ì¶œ 2,500ì–µ ë‹¬ëŸ¬ ëŒíŒŒ ì „ë§Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·12'),
 Document(id='08dc4fef-5061-452a-8399-30dce7366e25', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='1. ì •ì±…/ë²•ì œ 2. ê¸°ì—…/ì‚°ì—… 3. ê¸°ìˆ /ì—°êµ¬ 4. ì¸ë ¥/êµìœ¡\në¯¸êµ­ í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼, 1,000ë§Œ ë‹¬ëŸ¬ ê·œëª¨ì˜ AI ì•ˆì „ ê¸°ê¸ˆ ì¡°ì„±\nKEY Contents\nn êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸, ì˜¤í”ˆAIê°€ ì°¸ì—¬í•˜ëŠ” í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼ì´ ìì„ ë‹¨ì²´ì™€ í•¨ê»˜ AI\nì•ˆì „ ì—°êµ¬ë¥¼ ìœ„í•œ 1,000ë§Œ ë‹¬ëŸ¬ ê·œëª¨ì˜ AI ì•ˆì „ ê¸°ê¸ˆì„ ì¡°ì„±\nn í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼ì€ AI ëª¨ë¸ì˜ ì·¨ì•½ì ì„ ë°œê²¬í•˜ê³  ê²€ì¦í•˜ëŠ” ë ˆë“œíŒ€ í™œë™ì„ ì§€ì›í•˜ê¸° ìœ„í•œ\nëª¨ë¸ í‰ê°€ ê¸°ë²• ê°œë°œì— ìê¸ˆì„ ì¤‘ì  ì§€ì›í•  ê³„íš'),
 Document(id='99e555b5-2f1d-4f3b-8939-c7b5e9424632', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='1,000ë§Œ ë‹¬ëŸ¬ ì´ìƒì„ ê¸°ë¶€\nâˆ™ ë˜í•œ ì‹ ê¸°ìˆ ì˜ ê±°ë²„ë„ŒìŠ¤ì™€ ì•ˆì „ ë¶„ì•¼ì—ì„œ ì „ë¬¸ì„±ì„ ê°–ì¶˜ ë¸Œë£¨í‚¹ìŠ¤ ì—°êµ¬ì†Œ ì¶œì‹ ì˜ í¬ë¦¬ìŠ¤ ë©”ì„œë¡¤(Chris\nMeserole)ì„ í¬ëŸ¼ì˜ ìƒë¬´ì´ì‚¬ë¡œ ì„ëª…\nn ìµœê·¼ AI ê¸°ìˆ ì´ ê¸‰ì†íˆ ë°œì „í•˜ë©´ì„œ AI ì•ˆì „ì— ê´€í•œ ì—°êµ¬ê°€ ë¶€ì¡±í•œ ì‹œì ì—, í¬ëŸ¼ì€ ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œ\ní•˜ê¸° ìœ„í•´ AI ì•ˆì „ ê¸°ê¸ˆì„ ì¡°ì„±\nâˆ™ ì°¸ì—¬ì‚¬ë“¤ì€ ì§€ë‚œ 7ì›” ë°±ì•…ê´€ ì£¼ì¬ì˜ AI ì•ˆì „ ì„œì•½ì—ì„œ ì™¸ë¶€ìì˜ AI ì‹œìŠ¤í…œ ì·¨ì•½ì  ë°œê²¬ê³¼ ì‹ ê³ ë¥¼\nì´‰ì§„í•˜ê¸°ë¡œ ì•½ì†í–ˆìœ¼ë©°, ì•½ì†ì„ ì´í–‰í•˜ê¸° ìœ„í•´ ê¸°ê¸ˆì„ í™œìš©í•´ ì™¸ë¶€ ì—°êµ¬ì§‘ë‹¨ì˜ AI ì‹œìŠ¤í…œ í‰ê°€ì—\nìê¸ˆì„ ì§€ì›í•  ê³„íš'),
 Document(id='ae3c18d8-fe91-48de-910e-ad9522a7a55a', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='ëª¨ë¸ í‰ê°€ ê¸°ë²• ê°œë°œì— ìê¸ˆì„ ì¤‘ì  ì§€ì›í•  ê³„íš\nÂ£í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼, ìì„ ë‹¨ì²´ì™€ í•¨ê»˜ AI ì•ˆì „ ì—°êµ¬ë¥¼ ìœ„í•œ ê¸°ê¸ˆ ì¡°ì„±\nn êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸, ì˜¤í”ˆAIê°€ ì¶œë²”í•œ í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼ì´ 2023ë…„ 10ì›” 25ì¼ AI ì•ˆì „\nì—°êµ¬ë¥¼ ìœ„í•œ ê¸°ê¸ˆì„ ì¡°ì„±í•œë‹¤ê³  ë°œí‘œ\nâˆ™ ì°¸ì—¬ì‚¬ë“¤ì€ ë§¥ê±°ë²ˆ ì¬ë‹¨(Patrick J. McGovern Foundation), ë°ì´ë¹„ë“œ ì•¤ ë£¨ì‹¤ íŒ¨ì»¤ë“œ ì¬ë‹¨(The\nDavid and Lucile Packard Foundation) ë“±ì˜ ìì„ ë‹¨ì²´ì™€ í•¨ê»˜ AI ì•ˆì „ ì—°êµ¬ë¥¼ ìœ„í•œ ê¸°ê¸ˆì—'),
 Document(id='5ce535c1-ffa7-4a67-b75f-e435cbe5ce2f', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='2. ê¸°ì—…/ì‚°ì—…\nâ–¹ ë¯¸êµ­ í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼, 1,000ë§Œ ë‹¬ëŸ¬ ê·œëª¨ì˜ AI ì•ˆì „ ê¸°ê¸ˆ ì¡°ì„±Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·7\nâ–¹ ì½”íˆì–´, ë°ì´í„° íˆ¬ëª…ì„± í™•ë³´ë¥¼ ìœ„í•œ ë°ì´í„° ì¶œì²˜ íƒìƒ‰ê¸° ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·8\nâ–¹ ì•Œë¦¬ë°”ë°” í´ë¼ìš°ë“œ, ìµœì‹  LLM â€˜í†µì´ì¹˜ì—”ì› 2.0â€™ ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·9'),
 Document(id='0640388f-60e1-4f66-896c-9cb44faa15cf', metadata={'source': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': '/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': "D:20231208132838+09'00'", 'ModDate': "D:20231208132838+09'00'", 'PDFVersion': '1.4'}, page_content='ìê¸ˆì„ ì§€ì›í•  ê³„íš\nÂ£AI ì•ˆì „ ê¸°ê¸ˆìœ¼ë¡œ AI ë ˆë“œíŒ€ì„ ìœ„í•œ ëª¨ë¸ í‰ê°€ ê¸°ë²• ê°œë°œì„ ì¤‘ì  ì§€ì›í•  ê³„íš\nn í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼ì€ AI ì•ˆì „ ê¸°ê¸ˆì„ í†µí•´ AI ë ˆë“œíŒ€ í™œë™ì„ ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ í‰ê°€ ê¸°ë²•ì˜ ê°œë°œì„\nì¤‘ì  ì§€ì›í•  ì˜ˆì •\nâˆ™ í¬ëŸ¼ì— ë”°ë¥´ë©´ AI ë ˆë“œíŒ€ì— ëŒ€í•œ ìê¸ˆ ì§€ì›ì€ AI ëª¨ë¸ì˜ ì•ˆì „ê³¼ ë³´ì•ˆ ê¸°ì¤€ì˜ ê°œì„ ê³¼ í•¨ê»˜ AI ì‹œìŠ¤í…œ\nìœ„í—˜ ëŒ€ì‘ ë°©ì•ˆì— ê´€í•œ ì‚°ì—…ê³„ì™€ ì •ë¶€, ì‹œë¯¼ì‚¬íšŒì˜ í†µì°°ë ¥ í™•ë³´ì— ë„ì›€ì´ ë  ì „ë§ìœ¼ë¡œ, í¬ëŸ¼ì€ í–¥í›„ ëª‡\në‹¬ ì•ˆì— ê¸°ê¸ˆ ì§€ì›ì„ ìœ„í•œ ì œì•ˆ ìš”ì²­ì„ ë°›ì„ ê³„íš')]
```

ì´ì „ì— ê²€ìƒ‰í•œ ê²°ê³¼ë¥¼ chainì˜ contextë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

```python
# ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
answer = pdf_chain.invoke(
    {
        "question": "ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "context": search_result,
        "chat_history": [],
    }
)
print(answer)
```

```
Output:
êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í•˜ì˜€ìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ 15ì–µ ë‹¬ëŸ¬ë¥¼ í–¥í›„ íˆ¬ìí•  ê³„íšì…ë‹ˆë‹¤. 

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 13)
```

## 2.2 State ì •ì˜

```python
from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages

# GraphState ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    question: Annotated[str, "Question"] # ì§ˆë¬¸
    context: Annotated[str, "Context"] # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
    answer: Annotated[str, "Answer"] # ë‹µë³€
    messages: Annotated[str, add_messages] # ë©”ì‹œì§€(ëˆ„ì ë˜ëŠ” list)
```

## 2.3 ë…¸ë“œ(Node) ì •ì˜

```python
from langchain_teddynote.messages import messages_to_history

# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ
def retrieve_document(state: GraphState) -> GraphState:
    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    latest_question = state["question"]

    # ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    retrieved_docs = pdf_retriever.invoke(latest_question)

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤. (í”„ë¡¬í”„íŠ¸ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê¸° ìœ„í•¨)
    retrieved_docs = format_docs(retrieved_docs)

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    return GraphState(context=retrieved_docs)

# ë‹µë³€ ìƒì„± ë…¸ë“œ
def llm_answer(state: GraphState) -> GraphState:
    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    latest_question = state["question"]

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    context = state["context"]

    # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    response = pdf_chain.invoke(
        {
            "question": latest_question,
            "context": context,
            "chat_history": messages_to_history(state["messages"]),
        }
    )
    # ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸, ë‹µë³€) ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.
    return GraphState(
        answer = response, messages=[("user", latest_question), ("assistant", response)]
    )
```

## 2.4 Edge ì—°ê²°

```python
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

# ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì •ì˜
workflow.add_node("retrieve", retrieve_document)
workflow.add_node("llm_answer", llm_answer)

# ì—£ì§€ ì •ì˜
workflow.add_edge("retrieve", "llm_answer") # ê²€ìƒ‰ -> ë‹µë³€
workflow.add_edge("llm_answer", END) # ë‹µë³€ -> ì¢…ë£Œ

# ê·¸ë˜í”„ ì§„ì…ì  ì„¤ì •
workflow.set_entry_point("retrieve")

# ì²´í¬í¬ì¸í„° ì„¤ì •
memory = MemorySaver()

# ì»´íŒŒì¼
app = workflow.compile(checkpointer=memory)
```

ì»´íŒŒì¼í•œ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.graphs import visualize_graph

visualize_graph(app)
```

<div align="center">
  <img src="/assets/images/langchain/15/naive_rag_graph.png" width="25%" height="20%"/>
</div>

<br>

## 2.5 ê·¸ë˜í”„ ì‹¤í–‰

```python
from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import stream_graph, random_uuid

# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id": random_uuid()})

# ì§ˆë¬¸ ì…ë ¥
inputs = GraphState(question="ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.")

# ê·¸ë˜í”„ ì‹¤í–‰
stream_graph(app, inputs, config, node_names=["llm_answer"])
```

```
Output:

==================================================
ğŸ”„ Node: llm_answer ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í•˜ì˜€ìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí–ˆìŠµë‹ˆë‹¤. ì•„ë§ˆì¡´ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 40ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ì ê³„íšì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 14)
```

```python
outputs = app.get_state(config).values

print(f'Question: {outputs["question"]}')
print("===" * 20)
print(f'Answer:\n{outputs["answer"]}')
```

```
Output:
Question: ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.
============================================================
Answer:
êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í•˜ì˜€ìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí–ˆìŠµë‹ˆë‹¤. ì•„ë§ˆì¡´ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 40ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ì ê³„íšì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 14)
```

# 3. ê´€ë ¨ì„± ê²€ì‚¬(Relevance Checker) ëª¨ë“ˆ ì¶”ê°€

ì´ì „ì— ì§„í–‰í–ˆë˜ Naive RAG Graphì— ë‹µë³€ì˜ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± ì²´í¬ë¥¼ ì¶”ê°€í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë™ì¼í•˜ë©°, ì˜ˆì œ ì‹¤í–‰ì— í•„ìš”í•œ ê¸°ë³¸ì ì¸ ì½”ë“œë“¤ì€ ì´ì „ í•­ëª©ì„ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

## 3.1 ê¸°ë³¸ PDF ê¸°ë°˜ Retrieval Chain ìƒì„±

```python
# PDF ë¬¸ì„œ ë¡œë“œ
pdf = PDFRetrievalChain(["/content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf"]).create_chain()

# retrieverì™€ chainì„ ìƒì„±í•©ë‹ˆë‹¤.
pdf_retriever = pdf.retriever
pdf_chain = pdf.chain
```

## 3.2 Graph State ì •ì˜

ì´ì „ ì˜ˆì œì—ì„œ ë™ì¼í•˜ì§€ë§Œ ê´€ë ¨ì„± ì²´í¬ë¥¼ ìœ„í•œ relevance í•­ëª©ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.

```python
from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages

# GraphState ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    question: Annotated[str, "Question"] # ì§ˆë¬¸
    context: Annotated[str, "Context"] # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
    answer: Annotated[str, "Answer"] # ë‹µë³€
    messages: Annotated[str, add_messages] # ë©”ì‹œì§€(ëˆ„ì ë˜ëŠ” list)
    relevance: Annotated[str, "Relevance"] # ê´€ë ¨ì„±
```

## 3.3 ë…¸ë“œ(Node) ì •ì˜

ì´ì „ Naive RAG Graphì—ì„œ ê´€ë ¨ì„± ì²´í¬ë¥¼ ìœ„í•œ `relevance_check`ì™€ `is_relevant` ë…¸ë“œë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
from langchain_openai import ChatOpenAI
from langchain_teddynote.evaluator import GroundednessChecker
from langchain_teddynote.messages import messages_to_history

# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ
def retrieve_document(state: GraphState) -> GraphState:
    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    latest_question = state["question"]

    # ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    retrieved_docs = pdf_retriever.invoke(latest_question)

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤. (í”„ë¡¬í”„íŠ¸ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê¸° ìœ„í•¨)
    retrieved_docs = format_docs(retrieved_docs)

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    return GraphState(context=retrieved_docs)

# ë‹µë³€ ìƒì„± ë…¸ë“œ
def llm_answer(state: GraphState) -> GraphState:
    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    latest_question = state["question"]

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    context = state["context"]

    # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    response = pdf_chain.invoke(
        {
            "question": latest_question,
            "context": context,
            "chat_history": messages_to_history(state["messages"]),
        }
    )

    # ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸ , ë‹µë³€) ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.
    return GraphState(
        answer=response, messages=[("user", latest_question), ("assistant", response)]
    )

# ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œ
def relevance_check(state: GraphState) -> GraphState:
    # ê´€ë ¨ì„± í‰ê°€ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    question_answer_relevant = GroundednessChecker(
        llm=ChatOpenAI(model="gpt-4o-mini", temperature=0), target="question-retrieval"
    ).create()

    # ê´€ë ¨ì„± ì²´í¬ë¥¼ ì‹¤í–‰("yes" or "no")
    response = question_answer_relevant.invoke(
        {"question": state["question"], "context": state["context"]}
    )

    print("==== [RELEVANCE CHECK] ====")  
    print(response.score) 

    return GraphState(relevance=response.score)

# ê´€ë ¨ì„± ì²´í¬í•˜ëŠ” í•¨ìˆ˜(router)
def is_relevant(state: GraphState) -> GraphState:
    return state["relevance"]
```

## 3.4 Graph Edge ì •ì˜

```python
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("retrieve", retrieve_document)

# ê´€ë ¨ì„± ì²´í¬ ë…¸ë“œ ì¶”ê°€
workflow.add_node("relevance_check", relevance_check)
workflow.add_node("llm_answer", llm_answer)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge("retrieve", "relevance_check")

# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
workflow.add_conditional_edges(
    "relevance_check",
    is_relevant,
    {
        "yes": "llm_answer",
        "no": "retrieve",
    },
)

# ê·¸ë˜í”„ ì§„ì…ì  ì„¤ì •
workflow.set_entry_point("retrieve")

# ì²´í¬í¬ì¸í„° ì„¤ì •
memory = MemorySaver()

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile(checkpointer=memory)
```

ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.graphs import visualize_graph  

visualize_graph(app)  
```

<div align="center">
  <img src="/assets/images/langchain/15/relevance_checker_graph.png" width="25%" height="20%"/>
</div>

<br>

## 3.5 ê·¸ë˜í”„ ì‹¤í–‰

```python
from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import stream_graph, random_uuid

# config ì„¤ì •
config = RunnableConfig(recursion_limit=20, configurable={"thread_id": random_uuid()})

# ì§ˆë¬¸ ì…ë ¥
inputs = GraphState(question="ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.")

# ê·¸ë˜í”„ ì‹¤í–‰
stream_graph(app, inputs, config, node_names = ["relevance_check", "llm_answer"])
```

```
Output:

==================================================
ğŸ”„ Node: relevance_check ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
{"score":"yes"}==== [RELEVANCE CHECK] ====
yes

==================================================
ğŸ”„ Node: llm_answer ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í•˜ì˜€ìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí–ˆìŠµë‹ˆë‹¤. ì•„ë§ˆì¡´ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 40ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ì ê³„íšì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 14)
```

```python
outputs = app.get_state(config).values  

print(f'Question: {outputs["question"]}')  
print("===" * 20)  
print(f'Answer:\n{outputs["answer"]}')  
```

```
Output:
Question: ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.
============================================================
Answer:
êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í•˜ì˜€ìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí–ˆìŠµë‹ˆë‹¤. ì•„ë§ˆì¡´ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 40ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ì ê³„íšì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 14)
```

```python
print(outputs["relevance"])  
```

```
Output:
yes
```

ê·¸ëŸ¬ë©´ ê´€ë ¨ì„± ê²€ì‚¬ë¥¼ ì‹¤íŒ¨í–ˆì„ ê²½ìš°ì— ëŒ€í•´ì„œë„ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

```python
from langgraph.errors import GraphRecursionError
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(recursion_limit=10, configurable={"thread_id": random_uuid()})

# ì§ˆë¬¸ ì…ë ¥
inputs = GraphState(question="í…Œë””ë…¸íŠ¸ì˜ ë­ì²´ì¸ íŠœí† ë¦¬ì–¼ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.")

try:
    stream_graph(app, inputs, config, node_names=["relevance_check", "llm_answer"])
except GraphRecursionError as recursion_error:
    print(f"GraphRecursionError: {recursion_error}")
```

```
Output:

==================================================
ğŸ”„ Node: relevance_check ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
{"score":"no"}==== [RELEVANCE CHECK] ====
no
{"score":"no"}==== [RELEVANCE CHECK] ====
no
{"score":"no"}==== [RELEVANCE CHECK] ====
no
{"score":"no"}==== [RELEVANCE CHECK] ====
no
{"score":"no==== [RELEVANCE CHECK] ===="}
no
GraphRecursionError: Recursion limit of 10 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT
```

# 4. ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ ì¶”ê°€

ê´€ë ¨ì„± ê²€ì‚¬ ë…¸ë“œë¥¼ ì¶”ê°€í•œ ê·¸ë˜í”„ì— ì´ì œ ê´€ë ¨ì„± ê²€ì‚¬ë¥¼ ì‹¤íŒ¨í–ˆì„ ê²½ìš° ì›¹ ê²€ìƒ‰ì„ ì§„í–‰í•˜ë„ë¡ í•˜ëŠ” ê·¸ë˜í”„ë¡œ ë°œì „ì‹œì¼œ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì´ì „ì˜ ê´€ë ¨ì„± ê²€ì‚¬ ì˜ˆì œì—ì„œ ì›¹ ê²€ìƒ‰ ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì¶”ê°€í•œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ì„¤ì¹˜í•´ì•¼ í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì´ì „ ì˜ˆì œì—ì„œ ì‚¬ìš©ë˜ì–´ ì¤‘ë³µë˜ëŠ” ì½”ë“œë“¤ì€ ìƒëµí–ˆìŠµë‹ˆë‹¤.

## 4.1 ê²€ìƒ‰ ë…¸ë“œ ì¶”ê°€

`TavilySearch` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Web Searchë¥¼ ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.tools.tavily import TavilySearch

# WebSearch ë…¸ë“œ
def web_search(state: GraphState) -> GraphState:
    # ê²€ìƒ‰ ë„êµ¬ ìƒì„±
    tavily_tool = TavilySearch()

    search_query = state["question"]

    # ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ ì˜ˆì œ
    search_result = tavily_tool.search(
        query=search_query,
        topic="news",
        days=1,
        max_results=3,
        format_output=True,
    )

    return GraphState(context="\n".join(search_result))
```

## 4.2 Graph Edge ì •ì˜

```python
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("retrieve", retrieve_document)
workflow.add_node("relevance_check", relevance_check)
workflow.add_node("llm_answer", llm_answer)

# Web Search ë…¸ë“œ ì¶”ê°€
workflow.add_node("web_search", web_search) # ê²€ìƒ‰ -> ê´€ë ¨ì„± ì²´í¬

# ì—£ì§€ ì¶”ê°€
workflow.add_edge("retrieve", "relevance_check")

# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
workflow.add_conditional_edges(
    "relevance_check",
    is_relevant,
    {
        "yes": "llm_answer",
        "no": "web_search",
    },
)

workflow.add_edge("web_search", "llm_answer")
workflow.add_edge("llm_answer", END)

# ê·¸ë˜í”„ ì§„ì…ì  ì„¤ì •
workflow.set_entry_point("retrieve")

# ì²´í¬ í¬ì¸í„° ì„¤ì •
memory = MemorySaver()

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile(checkpointer=memory)
```

ì»´íŒŒì¼í•œ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.graphs import visualize_graph  

visualize_graph(app)  
```

<div align="center">
  <img src="/assets/images/langchain/15/web_search_add_graph.png" width="25%" height="20%"/>
</div>

<br>

## 4.3 ê·¸ë˜í”„ ì‹¤í–‰

```python
from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import stream_graph, random_uuid


# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
config = RunnableConfig(recursion_limit=10, configurable={"thread_id": random_uuid()})

# ì§ˆë¬¸ ì…ë ¥
inputs = GraphState(question="ë„ë„ë“œ íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹")

# ê·¸ë˜í”„ ì‹¤í–‰
stream_graph(app, inputs, config, node_names=["relevance_check", "llm_answer"])
```

ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ë©´ ì¿¼ë¦¬ê°€ ë¬¸ì„œì™€ëŠ” ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì´ê¸° ë•Œë¬¸ì— ê´€ë ¨ì„± ê²€ì‚¬ë¥¼ ì§„í–‰í•˜ë©´ "no"ê°€ ë‚˜ì˜¤ê²Œ ë˜ê³  ì›¹ ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ì—¬ ì›¸ ê²€ìƒ‰ ê²°ê³¼ì™€ ì¿¼ë¦¬ë¥¼ í•¨ê»˜ LLMì— ì œê³µí•˜ì—¬ ë‹µë³€ì„ ì–»ì–´ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
Output:

==================================================
ğŸ”„ Node: relevance_check ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
{"score":"no"}==== [RELEVANCE CHECK] ====
no

==================================================
ğŸ”„ Node: llm_answer ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
ë„ë„ë“œ íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì€ ìµœê·¼ì— ìƒˆë¡œ ì°½ì„¤í•œ í‰í™” ìœ„ì›íšŒ(Board of Peace)ì˜ ì¼ì›ë“¤ì´ ê°€ì ì¬ê±´ì„ ìœ„í•´ 50ì–µ ë‹¬ëŸ¬ë¥¼ ì•½ì†í–ˆë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ì´ ìœ„ì›íšŒëŠ” êµ­ì œ ì•ˆì •í™” ë° ê²½ì°°ë ¥ì— ìˆ˜ì²œ ëª…ì˜ ì¸ë ¥ì„ ë°°ì¹˜í•  ê³„íšì…ë‹ˆë‹¤. íŠ¸ëŸ¼í”„ëŠ” ì´ ìœ„ì›íšŒê°€ ì—­ì‚¬ìƒ ê°€ì¥ ì¤‘ìš”í•œ êµ­ì œ ê¸°êµ¬ê°€ ë  ê²ƒì´ë¼ê³  ê°•ì¡°í–ˆìŠµë‹ˆë‹¤. 

**Source**
- [Newsweek](https://www.newsweek.com/donald-trump-ranking-approval-rating-presidents-yougov-11526370)
- [AP News](https://apnews.com/article/trump-gaza-board-peace-reconstruction-stabilization-685251b3e8f24cf8779d6fe3f5f2ca04)
```

# 5. ì¿¼ë¦¬ ì¬ì‘ì„± ëª¨ë“ˆ ì¶”ê°€

ì´ì „ê¹Œì§€ ì›¹ ê²€ìƒ‰ ë…¸ë“œë¥¼ ì¶”ê°€í•œ ê·¸ë˜í”„ì— ì¿¼ë¦¬ ì¬ì‘ì„± ë…¸ë“œë¥¼ ì¶”ê°€í•œ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

## 5.1 Query Rewrite ë…¸ë“œ ì¶”ê°€

Queryë¥¼ ì¬ì‘ì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ê¸°ì¡´ì˜ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•©ë‹ˆë‹¤.

```python
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Query Rewrite í”„ë¡¬í”„íŠ¸ ì •ì˜
re_write_prompt = PromptTemplate(
    template = """Reformulate the given question to enhance its effectiveness for vectorstore retrieval.

- Analyze the initial question to identify areas for improvement such as specificity, clarity, and relevance.
- Consider the context and potential keywords that would optimize retrieval.
- Maintain the intent of the original question while enhancing its structure and vocabulary.

# Steps

1. **Understand the Original Question**: Identify the core intent and any keywords.
2. **Enhance Clarity**: Simplify language and ensure the question is direct and to the point.
3. **Optimize for Retrieval**: Add or rearrange keywords for better alignment with vectorstore indexing.
4. **Review**: Ensure the improved question accurately reflects the original intent and is free of ambiguity.

# Output Format

- Provide a single, improved question.
- Do not include any introductory or explanatory text; only the reformulated question.

# Examples

**Input**: 
"What are the benefits of using renewable energy sources over fossil fuels?"

**Output**: 
"How do renewable energy sources compare to fossil fuels in terms of benefits?"

**Input**: 
"How does climate change impact polar bear populations?"

**Output**: 
"What effects does climate change have on polar bear populations?"

# Notes

- Ensure the improved question is concise and contextually relevant.
- Avoid altering the fundamental intent or meaning of the original question.


[REMEMBER] Re-written question should be in the same language as the original question.

# Here is the original question that needs to be rewritten:
{question}
""",
    input_variables=["generation", "question"],
)

question_rewriter = (
    re_write_prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0) | StrOutputParser()
)
```

```python
# ì§ˆë¬¸ ì¬ì‘ì„±
question = "ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ë¯¸êµ­ê¸°ì—…"

question_rewriter.invoke({"question": question})
```

ì¿¼ë¦¬ê°€ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
Output:
ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ë¯¸êµ­ ê¸°ì—…ì€ ì–´ë–¤ ê³³ë“¤ì´ ìˆë‚˜ìš”?
```

```python
# Query Rewrite ë…¸ë“œ ì •ì˜
def query_rewirte(state: GraphState) -> GraphState:
    latest_question = state["question"]
    question_rewritten = question_rewriter.invoke({"question": latest_question})
    return GraphState(question=question_rewritten)
```

## 5.2 Graph Edge ì •ì˜

```python
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("retrieve", retrieve_document)
workflow.add_node("relevance_check", relevance_check)
workflow.add_node("llm_answer", llm_answer)
workflow.add_node("web_search", web_search)

# Query Rewrite ë…¸ë“œ ì¶”ê°€
workflow.add_node("query_rewrite", query_rewirte)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge("query_rewrite", "retrieve")
workflow.add_edge("retrieve", "relevance_check")

# ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "relevance_check",
    is_relevant,
    {
        "yes": "llm_answer",
        "no": "web_search",
    },
)

workflow.add_edge("web_search", "llm_answer")
workflow.add_edge("llm_answer", END)

# ê·¸ë˜í”„ ì§„ì…ì  ì„¤ì •
workflow.set_entry_point("query_rewrite")

# ì²´í¬í¬ì¸í„° ì„¤ì •
memory = MemorySaver()

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile(checkpointer=memory)
```

ì»´íŒŒì¼í•œ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

```python 
from langchain_teddynote.graphs import visualize_graph

visualize_graph(app)
```

<div align="center">
  <img src="/assets/images/langchain/15/query_rewrite_add_graph.png" width="25%" height="20%"/>
</div>

<br>

## 5.3 ê·¸ë˜í”„ ì‹¤í–‰

```python
from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import stream_graph, random_uuid

# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
config = RunnableConfig(recursion_limit=10, configurable={"thread_id": random_uuid()})

# ì§ˆë¬¸ ì…ë ¥
inputs = GraphState(question="ì•¤ìŠ¤ë¡œí”½ íˆ¬ì ê¸ˆì•¡")

# ê·¸ë˜í”„ ì‹¤í–‰
stream_graph(app, inputs, config, node_names=["query_rewrite", "llm_answer"])
```

ì…ë ¥ ì¿¼ë¦¬ì¸ "ì•¤ìŠ¤ë¡œí”½ íˆ¬ì ê¸ˆì•¡"ì´ "ì•¤ìŠ¤ë¡œí”½ì— ëŒ€í•œ íˆ¬ì ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?"ë¡œ ë³€ê²½ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
Output:

==================================================
ğŸ”„ Node: query_rewrite ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
ì•¤ìŠ¤ë¡œí”½ì— ëŒ€í•œ íˆ¬ì ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?==== [RELEVANCE CHECK] ====
yes

==================================================
ğŸ”„ Node: llm_answer ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í•˜ì˜€ìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí•˜ê³  í–¥í›„ 15ì–µ ë‹¬ëŸ¬ë¥¼ ì¶”ê°€ë¡œ íˆ¬ìí•  ê³„íšì…ë‹ˆë‹¤.

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 14)
```

# 6. Agentic RAG

ì¼ë°˜ì ì¸ Naive RAGë¥¼ ë„˜ì–´ Agent ê¸°ë°˜ì˜ Agentic RAGë¥¼ êµ¬í˜„í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

## 6.1 Retrieve ë„êµ¬ ì •ì˜

Agentì— ì‚¬ìš©í•  Retrieve ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì˜ˆì œ ì½”ë“œì— ì‚¬ìš©ëœ `pdf_retriever`ëŠ” ì´ì „ ì˜ˆì œë“¤ì—ì„œ ì •ì˜í•œ ê²ƒì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. 

```python
from langchain_core.tools.retriever import create_retriever_tool
from langchain_core.prompts import PromptTemplate

# PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ë„êµ¬ ìƒì„±
retriever_tool = create_retriever_tool(
    pdf_retriever,
    "pdf_retriever",
    "Search and return information about SPRI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2023.",
    document_prompt=PromptTemplate.from_template(
        "<document><context>{page_content}</context><metadata><source>{source}</source><page>{page}</page></metadata></document>"
    ),
)

# ìƒì„±ëœ ê²€ìƒ‰ ë„êµ¬ë¥¼ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì—¬ ì—ì´ì „íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
tools = [retriever_tool]
```

## 6.2 Agent Graph ì •ì˜

### 6.2.1 Agent State ì •ì˜

```python
from typing import Annotated, Sequence, TypedDict
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

# ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” íƒ€ì… ë”•ì…”ë„ˆë¦¬, ë©”ì‹œì§€ ì‹œí€€ìŠ¤ë¥¼ ê´€ë¦¬í•˜ê³  ì¶”ê°€ ë™ì‘ ì •ì˜
class AgentState(TypedDict):
    # add_messages reducer í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ì‹œí€€ìŠ¤ë¥¼ ê´€ë¦¬
    messages: Annotated[Sequence[BaseMessage], add_messages]
```

## 6.2.2 ë…¸ë“œì™€ ì—£ì§€ ì •ì˜

```python
from typing import Literal
from langchain_classic import hub
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import tools_condition
from langchain_teddynote.models import get_model_name, LLMs

# ìµœì‹  ëª¨ë¸ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
MODEL_NAME = get_model_name(LLMs.GPT4)

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class grade(BaseModel):
    """A binary score for relevance checks"""

    binary_score: str = Field(
        description="Response 'yes' if the document is relevant to the question or 'no' if it is not."
    )

def grade_documents(state) -> Literal["generate", "rewrite"]:
    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)

    # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ LLM ì„¤ì •
    llm_with_tool = model.with_structured_output(grade)

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    prompt = PromptTemplate(
        template="""You are a grader assessing relevance of a retrieved document to a user question. \n 
        Here is the retrieved document: \n\n {context} \n\n
        Here is the user question: {question} \n
        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n
        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.""",
        input_variables=["context", "question"],
    )

    # llm + tool ë°”ì¸ë”© ì²´ì¸ ìƒì„±
    chain = prompt | llm_with_tool

    # í˜„ì¬ ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
    messages = state["messages"]

    # ê°€ì¥ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì¶”ì¶œ
    last_message = messages[-1]

    # ì›ë˜ ì§ˆë¬¸ ì¶”ì¶œ
    question = messages[0].content

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶”ì¶œ
    retrieved_docs = last_message.content

    # ê´€ë ¨ì„± í‰ê°€ ì‹¤í–‰
    scored_result = chain.invoke({"question": question, "context": retrieved_docs})

    # ê´€ë ¨ì„± ì—¬ë¶€ ì¶”ì¶œ
    score = scored_result.binary_score

    # ê´€ë ¨ì„± ì—¬ë¶€ì— ë”°ë¥¸ ê²°ì •
    if score == "yes":
        print("==== [DECISION: DOCS RELEVANT] ====")
        return "generate"
    else:
        print("==== [DECISION: DOCS NOT RELEVANT] ====")
        print(score)
        return "rewrite"

def agent(state):
    # í˜„ì¬ ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
    messages = state["messages"]

    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    model = ChatOpenAI(temperature=0, streaming=True, model=MODEL_NAME)

    # retriever tool ë°”ì¸ë”©
    model = model.bind_tools(tools)

    # ì—ì´ì „íŠ¸ ì‘ë‹µ ìƒì„±
    response = model.invoke(messages)

    # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë˜ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜
    return {"messages": [response]}

def rewrite(state):
    print("==== [QUERY REWRITE] ====")
    
    # í˜„ì¬ ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
    messages = state["messages"]

    # ì›ë˜ ì§ˆë¬¸ ì¶”ì¶œ
    question = messages[0].content

    # ì§ˆë¬¸ ê°œì„ ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    msg = [
        HumanMessage(
            content=f""" \n 
    Look at the input and try to reason about the underlying semantic intent / meaning. \n 
    Here is the initial question:
    \n ------- \n
    {question} 
    \n ------- \n
    Formulate an improved question: """,
        )
    ]

    # LLM ëª¨ë¸ë¡œ ì§ˆë¬¸ ê°œì„ 
    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)

    # Query-Transform ì²´ì¸ ì‹¤í–‰
    response = model.invoke(msg)

    # ì¬ì‘ì„±ëœ ì§ˆë¬¸ ë°˜í™˜
    return {"messages": [response]}

def generate(state):
    # í˜„ì¬ ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
    messages = state["messages"]

    # ì›ë˜ ì§ˆë¬¸ ì¶”ì¶œ
    question = messages[0].content

    # ê°€ì¥ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì¶”ì¶œ
    docs = messages[-1].content

    # RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    prompt = hub.pull("teddynote/rag-prompt")

    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, streaming=True)

    # RAG ì²´ì¸ êµ¬ì„±
    rag_chain = prompt | llm | StrOutputParser()

    # ë‹µë³€ ìƒì„± ì‹¤í–‰
    response = rag_chain.invoke({"context": docs, "question": question})
    return {"messages": [response]}
```

### 6.2.3 ê·¸ë˜í”„ ì •ì˜

```python
from langgraph.graph import END, StateGraph, START
from langgraph.prebuilt import ToolNode

workflow = StateGraph(AgentState)

workflow.add_node("agent", agent)
retrieve = ToolNode([retriever_tool])
workflow.add_node("retrieve", retrieve)
workflow.add_node("rewrite", rewrite)
workflow.add_node("generate", generate)

# ì‹œì‘ì ì—ì„œ ì—ì´ì „íŠ¸ ë…¸ë“œë¡œ ì—°ê²°
workflow.add_edge(START, "agent")

# ê²€ìƒ‰ ì—¬ë¶€ ê²°ì •ì„ ìœ„í•œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
workflow.add_conditional_edges(
    "agent",
    # ì—ì´ì „íŠ¸ ê²°ì • í‰ê°€
    tools_condition,
    {
        "tools": "retrieve",
        END: END
    },
)

# ì•¡ì…˜ ë…¸ë“œ ì‹¤í–‰ í›„ ì²˜ë¦¬ë  ì—£ì§€ ì •ì˜
workflow.add_conditional_edges(
    "retrieve",
    # ë¬¸ì„œ í’ˆì§ˆ í‰ê°€
    grade_documents,
)
workflow.add_edge("generate", END)
workflow.add_edge("rewrite", "agent")

graph = workflow.compile()
```

ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.graphs import visualize_graph

visualize_graph(graph)
```

<div align="center">
  <img src="/assets/images/langchain/15/agentic_rag_graph.png" width="25%" height="20%"/>
</div>

<br>

```python
from langchain_teddynote.messages import stream_graph
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(recursion_limit=10, configurable={"thread_id": "1"})

inputs = {
    "messages":[
        ("user", "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€?"),
    ]
}

# ê·¸ë˜í”„ ì‹¤í–‰
stream_graph(graph, inputs, config, node_names=["agent", "rewrite", "generate"])
```

```
Output:

==================================================
ğŸ”„ Node: agent ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ **"ì‚¼ì„± Gauss(ê°€ìš°ìŠ¤)"**ì…ë‹ˆë‹¤.

2023ë…„ 11ì›”, ì‚¼ì„±ì „ìëŠ” ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI ëª¨ë¸ì¸ "Samsung Gauss"ë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ìì—°ì–´ ì²˜ë¦¬, ì½”ë“œ ìƒì„±, ì´ë¯¸ì§€ ìƒì„± ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— í™œìš©ë  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. "Gauss"ë¼ëŠ” ì´ë¦„ì€ ìˆ˜í•™ì ì¹´ë¥¼ í”„ë¦¬ë“œë¦¬íˆ ê°€ìš°ìŠ¤(Carl Friedrich Gauss)ì—ì„œ ë”°ì˜¨ ê²ƒìœ¼ë¡œ, ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ì›ë¦¬ ì¤‘ í•˜ë‚˜ì¸ 'ì •ê·œë¶„í¬(Gaussian distribution)'ì™€ë„ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.
```

ì•„ë˜ëŠ” ë¬¸ì„œ ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ì§ˆë¬¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.

```python
inputs = {
    "messages": [
        ("user", "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?"),
    ]
}

# ê·¸ë˜í”„ ì‹¤í–‰
stream_graph(graph, inputs, config, node_names=["agent", "rewrite", "generate"])
```

```
Output:

==================================================
ğŸ”„ Node: agent ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œ(ì„œìš¸)ì…ë‹ˆë‹¤.
```

# 7. Adaptive RAG

Adaptive RAGëŠ” ì¿¼ë¦¬ ë¶„ì„ê³¼ ëŠ¥ë™ì /ìê¸° ìˆ˜ì • RAGë¥¼ ê²°í•©í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤. ì´ë²ˆì—ëŠ” LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ê³¼ ìê¸° ìˆ˜ì • RAG ê°„ì˜ ë¼ìš°íŒ… êµ¬í˜„ ì˜ˆì œë¡œ Adaptive RAGì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. 

ì£¼ë¡œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

- Create Index: ì¸ë±ìŠ¤ ìƒì„± ë° ë¬¸ì„œ ë¡œë“œ
- LLMs: LLMì„ ì‚¬ìš©í•œ ì¿¼ë¦¬ ë¼ìš°íŒ… ë° ë¬¸ì„œ í‰ê°€
- Web Search Tool: ì›¹ ê²€ìƒ‰ ë„êµ¬ ì„¤ì •
- Construct the Graph: ê·¸ë˜í”„ ìƒíƒœ ë° íë¦„ ì •ì˜
- Compile Graph: ê·¸ë˜í”„ ì»´íŒŒì¼ ë° ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
- Use Graph: ê·¸ë˜í”„ ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸

## 7.1 ê¸°ë³¸ PDF ê¸°ë°˜ Retrieval Chain ìƒì„±

ì´ì „ì— ì§„í–‰í–ˆë˜ ì˜ˆì œë“¤ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

## 7.2 ì¿¼ë¦¬ ë¼ìš°íŒ…ê³¼ ë¬¸ì„œ í‰ê°€

LLMs ë‹¨ê³„ì—ì„œëŠ” ì¿¼ë¦¬ ë¼ìš°íŒ…ê³¼ ë¬¸ì„œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ Adaptive RAGì˜ ì¤‘ìš”í•œ ë¶€ë¶„ìœ¼ë¡œ, íš¨ìœ¨ì ì¸ ì •ë³´ ê²€ìƒ‰ê³¼ ìƒì„±ì— ê¸°ì—¬í•©ë‹ˆë‹¤. 

- ì¿¼ë¦¬ ë¼ìš°íŒ…: ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì •ë³´ ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¿¼ë¦¬ì˜ ëª©ì ì— ë§ëŠ” ìµœì ì˜ ê²€ìƒ‰ ê²½ë¡œë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë¬¸ì„œ í‰ê°€: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆê³¼ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì—¬ ìµœì¢… ê²°ê³¼ì˜ ì •í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì€ LLMsì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ”ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.

ì´ ë‹¨ê³„ëŠ” Adaptive RAGì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ì§€ì›í•˜ë©°, ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ ì œê³µì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

```python
from typing import Literal

from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_teddynote.models import get_model_name, LLMs

# ìµœì‹  LLM ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
MODEL_NAME = get_model_name(LLMs.GPT4)

# ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë°ì´í„° ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•˜ëŠ” ë°ì´í„° ëª¨ë¸
class RouteQuery(BaseModel):
    """Route a user query to the most relevant datasource."""

    # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒì„ ìœ„í•œ ë¦¬í„°ëŸ´ íƒ€ì… í•„ë“œ
    datasource: Literal["vectorstore", "web_search"] = Field(
        ...,
        description="Given a user question choose to route it to web search or a vectorstore",
    )

# LLM ì´ˆê¸°í™” ë° í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±
llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
structured_llm_router = llm.with_structured_output(RouteQuery)

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸
system = """You are an expert at routing a user question to a vectorstore or web search.
The vectorstore contains documents related to DEC 2023 AI Brief Report(SPRI) with Samsung Gause, Anthropic, etc.
Use the vectorstore for questions on these topics. Otherwise, use web-search."""

# Routingì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ êµ¬ì¡°í™”ëœ LLM ë¼ìš°í„°ë¥¼ ê²°í•©í•˜ì—¬ ì§ˆë¬¸ ë¼ìš°í„° ìƒì„±
question_router = route_prompt | structured_llm_router
```

ì¿¼ë¦¬ ë¼ìš°íŒ… ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸ í•´ë³¸ ë’¤ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë´…ë‹ˆë‹¤.

```python
# ë¬¸ì„œ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸
print(
    question_router.invoke(
        {"question": "AI Briefì—ì„œ ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€?"}
    )
)
```

```
Output:
datasource='vectorstore'
```

```python
# ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸
print(question_router.invoke({"question": "íŒêµì—ì„œ ê°€ì¥ ë§›ìˆëŠ” ë”¤ì„¬ì§‘ ì°¾ì•„ì¤˜"}))
```

```
Output:
datasource='web_search'
```

## 7.3 ê²€ìƒ‰ í‰ê°€ê¸°(Retrieval Grader)

```python
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# ë¬¸ì„œ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ëª¨ë¸ ì •ì˜
class GradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""

    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )

# LLM ì´ˆê¸°í™” ë° í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±
llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
system = """You are a grader assessing relevance of a retrieved document to a user question. \n 
    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n
    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""

grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
    ]
)

# ë¬¸ì„œ ê²€ìƒ‰ê²°ê³¼ í‰ê°€ê¸° ìƒì„± 
retrieval_grader = grade_prompt | structured_llm_grader
```

ìƒì„±í•œ `retrieval_grader`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ê²°ê³¼ë¥¼ í‰ê°€í•´ ë´…ë‹ˆë‹¤.

```python
question = "ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€?"

# ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
docs = pdf_retriever.invoke(question)

# ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° 
retrieved_doc = docs[1].page_content

# í‰ê°€ ê²°ê³¼ ì¶œë ¥
print(retrieval_grader.invoke({"question": question, "document": retrieved_doc}))
```

```
Output:
binary_score='yes'
```

## 7.4 ë‹µë³€ ìƒì„±ì„ ìœ„í•œ RAG ì²´ì¸ ìƒì„±

```python
from langchain_classic import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# LangChain Hubì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°(RAG í”„ë¡¬í”„íŠ¸ëŠ” ììœ ë¡­ê²Œ ìˆ˜ì • ê°€ëŠ¥)
prompt = hub.pull("teddynote/rag-prompt")

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)

# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(
        [
            f'<document><content>{doc.page_content}</content><source>{doc.metadata["source"]}</source><page>{doc.metadata["page"]+1}</page></document>'
            for doc in docs
        ]
    )

# RAG ì²´ì¸ ìƒì„±
rag_chain = prompt | llm | StrOutputParser()
```

ì´ì œ ìƒì„±í•œ `rag_chain`ì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
# RAG ì²´ì¸ì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
generation = rag_chain.invoke({"context": format_docs(docs), "question": question})
print(generation)
```

```
Output:
ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™(Samsung Gauss)ì´ë‹¤.

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (p.13)
```

## 7.5 ë‹µë³€ì˜ Hallucination ì²´ì»¤ ì¶”ê°€

```python
# í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬ë¥¼ ìœ„í•œ ë°ì´í„° ëª¨ë¸ ì •ì˜
class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in generation answer."""

    binary_score: str = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )

# í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
structured_llm_grader = llm.with_structured_output(GradeHallucinations)

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
system = """You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n 
    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."""

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
    ]
)

# í™˜ê° í‰ê°€ê¸° ìƒì„±
hallucination_grader = hallucination_prompt | structured_llm_grader
```

ìƒì„±í•œ `hallucination_grader`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

```python
# í‰ê°€ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€ í‰ê°€
hallucination_grader.invoke({"documents": docs, "generation": generation})
```

```
Output:
GradeHallucinations(binary_score='yes')
```

## 7.6 ë‹µë³€ í‰ê°€ê¸° ì¶”ê°€

LLMìœ¼ë¡œë¶€í„° ìƒì„±ëœ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” í‰ê°€ê¸°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
class GradeAnswer(BaseModel):
    """Binary scoring to evaluate the appropriateness of answers to questions"""

    binary_score: str = Field(
        description="Indicate 'yes' or 'no' whether the answer solves the question"
    )

# í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•œ LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
structured_llm_grader = llm.with_structured_output(GradeAnswer)

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
system = """You are a grader assessing whether an answer addresses / resolves a question \n 
     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question."""
answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
    ]
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ êµ¬ì¡°í™”ëœ LLM í‰ê°€ê¸°ë¥¼ ê²°í•©í•˜ì—¬ ë‹µë³€ í‰ê°€ê¸° ìƒì„±
answer_grader = answer_prompt | structured_llm_grader
```

```python
# í‰ê°€ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì´ ì§ˆë¬¸ì„ í•´ê²°í•˜ëŠ”ì§€ ì—¬ë¶€ í‰ê°€ 
answer_grader.invoke({"question": question, "generation": generation})
```

```
Output:
GradeAnswer(binary_score='yes')
```

## 7.7 ì¿¼ë¦¬ ì¬ì‘ì„±

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model=MODEL_NAME, temperature=0)

system = """You a question re-writer that converts an input question to a better version that is optimized \n 
for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning."""

# Query Rewriter í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
re_write_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        (
            "human",
            "Here is the initial question: \n\n {question} \n Formulate an improved question."
        ),
    ]
)

# Query Rewriter ìƒì„±
question_rewriter = re_write_prompt | llm | StrOutputParser()
```

êµ¬ì¶•í•œ `question_rewriter`ì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ê°œì„ ëœ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
# ì§ˆë¬¸ ì¬ì‘ì„±ê¸°ì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ê°œì„ ëœ ì§ˆë¬¸ ìƒì„±
question_rewriter.invoke({"question": question})
```

```
Output:
ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥(AI) ì„œë¹„ìŠ¤ ë˜ëŠ” í”Œë«í¼ì˜ ê³µì‹ ëª…ì¹­ì€ ë¬´ì—‡ì¸ê°€?
```

## 7.8 ì›¹ ê²€ìƒ‰ ë„êµ¬

ì›¹ ê²€ìƒ‰ ë„êµ¬ëŠ” Adaptive RAGì˜ ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œë¡œ, ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ì‚¬ìš©ìê°€ ìµœì‹  ì´ë²¤íŠ¸ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì— ëŒ€í•´ ì‹ ì†í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ë„êµ¬ë¡œëŠ” ì˜ˆì œì— í•­ìƒ ì‚¬ìš©í•˜ë˜ TavilySearchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.tools.tavily import TavilySearch

# ì›¹ ê²€ìƒ‰ ë„êµ¬ ìƒì„±
web_search_tool = TavilySearch(max_results=3)
```

## 7.9 ê·¸ë˜í”„ êµ¬ì„±

### 7.9.1 ê·¸ë˜í”„ ìƒíƒœ ì •ì˜

```python
from typing import List
from typing_extensions import TypedDict, Annotated

class GraphState(TypedDict):
    """
    ê·¸ë˜í”„ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° ëª¨ë¸

    Attributes:
        question: ì§ˆë¬¸
        generation: LLM ìƒì„±ëœ ë‹µë³€
        documents: ë„íë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """

    question: Annotated[str, "User question"]
    generation: Annotated[str, "LLM generated answer"]
    documents: Annotated[List[str], "List of documents"]
```

### 7.9.2 ê·¸ë˜í”„ íë¦„ ì •ì˜

ê·¸ë˜í”„ì˜ íë¦„ì„ ì •ì˜í•˜ì—¬ Adaptive RAGì˜ ì‘ë™ ë°©ì‹ì„ ëª…í™•íˆ í•©ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ê·¸ë˜í”„ì˜ ìƒíƒœì™€ ì „í™˜ì„ ì„¤ì •í•˜ì—¬ ì¿¼ë¦¬ ì²˜ë¦¬ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤ .

- ìƒíƒœ ì •ì˜: ê·¸ë˜í”„ì˜ ê° ìƒíƒœë¥¼ ëª…í™•íˆ ì •ì˜í•˜ì—¬ ì¿¼ë¦¬ì˜ ì§„í–‰ ìƒí™©ì„ ì¶”ì í•©ë‹ˆë‹¤.
- ì „í™˜ ì„¤ì •: ìƒíƒœ ê°„ì˜ ì „í™˜ì„ ì„¤ì •í•˜ì—¬ ì¿¼ë¦¬ê°€ ì ì ˆí•œ ê²½ë¡œë¥¼ ë”°ë¼ ì§„í–‰ë˜ë„ë¡ í•©ë‹ˆë‹¤.
- íë¦„ ìµœì í™”: ê·¸ë˜í”„ì˜ íë¦„ì„ ìµœì í™”í•˜ì—¬ ì •ë³´ ê²€ìƒ‰ê³¼ ìƒì„±ì˜ ì •í™•ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### 7.9.3 ë…¸ë“œ ì •ì˜

```python
from langchain_core.documents import Document

# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ
def retrieve(state):
    print("==== [RETRIEVE] ====")
    question = state["question"]

    # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
    documents = pdf_retriever.invoke(question)
    return {"documents": documents, "question": question}

# ë‹µë³€ ìƒì„± ë…¸ë“œ
def generate(state):
    print("==== [GENERATE] ====")
    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    documents = state["documents"]

    # RAG ë‹µë³€ ìƒì„±
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"documents": documents, "question": question, "generation": generation}

def grade_documents(state):
    print("==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====")
    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    documents = state["documents"]

    # ê° ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    filtered_docs = []
    for d in documents:
        score = retrieval_grader.invoke(
            {"question": question, "document": d.page_content}
        )
        grade = score.binary_score
        if grade == "yes":
            print("---GRADE: DOCUMENT RELEVANT---")
            # ê´€ë ¨ì„±ì´ ìˆëŠ” ë¬¸ì„œ ì¶”ê°€
            filtered_docs.append(d)
        else:
            # ê´€ë ¨ì„±ì´ ì—†ëŠ” ë¬¸ì„œëŠ” ê±´ë„ˆë›°ê¸°
            print("---GRADE: DOCUMENT NOT RELEVANT---")
            continue
    return {"documents": filtered_docs, "question": question}

# ì§ˆë¬¸ ì¬ì‘ì„± ë…¸ë“œ
def transform_query(state):
    print("====- [TRANSFORM QUERY] ====")
    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    documents = state["documents"]

    # ì§ˆë¬¸ ì¬ì‘ì„±
    better_question = question_rewriter.invoke({"question": question})
    return {"documents": documents, "question": better_question}

# ì›¹ ê²€ìƒ‰ ë…¸ë“œ
def web_search(state):
    print("==== [WEB SEARCH] ====")

    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]

    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    web_results = web_search_tool.invoke({"query": question})
    web_results_docs = [
        Document(
            page_content=web_result["content"],
            metadata={"source": web_result["url"]},
        )
        for web_result in web_results
    ]

    return {"documents": web_results_docs, "question": question}
```

### 7.9.4 ì—£ì§€ ì •ì˜

```python
# ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œ
def route_question(state):
    print("==== [ROUTE QUESTION] ====")

    # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]

    # ì§ˆë¬¸ ë¼ìš°íŒ…
    source = question_router.invoke({"question": question})

    # ì§ˆë¬¸ ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¥¸ ë…¸ë“œ ë¼ìš°íŒ…
    if source.datasource == "web_search":
        print("==== [ROUTE QUESTION TO WEB SEARCH] ====")
        return "web_search"
    elif source.datasource == "vectorstore":
        print("==== [ROUTE QUESTION TO VECTORSTORE] ====")
        return "vectorstore"

# ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ë…¸ë“œ
def decide_to_generate(state):
    print("==== [DECISION TO GENERATE] ====")
    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    filtered_documents = state["documents"]

    if not filtered_documents:
        # ëª¨ë“  ë¬¸ì„œê°€ ê´€ë ¨ì„± ì—†ëŠ” ê²½ìš° ì§ˆë¬¸ ì¬ì‘ì„±
        print(
            "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ===="
        )
        return "transform_query"
    else:
        # ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° ë‹µë³€ ìƒì„±
        print("==== [DECISION: GENERATE] ====")
        return "generate"

def hallucination_check(state):
    print("==== [CHECK HALLUCINATIONS] ====")

    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]

    # í™˜ê° í‰ê°€
    score = hallucination_grader.invoke(
        {"documents": documents, "generation": generation}
    )
    grade = score.binary_score

    # Hallucination ì—¬ë¶€ í™•ì¸
    if grade == "yes":
        print("==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====")

        # ë‹µë³€ì˜ ê´€ë ¨ì„±(Relevance) í‰ê°€
        print("==== [GRADE GENERATED ANSWER vs QUESTION] ====")
        score = answer_grader.invoke({"question": question, "generation": generation})
        grade = score.binary_score

        # ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ì²˜ë¦¬
        if grade == "yes":
            print("==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====")
            return "relevant"
        else:
            print("==== [DECISION: GENERATED ANSWER DOES NOT ADDRESS QUESTION] ====")
            return "not_relevant"
    else:
        print("==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====")
        return "hallucination"
```

### 7.9.5 ê·¸ë˜í”„ ì»´íŒŒì¼

ê·¸ë˜í”„ ì»´íŒŒì¼ ë‹¨ê³„ì—ì„œëŠ” Adaptive RAGì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ë§Œë“­ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ê·¸ë˜í”„ì˜ ê° ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì—°ê²°í•˜ì—¬ ì¿¼ë¦¬ ì²˜ë¦¬ì˜ ì „ì²´ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤.

```python
from langgraph.graph import END, StateGraph, START
from langgraph.checkpoint.memory import MemorySaver

workflow = StateGraph(GraphState)

# ë…¸ë“œ ì •ì˜
workflow.add_node("web_search", web_search)
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("transform_query", transform_query)

workflow.add_conditional_edges(
    START,
    route_question,
    {
        "web_search": "web_search",
        "vectorstore": "retrieve",
    },
)

workflow.add_edge("web_search", "generate")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    },
)

workflow.add_edge("transform_query", "retrieve")
workflow.add_conditional_edges(
    "generate",
    hallucination_check,
    {
        "hallucination": "generate",
        "relevant": END,
        "not relevant": "transform_query",
    },
)

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile(checkpointer=MemorySaver())
```

ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.graphs import visualize_graph

visualize_graph(app)
```

<div align="center">
  <img src="/assets/images/langchain/15/adaptive_rag_graph.png" width="25%" height="20%"/>
</div>

<br>

### 7.9.6 ê·¸ë˜í”„ ì‚¬ìš©

ê·¸ë˜í”„ ì‚¬ìš© ë‹¨ê³„ì—ì„œëŠ” Adaptive RAGì˜ ì‹¤í–‰ì„ í†µí•´ ì¿¼ë¦¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ê° ë…¸ë“œì™€ ì—£ì§€ë¥¼ ë”°ë¼ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
from langchain_teddynote.messages import stream_graph
from langchain_core.runnables import RunnableConfig
import uuid

# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id": uuid.uuid4()})

# ì§ˆë¬¸ ì…ë ¥
inputs = {
    "question": "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì˜ ì´ë¦„ì€?",
}

# ê·¸ë˜í”„ ì‹¤í–‰
stream_graph(app, inputs, config, node_names=["agent", "rewrite", "generate"])
```

```
Output:
==== [ROUTE QUESTION] ====
==== [ROUTE QUESTION TO VECTORSTORE] ====
==== [RETRIEVE] ====
==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
---GRADE: DOCUMENT NOT RELEVANT---
==== [DECISION TO GENERATE] ====
==== [DECISION: GENERATE] ====
==== [GENERATE] ====

==================================================
ğŸ”„ Node: generate ğŸ”„
- - - - - - - - - - - - - - - - - - - - - - - - - 
ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ì´ë‹¤.

**Source**
- /content/drive/MyDrive/LangChain/pdf_data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (p.12)==== [CHECK HALLUCINATIONS] ====
{"binary_score":"yes"}==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====
==== [GRADE GENERATED ANSWER vs QUESTION] ====
{"binary_score":"yes"}==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====
```

# ë§ˆë¬´ë¦¬

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” LangGraphë¥¼ ì´ìš©í•œ RAGì˜ êµ¬ì¡° ì„¤ê³„ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ì ì¸ Native RAGì— ì—¬ëŸ¬ ë„êµ¬ë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•´ ë³´ì•˜ê³ , ì´ë¥¼ í†µí•©í•œ Agentic RAG êµ¬ì¶•ë„ ì§„í–‰í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë¿ë§Œ ì•„ë‹ˆë¼ LLMì´ ìƒì„±í•œ ë‹µë³€ì— ëŒ€í•œ í™˜ê° í˜„ìƒ ê²€ì‚¬ì™€ ë‹µë³€ì˜ í’ˆì§ˆ ê²€ì‚¬ê¹Œì§€ ì§„í–‰í•˜ëŠ” ë„êµ¬ë¥¼ ì •ì˜í•˜ì—¬ ë…¸ë“œë¡œ ë§Œë“¤ì–´ Adaptive RAGê¹Œì§€ êµ¬í˜„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.

ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©°, ë³¸ë¬¸ ë‚´ìš© ì¤‘ì— ì˜ëª»ëœ ë‚´ìš©ì´ë‚˜ ì˜¤íƒ€ ê¶ê¸ˆí•˜ì‹  ì‚¬í•­ì´ ìˆìœ¼ì‹¤ ê²½ìš° ëŒ“ê¸€ ë‚¨ê²¨ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

# ì°¸ì¡°

- í…Œë””ë…¸íŠ¸ - LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼KR(https://wikidocs.net/book/14314)